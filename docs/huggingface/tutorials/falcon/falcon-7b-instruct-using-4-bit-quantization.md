# ä½¿ç”¨ 4-bit quantization èˆ‡ 6GB GPU é‹è¡Œ Falcon-7B-Instruct

![](./assets/TII-OG.jpg)

åŸæ–‡: [Run your private LLM: Falcon-7B-Instruct with less than 6GB of GPU using 4-bit quantization](https://vilsonrodrigues.medium.com/run-your-private-llm-falcon-7b-instruct-with-less-than-6gb-of-gpu-using-4-bit-quantization-ff1d4ffbabcc)

Falcon æ˜¯ [TII](https://falconllm.tii.ae/) åœ¨ Apache 2.0 è¨±å¯ä¸‹æ¨å‡ºçš„å®Œå…¨é–‹æºæ¨¡å‹ã€‚

!!! quote
    Falcon LLM æ˜¯åœ¨ 1 è¬å„„å€‹ tokens ä¸Šè¨“ç·´çš„æœ‰è‘— 400 å„„å€‹åƒæ•¸çš„åŸºç¤å¤§å‹èªè¨€æ¨¡å‹ (LLM)ã€‚ TII ç¾å·²ç™¼å¸ƒ Falcon LLM â€” 40B æ¨¡å‹ã€‚

    è©²æ¨¡å‹åƒ…ä½¿ç”¨ GPT-3 è¨“ç·´è¨ˆç®— 75% çš„ç®—åŠ›ã€Chinchilla çš„ 40% çš„ç®—åŠ›å’Œ PaLM-62B çš„ 80% çš„ç®—åŠ›ã€‚

![](./assets/0_pf9ogNf-_ASDEZYu.webp)

åœ¨ HuggingFace çš„ OpenLLM æ’è¡Œæ¦œä¸Šï¼ŒFalcon æ›¾å æ“šæ’åç¬¬ä¸€çš„ä½ç½®ï¼Œå£“åˆ¶äº† META çš„ LLaMA-65Bã€‚

!!! quote
    Falcon æ˜¯ä¸€å€‹åŒ…å« 400 å„„å€‹åƒæ•¸çš„è‡ªå›æ­¸è§£ç¢¼å™¨(autoregressive decoder-only) æ¨¡å‹ï¼Œåœ¨ 1 è¬å„„å€‹ token ä¸Šé€²è¡Œè¨“ç·´ã€‚å®ƒåœ¨ AWS ä¸Šçš„ **384** å€‹ GPU ä¸Šé€²è¡Œäº†å…©å€‹æœˆçš„è¨“ç·´ã€‚

    Falcon çš„é è¨“ç·´æ•¸æ“šé›†æ˜¯å¾ç¶²çµ¡ä¸Šä½¿ç”¨çˆ¬èŸ²ä¾†æŠ“å–æ”¶é›†çš„ã€‚ä½¿ç”¨ `CommonCrawl` çš„ dumpsï¼Œç¶“éå¤§é‡éæ¿¾ï¼ˆä»¥åˆªé™¤æ©Ÿå™¨ç”Ÿæˆçš„æ–‡æœ¬å’Œæˆäººå…§å®¹ï¼‰å’Œé‡è¤‡æ•¸æ“šåˆªé™¤å¾Œï¼Œçµ„è£äº†åŒ…å«è¿‘ 5 è¬å„„å€‹ä»¤ç‰Œçš„é è¨“ç·´æ•¸æ“šé›†ã€‚

Falcon æœ‰ä¸€å€‹åƒæ•¸ç‚º 7B çš„å¼Ÿå¼Ÿï¼Œåœ¨ 1.5T çš„ token ä¸Šé€²è¡Œè¨“ç·´ã€‚å…©è€…éƒ½åœ¨ instruct æ•¸æ“šé›†ä¸­é€²è¡Œäº†å¾®èª¿ã€‚ç¼ºé»æ˜¯è©²æ¨¡å‹çš„åºåˆ—é•·åº¦ç‚º `2048` å€‹ tokensã€‚

- [Falcon-40B-Instruct](https://huggingface.co/tiiuae/falcon-40b-instruct)
- [Falcon-7B-Instruct](https://huggingface.co/tiiuae/falcon-7b-instruct)

`Falcon-7B-Instruct` çš„æ¨¡å‹å­˜åœ¨ç¡¬ç¢Ÿçš„ç©ºé–“å¤§å°ç´„ç‚º 15GBï¼Œå¦‚ä½•æ”¾ç½®åœ¨å°æ–¼ 6GB çš„ GPU ä¸Šï¼Ÿ é‡åŒ–ï¼ˆQuantization)ã€‚

## æº–å‚™æ¨¡å‹

[bitsandbytes](https://github.com/TimDettmers/bitsandbytes) æ˜¯ä¸€å€‹ä»¤äººé©šå˜†çš„å¥—ä»¶ï¼Œå¯åœ¨æ·±åº¦å­¸ç¿’æ¨¡å‹ä¸­æ‡‰ç”¨é‡åŒ–ä¾†å£“ç¸®æ¨¡å‹ä½¿ç”¨ GPU VMem çš„å¤§å°ã€‚ HuggingFace èˆ‡ bnb å•Ÿå‹•æ•´åˆã€‚åœ¨[Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA
](https://huggingface.co/blog/4bit-transformers-bitsandbytes)æ–‡ç« ä¸­ï¼Œä»–å€‘è§£é‡‹ç­å¦‚ä½•ä½¿ç”¨ 4-bit quantization ä¾†é‹è¡Œæ¨¡å‹ã€‚

æ¥è‘—æˆ‘å€‘å°‡åœ¨ Colab ä¸­åŸ·è¡Œäº†é€™äº›ç›¸åŒçš„æ­¥é©Ÿã€‚ä½†åœ¨é€²è¡Œä¹‹å‰ï¼Œæˆ‘å€‘éœ€è¦å°‡åŸå§‹ `Falcon-7B-Instruct` æ¨¡å‹æ¬Šé‡è½‰æ›ç‚ºè¼ƒå°çš„ chunkï¼Œä»¥ä¾¿ä½¿ç”¨ [Accelerate](https://huggingface.co/docs/accelerate/index) æœ‰æ•ˆåŠ è¼‰æ¨¡å‹ã€‚ç”±æ–¼åŸå§‹æ¬Šé‡å¾ˆå¤§ï¼Œå®ƒæ¶ˆè€—äº† Colab é‹ç®—ç’°å¢ƒä¸­çš„æ‰€æœ‰ RAMã€‚

- [vilsonrodrigues/falcon-7b-instruct-sharded](https://huggingface.co/vilsonrodrigues/falcon-7b-instruct-sharded)

![](./assets/falcon-7b-instruct-sharded.png)

## å•Ÿå‹• Falcon-7B-Instruct æ¨¡å‹

å¦‚æœæƒ³è¦é‡æ–°åœ¨ä½ è‡ªå·±çš„ç’°å¢ƒä¾†é€²è¡Œä¸‹åˆ—çš„æ“ä½œ, ä½ éœ€è¦:

1. ç™»å…¥ Colab 
2. ç¢ºå®š runtime æœ‰æ›è¼‰äº† T4 GPU

Colab Notebook éˆçµ: [ä½¿ç”¨ 4-bit quantization èˆ‡ 6GB GPU é‹è¡Œ Falcon-7B-Instruct](https://colab.research.google.com/drive/1dHb0eX4YKV2o2NZdXfOr76vTuIQVjHMT?usp=sharing)

### å®‰è£å¥—ä»¶

```python
!pip install -q -U bitsandbytes
!pip install -q -U git+https://github.com/huggingface/transformers.git
!pip install -q -U git+https://github.com/huggingface/peft.git
!pip install -q -U git+https://github.com/huggingface/accelerate.git
!pip install -q -U einops
!pip install -q -U safetensors
```

### é…ç½® `bitsandbytes`

4-bit çš„é‡åŒ–æ¨¡å‹å£“ç¸®æ•´åˆçš„è¨­å®šè£¡æœ‰ 2 ç¨®ä¸åŒçš„é‡åŒ–é¡å‹: `FP4` å’Œ `NF4`ã€‚`NF4` dtype ä»£è¡¨ `Normal Float 4`ï¼Œåœ¨ QLoRA è«–æ–‡ä¸­ä»‹ç´¹äº† `NF4` çš„çµæ§‹ã€‚

ä½ å¯ä»¥ä½¿ç”¨ [`BitsAndBytesConfig`](https://huggingface.co/docs/transformers/v4.32.1/en/main_classes/quantization#transformers.BitsAndBytesConfig) ä¸­çš„ `bnb_4bit_quant_type` åœ¨é€™å…©ç¨® dtype ä¹‹é–“åˆ‡æ›(é è¨­æƒ…æ³ä¸‹ï¼Œä½¿ç”¨ `FP4` é‡åŒ–)ã€‚

4-bit çš„é‡åŒ–æ¨¡å‹å£“ç¸®è¨­å®šå¯ä»¥ç¯€çœGPU VMemoryå…§å­˜çš„ä½¿ç”¨ï¼Œé€™æ¨£çš„æŠ€è¡“è®“æˆ‘å€‘å¯ä»¥åœ¨ NVIDIA-T4 16GB ä¸Šå¾®èª¿ llama-13b æ¨¡å‹ï¼Œåºåˆ—é•·åº¦ç‚º `1024`ï¼Œæ‰¹é‡å¤§å°ç‚º `1`ï¼Œæ¢¯åº¦ç´¯ç©æ­¥é•·ç‚º `4`ã€‚

å¦å¤– double quantization æ˜¯å¯å†è®“ VMemory ä½¿ç”¨æ¸›å°‘çš„æŠ€è¡“ã€‚`double quantization` æ˜¯åœ¨ç¬¬ä¸€æ¬¡é‡åŒ–ä¹‹å¾Œå†ä½¿ç”¨ç¬¬äºŒæ¬¡é‡åŒ–ï¼Œä»¥ä¾¿ç‚ºæ¯å€‹åƒæ•¸é¡å¤–ç¯€çœ `0.4` bitã€‚

è¦å•Ÿç”¨æ­¤åŠŸèƒ½ï¼Œåªéœ€åœ¨å‰µå»ºé‡åŒ–é…ç½®æ™‚æ·»åŠ  `bnb_4bit_use_double_quant=True` å³å¯ï¼åœ¨æœ¬æ¬¡çš„å¯¦é©—ä¸­, æˆ‘å€‘å°‡ä½¿ç”¨ `NF4`!

```python
import torch
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
)
```

### åŠ è¼‰ Model å’Œ Pipeline

```python
from transformers import AutoModelForCausalLM, AutoTokenizer,pipeline

# smaller chunks on safetensors for low RAM environments
model_id = "vilsonrodrigues/falcon-7b-instruct-sharded"

model_4bit = AutoModelForCausalLM.from_pretrained(
        model_id,
        device_map="auto",
        quantization_config=quantization_config,
        trust_remote_code=True)

tokenizer = AutoTokenizer.from_pretrained(model_id)
```

è®“æˆ‘å€‘æª¢è¦–ä¸€ä¸‹ Falcon æ¨¡å‹çµæ§‹:

```python
print(model_4bit)
```

çµæœ:

```bash hl_lines="13-15"
FalconForCausalLM(
  (transformer): FalconModel(
    (word_embeddings): Embedding(65024, 4544)
    (h): ModuleList(
      (0-31): 32 x FalconDecoderLayer(
        (self_attention): FalconAttention(
          (maybe_rotary): FalconRotaryEmbedding()
          (query_key_value): Linear4bit(in_features=4544, out_features=4672, bias=False)
          (dense): Linear4bit(in_features=4544, out_features=4544, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
        )
        (mlp): FalconMLP(
          (dense_h_to_4h): Linear4bit(in_features=4544, out_features=18176, bias=False)
          (act): GELU(approximate='none')
          (dense_4h_to_h): Linear4bit(in_features=18176, out_features=4544, bias=False)
        )
        (input_layernorm): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)
      )
    )
    (ln_f): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=4544, out_features=65024, bias=False)
)
```

å¾æ¨¡å‹æ¶æ§‹çš„ summary ä¸­, æˆ‘å€‘å¯è§€å¯Ÿåˆ°ç·šæ€§å±¤(linear layer)è¢«é‡åŒ–ç‚º `4bit`ã€‚

æª¢æŸ¥ Colab ä¸­çš„ VRAM æ¶ˆè€—ã€‚å¯çœ‹åˆ°å¤§ç´„ç”¨äº† 5.3GB é¡¯å­˜ï¼ ğŸ¤¯

### æ¨è«–é©—è­‰

æ¥è‘—è¦è¦é€™å€‹ Falcon-7b æ¨¡å‹ä¾†é€²è¡Œ inference, è®“æˆ‘å€‘ä½¿ç”¨ Hugging face çš„ [pipeline](https://huggingface.co/docs/transformers/main_classes/pipelines) é¡åˆ¥ä¾†åŒ…è¦†æ•´å€‹æ¨¡å‹èˆ‡è¨­å®šã€‚

```python
import torch
import transformers

pipeline = transformers.pipeline(
        "text-generation", # è¦é€²è¡Œçš„ task é¡å‹
        model=model_4bit,  # è¦ä½¿ç”¨çš„ llm model
        tokenizer=tokenizer, # è¦ä½¿ç”¨çš„ tokenizer
        use_cache=True,
        device_map="auto", # å®šç¾©è¨­å‚™
        max_length=296,
        do_sample=True,
        top_k=10,
        num_return_sequences=1,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.eos_token_id,
)
```

æ¸¬è©¦ä¸€ä¸‹:

```python
sequences = pipeline(
   "Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:"
   )
```

çµæœ:

```bash
[{'generated_text': "Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron: Ahhh, hello Daniel!\nDaniel: How's it goin' today?\nGirafatron: I'm a-giraf-in' just fine, thank you so much for asking!\nI'll be there soon\nGirafatron - a-giraf-in\nI'll be there soon.\nGirafatron, the giraffe of all the giraffes."}]
```

## èˆ‡ LangChain é…åˆä½¿ç”¨

[LangChain](https://github.com/hwchase17/langchain) æ˜¯ä¸€å€‹å¤šåŠŸèƒ½æ¡†æ¶ï¼Œæ—¨åœ¨æ”¯æŒèªè¨€æ¨¡å‹é©…å‹•çš„æ‡‰ç”¨ç¨‹åºçš„é–‹ç™¼ã€‚é€šéåˆ©ç”¨ `LangChain` çš„èƒ½åŠ›ï¼Œæˆ‘å€‘å¯ä»¥å¿«é€Ÿå‰µå»ºå¼·å¤§ä¸”é«˜æ•ˆçš„æ‡‰ç”¨ç¨‹åºâš¡ã€‚

```python
# Some error in colab. fix with
import locale
locale.getpreferredencoding = lambda: "UTF-8"
```

å®‰è£ `langchain` å¥—ä»¶:

```bash
!pip install langchain
```

### å‰µå»ºä¸€å€‹ç°¡å–®çš„ chain

```python
from langchain import HuggingFacePipeline
from langchain import PromptTemplate, LLMChain

llm = HuggingFacePipeline(pipeline=pipeline)

template = """Question: {question}
Answer: Let's think step by step."""

prompt = PromptTemplate(
    template=template, 
    input_variables= ["question"]
)

llm_chain = LLMChain(prompt=prompt, llm=llm)
```

è®“æˆ‘å€‘é€²è¡Œä¸€äº›é©—æ¸¬:

**Test#1:**

```python
llm_chain("How to prepare eggs?")
```

çµæœ:

```bash
{'question': 'How to prepare eggs?',
 'text': " First, we need eggs from hens. Next, you have to keep the hens comfortable. They don't lay if they're cold, so you need to have a warm environment. You also need to keep them away from other hens and predators. Then, the eggs need to be laid in a nesting box or nesting tray. You can keep the eggs clean by using a nesting box liner. Finally, after the eggs are laid, you need to keep the hens comfortable by providing a clean, dry nest."}
```

**Test#2:**

```python
template2 = """Question: /n {question}. Answer: """

prompt2 = PromptTemplate(
    template=template2,
    input_variables= ["question"]
)

llm_chain_2 = LLMChain(prompt=prompt2, llm=llm)

result_explanation = llm_chain_2("Explain antibiotics")

result_explanation['text']
```

çµæœ:

```bash
/n
Antibiotics are medications that are used to treat bacterial infections in the body. They work by preventing bacteria from growing and spreading, thus killing the bacteria or reducing their numbers. There are several classes of antibiotics, such as tetracycline, macrolide, penicillin, cephalosporin, and fluoroquinolones. Antioxidants, such as vitamin C, can also be taken along with antibiotics to reduce the risk of side effects.
```

## çµè«–

Falcon æ¨¡å‹å¾ˆé©šäººä½†å®ƒé‚„æ²’æœ‰é”åˆ°èˆ‡ ChatGPT ç›¸åŒçš„è¡¨ç¾ã€‚åŒæ™‚å®ƒä¹Ÿä¸æ˜¯æœ€å¿«çš„ token ç”Ÿæˆæ¨¡å‹ã€‚å³ä¾¿å¦‚æ­¤ï¼ŒFalcon-7B æ¨¡å‹ä¹Ÿå¯ä»¥å®Œæˆè¨±å¤š LLm åŸºæœ¬çš„æ‡‰ç”¨ã€‚å¦‚æœæƒ³è¦æ›´å®Œæ•´çš„ LLMï¼Œæ‚¨æ‡‰è©²é¸æ“‡ 40Bã€‚

