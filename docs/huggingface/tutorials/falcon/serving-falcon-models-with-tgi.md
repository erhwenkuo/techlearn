#  Falcon æ¨¡å‹æä¾›æ¨è«–æœå‹™ä½¿ç”¨ ğŸ¤— TGI

![](./assets/falcon-ub-inference-via-tgi.png)

åŸæ–‡: [Serving Falcon models with ğŸ¤— Text Generation Inference (TGI)](https://vilsonrodrigues.medium.com/serving-falcon-models-with-text-generation-inference-tgi-5f32005c663b)


åœ¨[ä½¿ç”¨ 4-bit quantization èˆ‡ 6GB GPU é‹è¡Œ Falcon-7B-Instruct](./falcon-7b-instruct-using-4-bit-quantization.md)æ–‡ç« ä¸­ï¼Œæˆ‘å€‘çœ‹åˆ°ä½¿ç”¨ 4bit quantization åœ¨ å–®å€‹ GPU (6 GB) ä¸­é‹è¡Œ `Falcon-7b-Instruct` æ¨¡å‹ã€‚æœ¬æ–‡å°‡å±•ç¤ºå¦‚ä½•åœ¨æœ¬åœ°å’Œé›²ç«¯é‹è¡Œ Falcon æ¨¡å‹ã€‚

ğŸ¤— `Text Generation Inference` æ˜¯ HuggingFace è¨­è¨ˆçš„ä¸€ç¨®å¯ç”¨æ–¼ç”Ÿç”¢çš„æ¨¡å‹æ¨ç†å·¥å…·ï¼Œå®ƒå¯è¼•é¬†ç‚º LLM æ‡‰ç”¨ç¨‹åºæä¾›æ”¯æŒã€‚å®ƒç”± Pythonã€Rust å’Œ gRPC æä¾›æ”¯æŒã€‚æ ¹æ“š Apache 2.0 è¨±å¯è­‰ã€‚

## TGI çš„åŠŸèƒ½

TGI åˆ—å‡ºçš„ä¸€äº›åŠŸèƒ½åŒ…æ‹¬ï¼š

- é€šéç°¡å–®çš„å•Ÿå‹•å™¨æä¾›æœ€æµè¡Œçš„å¤§å‹èªè¨€æ¨¡å‹
- å¼µé‡ä¸¦è¡Œå¯åœ¨å¤šå€‹ GPU ä¸Šå¯¦ç¾æ›´å¿«çš„æ¨ç†
- ä½¿ç”¨ Server-Sent Events (SSE) çš„ token ä¸²æµ
- [é€£çºŒæ‰¹è™•ç†å‚³å…¥è«‹æ±‚](https://github.com/huggingface/text-generation-inference/tree/main/router)ä»¥æé«˜ç¸½ååé‡
- ä½¿ç”¨ [flash-attention](https://github.com/HazyResearch/flash-attention) å’Œ [Paged Attention](https://github.com/vllm-project/vllm) é€²è¡Œæ¨è«–çš„å„ªåŒ–è½‰æ›å™¨ä»£ç¢¼
- ä½¿ç”¨ [bitsandbytes](https://github.com/TimDettmers/bitsandbytes) å’Œ [GPT-Q](https://arxiv.org/abs/2210.17323) é€²è¡Œé‡åŒ–
- æ”¯æŒ [Safetensors](https://github.com/huggingface/safetensors) æ¨¡å‹æ¬Šé‡åŠ è¼‰
- æ”¯æŒå¤§å‹èªè¨€æ¨¡å‹çš„æµ®æ°´å°åŠŸèƒ½
- Logits warperï¼ˆtemperature ç¸®æ”¾ã€top-pã€top-kã€é‡è¤‡æ‡²ç½°ï¼Œæ›´å¤šè©³ç´°ä¿¡æ¯è«‹åƒè¦‹ [Transformers.LogitsProcessor](https://huggingface.co/docs/transformers/internal/generation_utils#transformers.LogitsProcessor)ï¼‰
- åœæ­¢åºåˆ—
- æ¦‚ç‡ç”¢å‡ºçš„æ—¥èªŒ
- ç”Ÿç”¢å°±ç·’ï¼ˆä½¿ç”¨ Open Telemetryã€Prometheus æŒ‡æ¨™é€²è¡Œåˆ†ä½ˆå¼è·Ÿè¸ªï¼‰
- è‡ªå®šç¾©æç¤ºç”Ÿæˆï¼šé€šéæä¾›è‡ªå®šç¾©æç¤ºä¾†æŒ‡å°æ¨¡å‹çš„è¼¸å‡ºï¼Œè¼•é¬†ç”Ÿæˆæ–‡æœ¬ã€‚
- å¾®èª¿æ”¯æŒï¼šåˆ©ç”¨é‡å°ç‰¹å®šä»»å‹™çš„å¾®èª¿æ¨¡å‹ä¾†å¯¦ç¾æ›´é«˜çš„æº–ç¢ºæ€§å’Œæ€§èƒ½ã€‚

## å„ªåŒ– LLM æ¶æ§‹

| Name | Model Id |
|-------|---------|
|[BLOOM](https://huggingface.co/bigscience/bloom)|`bigscience/bloom`|
|[FLAN-T5](https://huggingface.co/google/flan-t5-xxl)|`google/flan-t5-xxl`|
|[Galactica](https://huggingface.co/facebook/galactica-120b)|`facebook/galactica-120b`|
|[GPT-Neox](https://huggingface.co/EleutherAI/gpt-neox-20b)|`EleutherAI/gpt-neox-20b`|
|[Llama](https://github.com/facebookresearch/llama)||
|[OPT](https://huggingface.co/facebook/opt-66b)|`facebook/opt-66b`|
|[SantaCoder](https://huggingface.co/bigcode/santacoder)|`bigcode/santacoder`|
|[Starcoder](https://huggingface.co/bigcode/starcoder)|`bigcode/starcoder`|
|[Falcon 7B](https://huggingface.co/tiiuae/falcon-7b)|`tiiuae/falcon-7b`|
|[Falcon 40B](https://huggingface.co/tiiuae/falcon-40b)|`tiiuae/falcon-40b`|
|[MPT](https://huggingface.co/mosaicml/mpt-30b)|`mosaicml/mpt-30b`|
|[Llama V2](https://huggingface.co/meta-llama)|`meta-llama/Llama-2-7b`|
|[Code Llama](https://huggingface.co/codellama)|`codellama/CodeLlama-7b-Python-hf`|

å…¶ä»– LLM æ¶æ§‹å‰‡æœƒåœ¨ best effort çš„åŸºç¤ä¸Šå¾—åˆ°æ”¯æŒï¼Œä½¿ç”¨ï¼š

```python
AutoModelForCausalLM.from_pretrained(<model>, device_map="auto")

# or

AutoModelForSeq2SeqLM.from_pretrained(<model>, device_map="auto")
```

## TGI Routers

æ¢ç´¢ TGI è·¯ç”±å™¨çš„æœ€ä½³æ–¹å¼æ˜¯ä½¿ç”¨ swagger é é¢ï¼š

- `/info` â€” [GET] â€” Text Generation Inference endpoint info
- `/metrics` â€” [GET] â€” Prometheus metrics scrape endpoint
- `/generate` â€” [POST] â€” Generate tokens
- `/generate_stream` â€” [POST] â€” Generate a stream of token using Server-Sent Events
- `/` â€” [POST] â€” Generate tokens if stream == false or a stream of token if stream == true


![](./assets/tgi-swagger.png)


## Serving

Hugging face æä¾›äº†å®¹å™¨é¡åƒä¾†è®“å¤§å®¶å¯ä¸‹è¼‰ä½¿ç”¨

```bash
docker pull ghcr.io/huggingface/text-generation-inference:1.0.3
```

## TGI Parameters

TGI æä¾›äº†è¨±å¤šçš„è¨­å®šåƒæ•¸, ä½¿ç”¨ä¸‹åˆ—æŒ‡ä»¤ä¾†åˆ—å‡ºç›¸é—œçš„åƒæ•¸èªªæ˜:

```bash
docker run ghcr.io/huggingface/text-generation-inference:1.0.3 --help
```

| é¸é … | ç’°å¢ƒè®Šæ•¸åç¨± | è¨­å®šå€¼ | èªªæ˜ |
|--------|-----|-------------|---------|
|`--model-id <MODEL_ID>`|MODEL_ID|default: bigscience/bloom-560m|è¦åŠ è¼‰çš„æ¨¡å‹çš„åç¨±ã€‚å¯ä»¥æ˜¯ <https://hf.co/models> ä¸Šåˆ—å‡ºçš„ MODEL_IDï¼Œä¾‹å¦‚ `gpt2` æˆ– `OpenAssistant/oasst-sft-1-pythia-12b`ã€‚æˆ–è€…å®ƒå¯ä»¥æ˜¯åŒ…å«ç”± transformer çš„ `save_pretrained(...)` æ–¹æ³•ä¿å­˜çš„å¿…è¦æ–‡ä»¶çš„æœ¬åœ°ç›®éŒ„ã€‚|
|`--revision <REVISION>`|REVISION||å¦‚æœä½¿ç”¨çš„æ˜¯ Hugging face Hub ä¸Šçš„æ¨¡å‹ï¼Œå‰‡ç‚ºæ¨¡å‹çš„å¯¦éš›ä¿®è¨‚ç‰ˆã€‚æ‚¨å¯ä»¥ä½¿ç”¨ç‰¹å®šçš„æäº¤ ID æˆ–åˆ†æ”¯ï¼Œä¾‹å¦‚ `refs/pr/2`ã€‚|
|`--validation-workers <VALIDATION_WORKERS>`|VALIDATION_WORKERS|default: 2|ç”¨æ–¼ router å…§éƒ¨è¨­å®šé©—è­‰ payload validation çš„ tokenizer çš„ worker æ•¸é‡ã€‚|
|`--sharded <SHARDED>`|SHARDED|possible values: true, false|æ˜¯å¦å°‡æ¨¡å‹åˆ†ç‰‡åˆ°å¤šå€‹ GPU é»˜èªæƒ…æ³ä¸‹ï¼ŒTGI å°‡ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„ GPU ä¾†é‹è¡Œæ¨¡å‹ã€‚å°‡å…¶è¨­ç½®ç‚º `false` æœƒåœç”¨ `num_shard`ã€‚|
|`--num-shard <NUM_SHARD>`|NUM_SHARD||å¦‚æœæ‚¨ä¸æƒ³ä½¿ç”¨è¨ˆç®—æ©Ÿä¸Šçš„æ‰€æœ‰ GPUï¼Œå‰‡è¦è¨­å®šåˆ†ç‰‡æ•¸é‡ã€‚æ‚¨å¯ä»¥ä½¿ç”¨`CUDA_VISIBLE_DEVICES=0,1 text- Generation-launcher... --num_shard 2` å’Œ `CUDA_VISIBLE_DEVICES=2,3 text- Generation-launcher... --num_shard 2` å•Ÿå‹• 2 å€‹å‰¯æœ¬ï¼Œæ¯å€‹å‰¯æœ¬æœ‰ 2 å€‹åˆ†ç‰‡ä¾‹å¦‚ï¼Œåœ¨å…·æœ‰ 4 å€‹ GPU çš„çµ¦å®šæ©Ÿå™¨ä¸Šã€‚|
|`--quantize <QUANTIZE>`|QUANTIZE|possible values: bitsandbytes, bitsandbytes-nf4, bitsandbytes-fp4, gptq|æ˜¯å¦å¸Œæœ›æ¨¡å‹è¢«é‡åŒ–ã€‚é€™å°‡ä½¿ç”¨`bitsandbytes` é€²è¡Œå‹•æ…‹é‡åŒ–ï¼Œæˆ– `gptq`ã€‚é€šéæä¾› `bitsandbytes-fp4` æˆ– `bitsandbytes-nf4` é¸é …ï¼Œå¯ä»¥å¯¦ç¾ 4bit é‡åŒ–ã€‚|
|`--dtype <DTYPE>`|DTYPE|possible values: float16, bfloat16|å¼·åˆ¶æ‡‰ç”¨æ–¼æ¨¡å‹çš„ dtypeã€‚æ­¤é¸é …ä¸èƒ½èˆ‡ `--quantize` ä¸€èµ·ä½¿ç”¨ã€‚|
|`--trust-remote-code`|TRUST_REMOTE_CODE|possible values: true, false|æ˜¯å¦è¦åŸ·è¡Œ Hub å»ºæ¨¡ä»£ç¢¼ã€‚åœ¨ä½¿ç”¨è‡ªå®šç¾©ä»£ç¢¼åŠ è¼‰æ¨¡å‹æ™‚ï¼Œé¼“å‹µé¡¯å¼å‚³é `revision`ï¼Œä»¥ç¢ºä¿åœ¨è¼ƒæ–°çš„ä¿®è¨‚ç‰ˆä¸­æ²’æœ‰è²¢ç»æƒ¡æ„ä»£ç¢¼ã€‚|
|`--max-concurrent-requests`|MAX_CONCURRENT_REQUESTS|default: 128|æ­¤ç‰¹å®šéƒ¨ç½²çš„æœ€å¤§ä¸¦ç™¼è«‹æ±‚æ•¸ã€‚è¨­ç½®è¼ƒä½çš„é™åˆ¶å°‡æ‹’çµ•å®¢æˆ¶ç«¯è«‹æ±‚ï¼Œè€Œä¸æ˜¯è®“ä»–å€‘ç­‰å¾…å¤ªé•·æ™‚é–“ï¼Œä¸¦ä¸”é€šå¸¸æœ‰åŠ©æ–¼æ­£ç¢ºè™•ç† backpressureã€‚|
|`--max-best-of`|MAX_BEST_OF|default: 2|é€™æ˜¯å®¢æˆ¶ç«¯è¨­ç½® `best_of` çš„æœ€å¤§å…è¨±å€¼ã€‚ Best of åŒæ™‚ç”Ÿæˆ `n` ä»£ï¼Œä¸¦è¿”å›æ•´å€‹ç”Ÿæˆåºåˆ—çš„ç¸½é«”å°æ•¸æ¦‚ç‡æ–¹é¢çš„æœ€ä½³å€¼ã€‚|
|`--max-stop-sequences`|MAX_STOP_SEQUENCES|default: 4|é€™æ˜¯å®¢æˆ¶ç«¯è¨­ç½® `stop_sequences` çš„æœ€å¤§å…è¨±å€¼ã€‚`stop_sequences` ç”¨æ–¼å…è¨±æ¨¡å‹ä¸åƒ…åƒ…åœæ­¢åœ¨ `EOS` ä»¤ç‰Œä¸Šï¼Œä¸¦å•Ÿç”¨æ›´è¤‡é›œçš„â€œæç¤ºâ€ï¼Œç”¨æˆ¶å¯ä»¥ä»¥ç‰¹å®šæ–¹å¼é å…ˆæç¤ºæ¨¡å‹ï¼Œä¸¦å®šç¾©èˆ‡æç¤ºä¸€è‡´çš„â€œè‡ªå·±çš„â€åœæ­¢ä»¤ç‰Œã€‚|
|`--max-top-n-tokens`|MAX_TOP_N_TOKENS|default: 5|é€™æ˜¯å®¢æˆ¶ç«¯è¨­ç½® `top_n_tokens` çš„æœ€å¤§å…è¨±å€¼ã€‚ `top_n_tokens` ç”¨æ–¼åœ¨æ¯å€‹ç”Ÿæˆæ­¥é©Ÿè¿”å›æœ‰é—œ `n` å€‹æœ€æœ‰å¯èƒ½çš„ä»¤ç‰Œçš„ä¿¡æ¯ï¼Œè€Œä¸åƒ…åƒ…æ˜¯æ¡æ¨£çš„ä»¤ç‰Œã€‚æ­¤ä¿¡æ¯å¯ç”¨æ–¼ä¸‹æ¸¸ä»»å‹™ï¼Œä¾‹å¦‚åˆ†é¡æˆ–æ’åã€‚|
|`--max-input-length`|MAX_INPUT_LENGTH|default: 1024|é€™æ˜¯ç”¨æˆ¶å…è¨±çš„æœ€å¤§è¼¸å…¥é•·åº¦ï¼ˆä»¥ token æ•¸é‡è¡¨ç¤ºï¼‰ã€‚è©²å€¼è¶Šå¤§ï¼Œç”¨æˆ¶å¯ä»¥ç™¼é€çš„æç¤ºè¶Šé•·ï¼Œé€™å¯èƒ½æœƒå½±éŸ¿è™•ç†è² è¼‰æ‰€éœ€çš„ç¸½é«”å…§å­˜ã€‚è«‹æ³¨æ„ï¼ŒæŸäº›æ¨¡å‹å¯ä»¥è™•ç†çš„åºåˆ—ç¯„åœæœ‰é™ã€‚|
|`--max-total-tokens`|MAX_TOTAL_TOKENS|default: 2048|é€™æ˜¯è¦è¨­ç½®çš„æœ€é‡è¦çš„å€¼ï¼Œå› ç‚ºå®ƒå®šç¾©äº†é‹è¡Œå®¢æˆ¶ç«¯è«‹æ±‚çš„ â€œå…§å­˜é ç®—â€ã€‚å®¢æˆ¶ç«¯å°‡ç™¼é€è¼¸å…¥åºåˆ—ä¸¦è¦æ±‚åœ¨é ‚éƒ¨ç”Ÿæˆ`max_new_tokens`ã€‚å€¼ç‚º `1512` çš„ç”¨æˆ¶å¯ä»¥ç™¼é€æç¤º `1000` ä¸¦è«‹æ±‚ `512` æ–°ä»¤ç‰Œï¼Œæˆ–è€…ç™¼é€æç¤º `1` ä¸¦è«‹æ±‚ `1511` max_new_tokensã€‚è©²å€¼è¶Šå¤§ï¼ŒRAM ä¸­æ¯å€‹è«‹æ±‚çš„é‡å°±è¶Šå¤§ï¼Œæ‰¹è™•ç†çš„æ•ˆç‡å°±è¶Šä½ã€‚|
|`--waiting-served-ratio`|WAITING_SERVED_RATIO|default: 1.2|é€™è¡¨ç¤ºç­‰å¾…æŸ¥è©¢èˆ‡æ­£åœ¨é‹è¡Œçš„æŸ¥è©¢çš„æ¯”ç‡ï¼Œæ‚¨å¸Œæœ›é–‹å§‹è€ƒæ…®æš«åœæ­£åœ¨é‹è¡Œçš„æŸ¥è©¢ä»¥å°‡ç­‰å¾…æŸ¥è©¢åŒ…å«åˆ°åŒä¸€æ‰¹æ¬¡ä¸­ã€‚ `waiting_served_ratio=1.2` æ„å‘³è‘—ç•¶ 12 å€‹æŸ¥è©¢æ­£åœ¨ç­‰å¾…ä¸¦ä¸”ç•¶å‰æ‰¹æ¬¡ä¸­åªå‰©ä¸‹ 10 å€‹æŸ¥è©¢æ™‚ï¼Œæˆ‘å€‘æª¢æŸ¥æ˜¯å¦å¯ä»¥å°‡é€™ 12 å€‹ç­‰å¾…æŸ¥è©¢æ”¾å…¥æ‰¹è™•ç†ç­–ç•¥ä¸­ï¼Œå¦‚æœå¯ä»¥ï¼Œå‰‡é€²è¡Œæ‰¹è™•ç†ï¼Œå°‡ 10 å€‹æ­£åœ¨é‹è¡Œçš„æŸ¥è©¢å»¶é²â€œé å¡«å……â€é‹è¡Œã€‚<br/>åƒ…ç•¶æ‰¹æ¬¡ä¸­æœ‰ `max_batch_total_tokens` å®šç¾©çš„ç©ºé–“æ™‚ï¼Œæ‰æ‡‰ç”¨æ­¤è¨­ç½®ã€‚|
|`--max-batch-prefill-tokens`|MAX_BATCH_PREFILL_TOKENS|default: 4096|é™åˆ¶é å¡«å……æ“ä½œçš„ token æ•¸é‡ã€‚ç”±æ–¼æ­¤æ“ä½œä½”ç”¨æœ€å¤šå…§å­˜ä¸¦ä¸”å—è¨ˆç®—é™åˆ¶ï¼Œå› æ­¤é™åˆ¶å¯ä»¥ç™¼é€çš„è«‹æ±‚æ•¸é‡å¾ˆæœ‰è¶£ã€‚|
|`--max-batch-total-tokens`|MAX_BATCH_TOTAL_TOKENS||**é‡è¦** é€™æ˜¯ä¸€é …é—œéµæ§åˆ¶ï¼Œå¯æœ€å¤§é™åº¦åœ°åˆ©ç”¨å¯ç”¨ç¡¬ä»¶ã€‚<br/>é€™ä»£è¡¨æ‰¹æ¬¡å…§æ½›åœ¨ token çš„ç¸½é‡ã€‚ç•¶ä½¿ç”¨å¡«å……æ™‚ï¼ˆä¸æ¨è–¦ï¼‰ï¼Œé€™ç›¸ç•¶æ–¼ `batch_size * max_total_tokens`ã€‚<br/>ç„¶è€Œï¼Œåœ¨ç„¡å¡«å……ï¼ˆflash attentionï¼‰ç‰ˆæœ¬ä¸­ï¼Œé€™å¯ä»¥æ›´å¥½ã€‚<br/>ç¸½çš„ä¾†èªªï¼Œé€™å€‹æ•¸å­—æ‡‰è©²æ˜¯é©åˆå‰©é¤˜å…§å­˜çš„æœ€å¤§å¯èƒ½æ•¸é‡ï¼ˆæ¨¡å‹åŠ è¼‰å¾Œï¼‰ã€‚ç”±æ–¼å¯¦éš›çš„å…§å­˜é–‹éŠ·å–æ±ºæ–¼å…¶ä»–åƒæ•¸ï¼Œä¾‹å¦‚æ‚¨æ˜¯å¦ä½¿ç”¨é‡åŒ–ã€é–ƒå­˜æ³¨æ„åŠ›æˆ–æ¨¡å‹å¯¦ç¾ï¼Œå› æ­¤æ–‡æœ¬ç”Ÿæˆæ¨ç†ç„¡æ³•è‡ªå‹•æ¨æ–·å‡ºè©²æ•¸å­—ã€‚|
|`--max-waiting-tokens`|MAX_WAITING_TOKENS|default: 20|æ­¤è¨­ç½®å®šç¾©åœ¨å¼·åˆ¶å°‡ç­‰å¾…æŸ¥è©¢æ”¾å…¥æ‰¹æ¬¡ä¹‹å‰å¯ä»¥å‚³éå¤šå°‘å€‹ä»¤ç‰Œï¼ˆå¦‚æœæ‰¹æ¬¡çš„å¤§å°å…è¨±ï¼‰ã€‚æ–°æŸ¥è©¢éœ€è¦ 1 å€‹â€œé å¡«å……â€è½‰ç™¼ï¼Œé€™èˆ‡â€œè§£ç¢¼â€ä¸åŒï¼Œå› æ­¤æ‚¨éœ€è¦æš«åœæ­£åœ¨é‹è¡Œçš„æ‰¹è™•ç†ï¼Œä»¥ä¾¿é‹è¡Œâ€œé å¡«å……â€ä¾†ç‚ºç­‰å¾…æŸ¥è©¢å‰µå»ºæ­£ç¢ºçš„å€¼ï¼Œä»¥ä¾¿èƒ½å¤ åŠ å…¥æ‰¹è™•ç†ã€‚<br/>å¦‚æœå€¼å¤ªå°ï¼ŒæŸ¥è©¢å°‡å§‹çµ‚â€œç«Šå–â€è¨ˆç®—ä¾†é‹è¡Œâ€œé å¡«å……â€ï¼Œä¸¦ä¸”é‹è¡ŒæŸ¥è©¢å°‡è¢«å»¶é²å¾ˆå¤šã€‚<br/>å¦‚æœå€¼å¤ªå¤§ï¼Œç­‰å¾…æŸ¥è©¢å¯èƒ½æœƒç­‰å¾…å¾ˆé•·æ™‚é–“ï¼Œç„¶å¾Œæ‰èƒ½åœ¨æ­£åœ¨é‹è¡Œçš„æ‰¹è™•ç†ä¸­ç²å¾—ä¸€å€‹ slotã€‚å¦‚æœæ‚¨çš„æœå‹™å™¨ç¹å¿™ï¼Œé€™æ„å‘³è‘—åœ¨ç©ºæœå‹™å™¨ä¸Šé‹è¡Œç´„ 2 ç§’çš„è«‹æ±‚æœ€çµ‚å¯èƒ½æœƒåœ¨ç´„ 20 ç§’å…§é‹è¡Œï¼Œå› ç‚ºæŸ¥è©¢å¿…é ˆç­‰å¾… 18 ç§’ã€‚<br/>é€™å€‹æ•¸å­—ä»¥ token æ•¸é‡è¡¨ç¤ºï¼Œä½¿å…¶èˆ‡â€œæ¨¡å‹â€æ›´åŠ ç„¡é—œï¼Œä½†çœŸæ­£é‡è¦çš„æ˜¯æœ€çµ‚ç”¨æˆ¶çš„æ•´é«”å»¶é²ã€‚|
|`--hostname`|HOSTNAME|default: 0.0.0.0|TGIæœå‹™ç›£è½çš„IPåœ°å€ã€‚|
|`--port`|PORT|default: 3000|ç›£è½çš„ç«¯å£|
|`--shard-uds-path`|SHARD_UDS_PATH|default: /tmp/text-generation-server|Web æœå‹™å™¨å’Œåˆ†ç‰‡ä¹‹é–“ç”¨æ–¼ gRPC é€šä¿¡çš„å¥—æ¥å­—çš„åç¨±ã€‚|
|`--master-addr`|MASTER_ADDR|default: localhost|ä¸»åˆ†ç‰‡å°‡ç›£è½çš„åœ°å€ã€‚(è¢« `torch distributed` ä½¿ç”¨çš„è¨­å®š)|
|`--master-port`|MASTER_PORT|default: 29500|ä¸»åˆ†ç‰‡ç«¯å£å°‡åµè½çš„åœ°å€ã€‚(è¢« `torch distributed` ä½¿ç”¨çš„è¨­å®š)|
|`--huggingface-hub-cache`|HUGGINGFACE_HUB_CACHE|default: /data|Huggingface Hub ç·©å­˜çš„ä½ç½®ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æƒ³æä¾›å·²å®‰è£çš„ç£ç›¤è·¯å¾‘ï¼Œå‰‡ç”¨æ–¼è¦†è“‹è©²ä½ç½®ã€‚|
|`--weights-cache-override`|WEIGHTS_CACHE_OVERRIDE||Huggingface é›†ç·šå™¨ç·©å­˜çš„ä½ç½®ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æƒ³æä¾›å·²å®‰è£çš„ç£ç›¤ï¼Œå‰‡ç”¨æ–¼è¦†è“‹è©²ä½ç½®ã€‚|
|`--disable-custom-kernels`|DISABLE_CUSTOM_KERNELS|default: false|å°æ–¼æŸäº›æ¨¡å‹ï¼ˆå¦‚Bloomï¼‰ï¼Œæ–‡æœ¬ç”Ÿæˆæ¨ç†å¯¦ç¾äº†è‡ªå®šç¾©cudaå…§æ ¸ä»¥åŠ é€Ÿæ¨ç†ã€‚é€™äº›å…§æ ¸åƒ…åœ¨ A100 ä¸Šé€²è¡Œäº†æ¸¬è©¦ã€‚å¦‚æœæ‚¨åœ¨ä¸åŒçš„ç¡¬ä»¶ä¸Šé‹è¡Œä¸¦é‡åˆ°å•é¡Œï¼Œè«‹ä½¿ç”¨æ­¤ flag ä¾†ç¦ç”¨å®ƒå€‘ã€‚|
|`--cuda-memory-fraction`|CUDA_MEMORY_FRACTION|default: 1.0|é™åˆ¶ CUDA å¯ç”¨å…§å­˜ã€‚å…è¨±çš„å€¼ç­‰æ–¼ç¸½å¯è¦‹å…§å­˜ä¹˜ä»¥ cuda-memory-fractionã€‚|
|`--rope-scaling`|ROPE_SCALING|possible values: linear, dynamic| Rope ç¸®æ”¾åƒ…ç”¨æ–¼ RoPE æ¨¡å‹ï¼Œä¸¦å…è¨±é‡æ–°ç¸®æ”¾ä½ç½®æ—‹è½‰ä»¥é©æ‡‰æ›´å¤§çš„ promptã€‚<br/>é€™å€‹è¨­å®šéœ€è¦èˆ‡ `rope_factor` ä¸€èµ·ä½¿ç”¨ã€‚<br/><ul><li>`--rope-factor 2.0` çµ¦å‡ºå› å­ç‚º `2.0` çš„ç·šæ€§ç¸®æ”¾</i><li>`--rope-scaling dynamic` çµ¦å‡ºå› å­ç‚º `1.0` çš„å‹•æ…‹ç¸®æ”¾</li><li>`--rope-scaling linear` çµ¦å‡ºå› å­ç‚º `1.0` çš„ç·šæ€§ç¸®æ”¾</li></ul>|
|`--rope-factor`|ROPE_FACTOR|float: 1.0, 2.0|Rope ç¸®æ”¾åƒ…ç”¨æ–¼ RoPE æ¨¡å‹, è«‹åƒé–± `rope_scaling`ã€‚|
|`--json-output`|JSON_OUTPUT||ä»¥ JSON æ ¼å¼è¼¸å‡ºæ—¥èªŒï¼ˆå°æ–¼ telemetry æœ‰ç”¨ï¼‰ã€‚|
|`--otlp-endpoint`|OTLP_ENDPOINT||å°‡è·Ÿè¸ªå’ŒæŒ‡æ¨™ç™¼é€åˆ°çš„ URL, TGI ä½¿ç”¨ opentelemetry ä¾†æ•´åˆ tracingã€‚ è«‹åƒé–±[æºç¢¼](https://github.com/huggingface/text-generation-inference/blob/main/server/text_generation_server/tracing.py)ã€‚|
|`--cors-allow-origin`|CORS_ALLOW_ORIGIN|||è¨­å®šè·¨ä¾†æºè³‡æºå…±ç”¨ (CORS) çš„å‘¼å«æ¸…å–®, ä¾‹å¦‚: `*,*.wistron.com`ã€‚|
|`--watermark-gamma`|WATERMARK_GAMMA|||
|`--watermark-delta`|WATERMARK_DELTA|||
|`--ngrok`|NGROK||å•Ÿç”¨ `ngrok` tunningã€‚|
|`--ngrok-authtoken`|NGROK_AUTHTOKEN||ngrok authentication ä»¤ç‰Œã€‚|
|`--ngrok-edge`|NGROK_EDGE||ngrok edge|

å“‡ï¼Œæœ‰å¾ˆå¤šé¸æ“‡ğŸ˜¬ï¼Œä½†é€™æ˜¯ğŸ¤—é–‹ç™¼è€…ğŸ‘¨â€ğŸ’»çš„å‡ºè‰²å·¥ä½œã€‚æ¥ä¸‹ä¾†æˆ‘å€‘å°‡ä½¿ç”¨ä¸€äº›åƒæ•¸ä½œç‚º envï¼š

```bash
# share a volume with the Docker container to avoid 
# downloading weights every run
volume=$PWD/data

# model on ğŸ¤— Hub
model=tiiuae/falcon-7b-instruct

# apply quant to reduce gpu consume
quantize=bitsandbytes
```

Falcon-7b model ä¸æ”¯æŒ Shard (Tensor Parallelism)ã€‚

```bash
num_shard=1
```

å¦å¤–

```bash
--trust-remote-code
```

å°æ–¼å…è¨±åˆ†ç‰‡çš„ Falcon-40B modelï¼Œæ‚¨æ‡‰è©²æ·»åŠ ï¼š

```bash
--sharded true --num-shard NUM_GPUS
```

`bitsandbytes` é‡åŒ–æ˜¯ç‚ºè¨“ç·´è€Œä¸æ˜¯æ¨ç†è€Œè¨­è¨ˆçš„ï¼Œé è¨ˆé‡åŒ–çš„å•Ÿç”¨æœƒæ¯”æ­£å¸¸æ²’æœ‰é‡åŒ–çš„æ¨¡å‹æ¨è«–è¦æ…¢ã€‚ GPT-Q å’Œ SQPR ä¸­çš„é‡åŒ–æ‰‹æ³•å·²ç¶“è¢«æ·»åŠ é€² TGIï¼Œæ‡‰è©²æœƒæ”¹é€² inference çš„é€Ÿåº¦ã€‚

```bash
--quantize bitsandbytes-nf4

# or

--quantize bitsandbytes-fp4
```

## åœ¨æœ¬åœ°ç’°å¢ƒä¸­é‹è¡Œ

è¦åœ¨æœ¬åœ°ç’°å¢ƒé‹è¡Œ TGI, éœ€è¦å…ˆè¡Œé…ç½® [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html) æ‰èƒ½ä½¿ç”¨ GPU ä¾†é€²è¡Œæ¨è«–ã€‚

**ç¯„ä¾‹-01**:

```bash
docker run --gpus all --shm-size 1g -p 8080:80 \ 
        -v $volume:/data \ 
        ghcr.io/huggingface/text-generation-inference:latest \
        --model-id $model --num-shard $num_shard \
        --quantize $quantize
```

**ç¯„ä¾‹-02**:

```bash
docker run --shm-size 1g -p 8080:80   -v $volume:/data   ghcr.io/huggingface/text-generation-inference:1.0.3   --model-id $model
```

**ç¯„ä¾‹-03**:

```bash
# share a volume with the Docker container to avoid 
# downloading weights every run
volume=$PWD/data

# model on ğŸ¤— Hub
model=bigscience/bloom-560m

docker run --gpus all --shm-size 1g -p 8080:80 \
  -v $volume:/data \
  ghcr.io/huggingface/text-generation-inference:1.0.3 \
  --model-id $model \
  --disable-custom-kernels \
  --sharded false \
  --quantize bitsandbytes-nf4 \
  --port 80
```

### Bash è…³æœ¬æ¸¬è©¦

å‘¼å« `/generate` çš„ API:

```bash
curl 127.0.0.1:8080/generate \
     -X POST \
     -d '{"inputs":"What is Deep Learning?","parameters":{"max_new_tokens":17}}' \
     -H 'Content-Type: application/json'
```

å‘¼å« `/generate_stream` çš„ API:

```bash
curl 127.0.0.1:8080/generate_stream \
    -X POST \
    -d '{"inputs":"What is Deep Learning?","parameters":{"max_new_tokens":17}}' \
    -H 'Content-Type: application/json'
```

å‘¼å« `/` çš„ API(å®£å‘Šè¦ä½¿ç”¨ `stream`):

```bash
curl 127.0.0.1:8080/ \
    -X POST \
    -d '{"inputs":"What is Deep Learning?", "parameters":{"max_new_tokens":17}, "stream":true}' \
    -H 'Content-Type: application/json'
```

### TGI client æ¸¬è©¦

å®‰è£å¥—ä»¶:

```bash
!pip install text-generation
```

ä½¿ç”¨ tgi clint ä¾†å‘¼å«ç›¸é—œçš„ API:

```python
from text_generation import Client

# Generate
client = Client("http://127.0.0.1:8080")
print(client.generate("What is Deep Learning?", max_new_tokens=17).generated_text)

# Generate stream
text = ""
for response in client.generate_stream("What is Deep Learning?", max_new_tokens=17):
    if not response.token.special:
        text += response.token.text
print(text)
```

### LangChain

å¦‚æœä½ çš„ Notebook æ˜¯è·‘åœ¨ Colab çš„è©±, åŸ·è¡Œä¸‹åˆ—å‘½ä»¤:

```python
# Some error in colab. fix with
import locale
locale.getpreferredencoding = lambda: "UTF-8"
```

å®‰è£å¥—ä»¶:

```bash
!pip install langchain transformers
```

å°‡ TGI ä½¿ç”¨ `HuggingFaceTextGenInference` é¡åˆ¥ä¾†åŒ…è¦†èµ·ä¾†:

```python
# Wrapper to TGI client with langchain

from langchain.llms import HuggingFaceTextGenInference

inference_server_url_local = "http://127.0.0.1:8080"

llm_local = HuggingFaceTextGenInference(
    inference_server_url=inference_server_url_local,
    max_new_tokens=400,
    top_k=10,
    top_p=0.95,
    typical_p=0.95,
    temperature=0.7,
    repetition_penalty=1.03,
)
```

è¨­å®š PromptTemplate ä¸¦æ§‹å»ºä¸€å€‹ chain:

```python
from langchain import PromptTemplate, LLMChain

template = """Question: {question}
Answer: Let's think step by step."""

prompt = PromptTemplate(
    template=template, 
    input_variables= ["question"]
)

llm_chain_local = LLMChain(prompt=prompt, llm=llm_local)
```

åŸ·è¡Œï¼š

```python
# llm_chain_local("your question")

llm_chain_local("What is Deep Learning?")
```

## æ³¨æ„äº‹é …

TGI æ˜¯ä¸€å€‹å‡ºè‰²çš„å·¥å…·ï¼Œå¯ä»¥é«˜æ•ˆé‹è¡Œå¤§å‹èªè¨€æ¨¡å‹ä¸¦åšå¥½ç”Ÿç”¢æº–å‚™ã€‚ä½†æ˜¯æ³¨æ„ç›¸é—œåƒæ•¸çš„é…ç½®èˆ‡ GPU èˆ‡æ¨¡å‹çš„å…¼å®¹æ€§
