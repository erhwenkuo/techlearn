# Create an audio dataset

æ‚¨å¯ä»¥é€šéåœ¨ Hugging Face Hub ä¸Šå‰µå»ºæ•¸æ“šé›†å­˜å„²åº«ä¾†èˆ‡æ‚¨çš„åœ˜éšŠæˆ–ç¤¾å€ä¸­çš„ä»»ä½•äººå…±äº«æ•¸æ“šé›†ï¼š

```python
from datasets import load_dataset

dataset = load_dataset("<username>/my_dataset")
```

å‰µå»ºå’Œå…±äº«éŸ³é »æ•¸æ“šé›†çš„æ–¹æ³•æœ‰å¤šç¨®ï¼š

- ä½¿ç”¨ `Dataset.push_to_hub()` å¾ python ä¸­çš„æœ¬åœ°æ–‡ä»¶å‰µå»ºéŸ³é »æ•¸æ“šé›†ã€‚é€™æ˜¯ä¸€å€‹ç°¡å–®çš„æ–¹æ³•ï¼Œåªéœ€è¦åœ¨ python ä¸­åŸ·è¡Œå¹¾å€‹æ­¥é©Ÿã€‚
- ä½¿ç”¨ `AudioFolder` æ§‹å»ºå™¨å‰µå»ºéŸ³é »æ•¸æ“šé›†å­˜å„²åº«ã€‚é€™æ˜¯ä¸€å€‹ no-code è§£æ±ºæ–¹æ¡ˆï¼Œç”¨æ–¼å¿«é€Ÿå‰µå»ºåŒ…å«æ•¸åƒå€‹éŸ³é »æ–‡ä»¶çš„éŸ³é »æ•¸æ“šé›†ã€‚
- é€šéç·¨å¯« `loading script` å‰µå»ºéŸ³é »æ•¸æ“šé›†ã€‚æ­¤æ–¹æ³•é©ç”¨æ–¼é€²éšç”¨æˆ¶ï¼Œéœ€è¦é¡å¤–æ’°å¯«ä¸€äº›ç¨‹å¼ç¢¼ï¼Œä½†æ‚¨åœ¨å¦‚ä½•å®šç¾©ã€ä¸‹è¼‰å’Œç”Ÿæˆæ•¸æ“šé›†æ–¹é¢å…·æœ‰æ›´å¤§çš„éˆæ´»æ€§ï¼Œé€™å°æ–¼æ›´è¤‡é›œæˆ–å¤§è¦æ¨¡çš„éŸ³é »æ•¸æ“šé›†éå¸¸æœ‰ç”¨ã€‚

## Local files

æ‚¨å¯ä»¥ä½¿ç”¨éŸ³é »æ–‡ä»¶çš„è·¯å¾‘åŠ è¼‰è‡ªå·±çš„æ•¸æ“šé›†ã€‚ä½¿ç”¨ `cast_column()` å‡½æ•¸ç²å–ä¸€åˆ—éŸ³é »æ–‡ä»¶è·¯å¾‘ï¼Œä¸¦å°‡å…¶æŠ•å°„åˆ° Audio featureï¼š

```python
audio_dataset = Dataset.from_dict({"audio": ["path/to/audio_1", "path/to/audio_2", ..., "path/to/audio_n"]}).cast_column("audio", Audio())

print(audio_dataset[0]["audio"])
```

çµæœ:

```bash
{'array': array([ 0.        ,  0.00024414, -0.00024414, ..., -0.00024414,
         0.        ,  0.        ], dtype=float32),
 'path': 'path/to/audio_1',
 'sampling_rate': 16000}
```

ç„¶å¾Œä½¿ç”¨ `Dataset.push_to_hub()` å°‡æ•¸æ“šé›†ä¸Šå‚³åˆ° Hugging Face Hubï¼š

```python
audio_dataset.push_to_hub("<username>/my_dataset")
```

é€™å°‡å‰µå»ºä¸€å€‹åŒ…å«æ‚¨çš„éŸ³é »æ•¸æ“šé›†çš„æ•¸æ“šé›†å­˜å„²åº«ï¼š

```bash
my_dataset/
â”œâ”€â”€ README.md
â””â”€â”€ data/
    â””â”€â”€ train-00000-of-00001.parquet
```

## AudioFolder

`AudioFolder` æ˜¯ä¸€å€‹æ•¸æ“šé›†ç”Ÿæˆå™¨ï¼Œæ—¨åœ¨å¿«é€ŸåŠ è¼‰åŒ…å«æ•¸åƒå€‹éŸ³é »æ–‡ä»¶çš„éŸ³é »æ•¸æ“šé›†ï¼Œè€Œç„¡éœ€æ‚¨ç·¨å¯«ä»»ä½•ä»£ç¢¼ã€‚åªè¦æ‚¨åœ¨å…ƒæ•¸æ“šæ–‡ä»¶ (`metadata.csv`/`metadata.jsonl`) ä¸­åŒ…å«æœ‰é—œæ•¸æ“šé›†çš„å…¶ä»–é¡å¤–ä¿¡æ¯ï¼ˆä¾‹å¦‚è½‰éŒ„ã€èªªè©±è€…å£éŸ³æˆ–èªªè©±è€…æ„åœ–ï¼‰ï¼Œ`AudioFolder` å°±æœƒè‡ªå‹•åŠ è¼‰é€™äº›ä¿¡æ¯ã€‚

!!! tip
    ğŸ’¡ æŸ¥çœ‹ [Split pattern hierarchy](https://huggingface.co/docs/datasets/v2.14.1/en/repository_structure#split-pattern-hierarchy) çµæ§‹ï¼Œäº†è§£æœ‰é—œ `AudioFolder` å¦‚ä½•æ ¹æ“šæ•¸æ“šé›†å­˜å„²åº«çµæ§‹å‰µå»ºæ•¸æ“šé›†åˆ†å‰²çš„æ›´å¤šä¿¡æ¯ã€‚

åœ¨ Hugging Face Hub ä¸Šå‰µå»ºæ•¸æ“šé›†å­˜å„²åº«ï¼Œä¸¦æŒ‰ç…§ `AudioFolder` çµæ§‹ä¸Šå‚³æ•¸æ“šé›†ç›®éŒ„ï¼š

```bash
my_dataset/
â”œâ”€â”€ README.md
â”œâ”€â”€ metadata.csv
â””â”€â”€ data/
```

æ•¸æ“šæ–‡ä»¶å¤¾å¯ä»¥æ˜¯æ‚¨æƒ³è¦çš„ä»»ä½•åç¨±ã€‚

!!! tip
    å¦‚æœæ•¸æ“šåˆ—åŒ…å«æ›´è¤‡é›œçš„æ ¼å¼ï¼ˆå¦‚æµ®é»æ•¸åˆ—è¡¨ï¼‰ï¼Œå‰‡å°‡å…ƒæ•¸æ“šå­˜å„²ç‚º `jsonl` æ–‡ä»¶æœƒå¾ˆæœ‰å¹«åŠ©ï¼Œä»¥é¿å…è§£æéŒ¯èª¤æˆ–å°‡å¾©é›œå€¼è®€å–ç‚ºå­—ç¬¦ä¸²ã€‚

å…ƒæ•¸æ“šæ–‡ä»¶æ‡‰åŒ…å« `file_name` åˆ—ï¼Œä»¥å°‡éŸ³é »æ–‡ä»¶éˆæ¥åˆ°å…¶å…ƒæ•¸æ“šï¼š

```csv title="metadata.csv"
file_name,transcription
data/first_audio_file.mp3,znowu siÄ™ duch z ciaÅ‚em zroÅ›nie w mÅ‚odocianej wstaniesz wiosnie i moÅ¼esz skutkiem tych lekÃ³w umieraÄ‡ wstawaÄ‡ wiek wiekÃ³w dalej tam byÅ‚y przestrogi jak siekaÄ‡ gÅ‚owÄ™ jak nogi
data/second_audio_file.mp3,juÅ¼ u ÅºwierzyÅ„ca podwojÃ³w krÃ³l zasiada przy nim ksiÄ…Å¼Ä™ta i panowie rada a gdzie wzniosÅ‚y krÄ…Å¼yÅ‚ ganek rycerze obok kochanek krÃ³l skinÄ…Å‚ palcem zaczÄ™to igrzysko
data/third_audio_file.mp3,pewnie kÄ™dyÅ› w obÅ‚Ä™dzie ubite minÄ™Å‚y szlaki zaczekajmy dzieÅ„ jaki poÅ›lemy szukaÄ‡ wszÄ™dzie dziÅ› jutro pewnie bÄ™dzie posÅ‚ali wszÄ™dzie sÅ‚ugi czekali dzieÅ„ i drugi gdy nic nie doczekali z pÅ‚aczem chcÄ… jechaÄ‡ dali
```

ç„¶å¾Œæ‚¨å¯ä»¥å°‡æ•¸æ“šé›†å­˜å„²åœ¨å¦‚ä¸‹ç›®éŒ„çµæ§‹ä¸­ï¼š

```bash
metadata.csv
data/first_audio_file.mp3
data/second_audio_file.mp3
data/third_audio_file.mp3
```

ç”¨æˆ¶ç¾åœ¨å¯ä»¥é€šéåœ¨ `load_dataset()` ä¸­æŒ‡å®šéŸ³é »æ–‡ä»¶å¤¾å’Œåœ¨ `data_dir` ä¸­æŒ‡å®šæ•¸æ“šé›†ç›®éŒ„ä¾†åŠ è¼‰æ•¸æ“šé›†å’Œé—œè¯çš„å…ƒæ•¸æ“šï¼š

```python
from datasets import load_dataset

dataset = load_dataset("audiofolder", data_dir="/path/to/data")

print(dataset["train"][0])
```

çµæœ:

```bash
{'audio':
    {'path': '/path/to/extracted/audio/first_audio_file.mp3',
    'array': array([ 0.00088501,  0.0012207 ,  0.00131226, ..., -0.00045776, -0.00054932, -0.00054932], dtype=float32),
    'sampling_rate': 16000},
 'transcription': 'znowu siÄ™ duch z ciaÅ‚em zroÅ›nie w mÅ‚odocianej wstaniesz wiosnie i moÅ¼esz skutkiem tych lekÃ³w umieraÄ‡ wstawaÄ‡ wiek wiekÃ³w dalej tam byÅ‚y przestrogi jak siekaÄ‡ gÅ‚owÄ™ jak nogi'
}
```

æ‚¨é‚„å¯ä»¥ä½¿ç”¨ `audiofolder` åŠ è¼‰æ¶‰åŠå¤šå€‹åˆ†å‰²çš„æ•¸æ“šé›†ã€‚ç‚ºæ­¤ï¼Œæ‚¨çš„æ•¸æ“šé›†ç›®éŒ„å¯èƒ½å…·æœ‰ä»¥ä¸‹çµæ§‹ï¼š

```bash
data/train/first_train_audio_file.mp3
data/train/second_train_audio_file.mp3

data/test/first_test_audio_file.mp3
data/test/second_test_audio_file.mp3
```

!!! warning
    è«‹æ³¨æ„ï¼Œå¦‚æœéŸ³é »æ–‡ä»¶ä¸ä½æ–¼å…ƒæ•¸æ“šæ–‡ä»¶æ—é‚Šï¼Œå‰‡ `file_name` åˆ—æ‡‰è©²æ˜¯éŸ³é »æ–‡ä»¶çš„å®Œæ•´ç›¸å°è·¯å¾‘ï¼Œè€Œä¸åƒ…åƒ…æ˜¯å…¶æ–‡ä»¶åã€‚

å°æ–¼æ²’æœ‰ä»»ä½•é—œè¯å…ƒæ•¸æ“šçš„éŸ³é »æ•¸æ“šé›†ï¼Œ`AudioFolder` æœƒæ ¹æ“šç›®éŒ„åç¨±è‡ªå‹•æ¨æ–·æ•¸æ“šé›†çš„é¡æ¨™ç±¤ã€‚å®ƒå¯èƒ½å°éŸ³é »åˆ†é¡ä»»å‹™æœ‰ç”¨ã€‚æ‚¨çš„æ•¸æ“šé›†ç›®éŒ„å¯èƒ½å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
data/train/electronic/01.mp3
data/train/punk/01.mp3

data/test/electronic/09.mp3
data/test/punk/09.mp3
```

ä½¿ç”¨ `AudioFolder` åŠ è¼‰æ•¸æ“šé›†ï¼Œå®ƒå°‡æ ¹æ“šç›®éŒ„åç¨±ï¼ˆèªè¨€ IDï¼‰å‰µå»ºä¸€å€‹æ¨™ç±¤åˆ—ï¼š

```python
from datasets import load_dataset

dataset = load_dataset("audiofolder", data_dir="/path/to/data")

print(dataset["train"][0])

print(dataset["train"][-1])
```

çµæœ:

```bash
{'audio':
    {'path': '/path/to/electronic/01.mp3',
     'array': array([ 3.9714024e-07,  7.3031038e-07,  7.5640685e-07, ...,
         -1.1963668e-01, -1.1681189e-01, -1.1244172e-01], dtype=float32),
     'sampling_rate': 44100},
 'label': 0  # "electronic"
}

{'audio':
    {'path': '/path/to/punk/01.mp3',
     'array': array([0.15237972, 0.13222949, 0.10627693, ..., 0.41940814, 0.37578005,
         0.33717662], dtype=float32),
     'sampling_rate': 44100},
 'label': 1  # "punk"
}
```

!!! warning
    å¦‚æœæ‰€æœ‰éŸ³é »æ–‡ä»¶éƒ½åŒ…å«åœ¨ä¸€å€‹ç›®éŒ„ä¸­æˆ–è€…å®ƒå€‘ä¸åœ¨åŒä¸€ç´šåˆ¥çš„ç›®éŒ„çµæ§‹ä¸­ï¼Œå‰‡ä¸æœƒè‡ªå‹•æ·»åŠ æ¨™ç±¤åˆ—ã€‚å¦‚æœéœ€è¦ï¼Œè«‹æ˜ç¢ºè¨­ç½® `drop_labels=False`ã€‚

!!! tip
    ä¸€äº›éŸ³é »æ•¸æ“šé›†ï¼ˆä¾‹å¦‚ Kaggle ç«¶è³½ä¸­çš„æ•¸æ“šé›†ï¼‰å°æ–¼æ¯å€‹åˆ†å‰²éƒ½æœ‰å–®ç¨çš„å…ƒæ•¸æ“šæ–‡ä»¶ã€‚å¦‚æœæ¯å€‹åˆ†å‰²çš„å…ƒæ•¸æ“šåŠŸèƒ½ç›¸åŒï¼Œå‰‡å¯ä»¥ä½¿ç”¨éŸ³é »æ–‡ä»¶å¤¾ä¸€æ¬¡æ€§åŠ è¼‰æ‰€æœ‰åˆ†å‰²ã€‚å¦‚æœæ¯å€‹æ‹†åˆ†çš„å…ƒæ•¸æ“šåŠŸèƒ½ä¸åŒï¼Œæ‚¨æ‡‰è©²ä½¿ç”¨å–®ç¨çš„ `load_dataset()` èª¿ç”¨ä¾†åŠ è¼‰å®ƒå€‘ã€‚


## Loading script

ç·¨å¯«æ•¸æ“šé›†åŠ è¼‰è…³æœ¬ä¾†æ‰‹å‹•å‰µå»ºæ•¸æ“šé›†ã€‚å®ƒå®šç¾©æ•¸æ“šé›†çš„åˆ†å‰²å’Œé…ç½®ï¼Œä¸¦è™•ç†ä¸‹è¼‰å’Œç”Ÿæˆæ•¸æ“šé›†ç¤ºä¾‹ã€‚è©²è…³æœ¬æ‡‰èˆ‡æ‚¨çš„æ•¸æ“šé›†æ–‡ä»¶å¤¾æˆ–å­˜å„²åº«å…·æœ‰ç›¸åŒçš„åç¨±ï¼š

```bash
my_dataset/
â”œâ”€â”€ README.md
â”œâ”€â”€ my_dataset.py
â””â”€â”€ data/
```

æ•¸æ“šæ–‡ä»¶å¤¾ `data` å¯ä»¥æ˜¯æ‚¨æƒ³è¦çš„ä»»ä½•åç¨±ï¼Œä¸ä¸€å®šè¦æ˜¯ `data`ã€‚è©²æ–‡ä»¶å¤¾æ˜¯å¯é¸çš„ï¼Œé™¤éæ‚¨åœ¨ Hub ä¸Šè¨—ç®¡æ•¸æ“šé›†ã€‚

æ­¤ç›®éŒ„çµæ§‹å…è¨±æ‚¨çš„æ•¸æ“šé›†åœ¨ä¸€è¡Œä¸­åŠ è¼‰ï¼š

```python
from datasets import load_dataset

dataset = load_dataset("path/to/my_dataset")
```

æœ¬æŒ‡å—å°‡å‘æ‚¨å±•ç¤ºå¦‚ä½•ç‚ºéŸ³é »æ•¸æ“šé›†å‰µå»ºæ•¸æ“šé›†åŠ è¼‰è…³æœ¬ï¼Œé€™èˆ‡ç‚ºæ–‡æœ¬æ•¸æ“šé›†å‰µå»ºåŠ è¼‰è…³æœ¬æœ‰é»ä¸åŒã€‚éŸ³é »æ•¸æ“šé›†é€šå¸¸å­˜å„²åœ¨ `tar.gz` æª”æ¡ˆä¸­ï¼Œé€™éœ€è¦ç‰¹å®šçš„æ–¹æ³•ä¾†æ”¯æŒæµæ¨¡å¼ã€‚é›–ç„¶ä¸éœ€è¦æµå¼å‚³è¼¸ï¼Œä½†æˆ‘å€‘å¼·çƒˆé¼“å‹µåœ¨éŸ³é »æ•¸æ“šé›†ä¸­å¯¦ç¾æµå¼å‚³è¼¸æ”¯æŒï¼Œå› ç‚ºï¼š

1. æ²’æœ‰å¤§é‡ç£ç›¤ç©ºé–“çš„ç”¨æˆ¶ç„¡éœ€ä¸‹è¼‰å³å¯ä½¿ç”¨æ‚¨çš„æ•¸æ“šé›†ã€‚åœ¨ [Stream æŒ‡å—](https://huggingface.co/docs/datasets/v2.14.1/en/stream) ä¸­äº†è§£æœ‰é—œ `streaming` çš„æ›´å¤šä¿¡æ¯ï¼
2. ç”¨æˆ¶å¯ä»¥åœ¨æ•¸æ“šé›†æŸ¥çœ‹å™¨ä¸­é è¦½æ•¸æ“šé›†ã€‚

ä»¥ä¸‹æ˜¯ä½¿ç”¨ TAR å­˜æª”çš„ç¯„ä¾‹ï¼š

```bash
my_dataset/
â”œâ”€â”€ README.md
â”œâ”€â”€ my_dataset.py
â””â”€â”€ data/
    â”œâ”€â”€ train.tar.gz
    â”œâ”€â”€ test.tar.gz
    â””â”€â”€ metadata.csv
```

é™¤äº†å­¸ç¿’å¦‚ä½•å‰µå»º streamable dataset ä¹‹å¤–ï¼Œæ‚¨é‚„å°‡å­¸ç¿’å¦‚ä½•ï¼š

- æ§‹å»ºä¸€å€‹ dataset builder class
- æ§‹å»º dataset configurations
- å¢åŠ  dataset metadata
- ä¸‹è¼‰ä¸¦å®šç¾© dataset splits
- æ§‹å»º dataset.
- ä¸Šå‚³ dataset åˆ° Hub

æœ€å¥½çš„å­¸ç¿’æ–¹æ³•æ˜¯æ‰“é–‹ç¾æœ‰çš„éŸ³é »æ•¸æ“šé›†åŠ è¼‰è…³æœ¬ï¼Œä¾‹å¦‚ [Vivos](https://huggingface.co/datasets/vivos/blob/main/vivos.py)ï¼Œç„¶å¾Œå­¸ç¿’ç›¸é—œçš„ç¨‹å¼æ’°å¯«ï¼

!!! tip
    æœ¬æŒ‡å—ä»‹ç´¹å¦‚ä½•è™•ç†å­˜å„²åœ¨ TAR å­˜æª”ä¸­çš„éŸ³é »æ•¸æ“š - é€™æ˜¯éŸ³é »æ•¸æ“šé›†æœ€å¸¸è¦‹çš„æƒ…æ³ã€‚æŸ¥çœ‹ [Minds14](https://huggingface.co/datasets/PolyAI/minds14/blob/main/minds14.py) æ•¸æ“šé›†ï¼Œç²å–ä½¿ç”¨ ZIP å­˜æª”çš„éŸ³é »è…³æœ¬ç¤ºä¾‹ã€‚

!!! info
    ç‚ºäº†å¹«åŠ©æ‚¨å…¥é–€ï¼Œæˆ‘å€‘å‰µå»ºäº†ä¸€å€‹ [loading script template](https://github.com/huggingface/datasets/blob/main/templates/new_dataset_script.py)ï¼Œæ‚¨å¯ä»¥å¾©åˆ¶ä¸¦ç”¨ä½œèµ·é»ï¼

### Create a dataset builder class

[GeneratorBasedBuilder](https://huggingface.co/docs/datasets/v2.14.1/en/package_reference/builder_classes#datasets.GeneratorBasedBuilder) æ˜¯å­—å…¸ç”Ÿæˆå™¨ç”Ÿæˆçš„æ•¸æ“šé›†çš„åŸºé¡ã€‚åœ¨æ­¤é¡ä¸­ï¼Œæä¾›äº†ä¸‰ç¨®æ–¹æ³•ä¾†å¹«åŠ©å‰µå»ºæ•¸æ“šé›†ï¼š

- `_info` å­˜å„²æœ‰é—œæ•¸æ“šé›†çš„ä¿¡æ¯ï¼Œä¾‹å¦‚å…¶æè¿°ã€è¨±å¯è­‰å’ŒåŠŸèƒ½ã€‚
- `_split_generators` ä¸‹è¼‰æ•¸æ“šé›†ä¸¦å®šç¾©å…¶åˆ†å‰²ã€‚
- `_generate_examples` ç”Ÿæˆæ•¸æ“šé›†çš„æ¨£æœ¬ï¼Œå…¶ä¸­åŒ…å«æ¯å€‹åˆ†å‰²çš„éŸ³é »æ•¸æ“šå’Œ info ä¸­æŒ‡å®šçš„å…¶ä»–ç‰¹å¾µã€‚

é¦–å…ˆå‰µå»ºæ•¸æ“šé›†é¡ä½œç‚º `GeneratorBasedBuilder` çš„å­é¡ä¸¦æ·»åŠ ä¸‰å€‹æ–¹æ³•ã€‚ä¸ç”¨æ“”å¿ƒå¡«å¯«é€™äº›æ–¹æ³•ï¼Œæ‚¨å°‡åœ¨æ¥ä¸‹ä¾†çš„å¹¾ç¯€ä¸­é–‹ç™¼é€™äº›æ–¹æ³•ï¼š

```python
class VivosDataset(datasets.GeneratorBasedBuilder):
    """VIVOS is a free Vietnamese speech corpus consisting of 15 hours of recording speech prepared for
    Vietnamese Automatic Speech Recognition task."""

    def _info(self):

    def _split_generators(self, dl_manager):

    def _generate_examples(self, prompts_path, path_to_clips, audio_files):
```

#### Multiple configurations

åœ¨æŸäº›æƒ…æ³ä¸‹ï¼Œæ•¸æ“šé›†å¯èƒ½å…·æœ‰å¤šå€‹é…ç½®ã€‚ä¾‹å¦‚ï¼Œ[LibriVox Indonesia](https://huggingface.co/datasets/indonesian-nlp/librivox-indonesia) æ•¸æ“šé›†æœ‰å¹¾ç¨®å°æ‡‰ä¸åŒèªè¨€çš„é…ç½®ã€‚

è¦å‰µå»ºä¸åŒçš„é…ç½®ï¼Œè«‹ä½¿ç”¨ `BuilderConfig` é¡åˆ¥å‰µå»ºæ•¸æ“šé›†çš„å­é¡ã€‚å”¯ä¸€éœ€è¦çš„åƒæ•¸æ˜¯é…ç½®çš„åç¨±ï¼Œå®ƒå¿…é ˆå‚³éçµ¦é…ç½®çš„è¶…é¡ `__init__()`ã€‚å¦å‰‡ï¼Œæ‚¨å¯ä»¥åœ¨é…ç½®é¡ä¸­æŒ‡å®šæ‰€éœ€çš„ä»»ä½•è‡ªå®šç¾©åƒæ•¸ã€‚

```python
class LibriVoxIndonesiaConfig(datasets.BuilderConfig):
    """BuilderConfig for LibriVoxIndonesia."""

    def __init__(self, name, version, **kwargs):
        self.language = kwargs.pop("language", None)
        self.release_date = kwargs.pop("release_date", None)
        self.num_clips = kwargs.pop("num_clips", None)
        self.num_speakers = kwargs.pop("num_speakers", None)
        self.validated_hr = kwargs.pop("validated_hr", None)
        self.total_hr = kwargs.pop("total_hr", None)
        self.size_bytes = kwargs.pop("size_bytes", None)
        self.size_human = size_str(self.size_bytes)

        description = (
            f"LibriVox-Indonesia speech to text dataset in {self.language} released on {self.release_date}. "
            f"The dataset comprises {self.validated_hr} hours of transcribed speech data"
        )

        super(LibriVoxIndonesiaConfig, self).__init__(
            name=name,
            version=datasets.Version(version),
            description=description,
            **kwargs,
        )
```

åœ¨ `GeneratorBasedBuilder` å…§çš„ `BUILDER_CONFIGS` é¡åˆ¥è®Šé‡ä¸­å®šç¾©é…ç½®ã€‚åœ¨æ­¤ç¤ºä¾‹ä¸­ï¼Œä½œè€…å¾å…¶å­˜å„²åº«ä¸­çš„å–®ç¨çš„ `release_stats.py` æ–‡ä»¶å°å…¥èªè¨€ï¼Œç„¶å¾Œå¾ªç’°éæ­·æ¯ç¨®èªè¨€ä»¥å‰µå»ºé…ç½®ï¼š

```python
class LibriVoxIndonesiaConfig(datasets.GeneratorBasedBuilder):
    DEFAULT_CONFIG_NAME = "all"

    BUILDER_CONFIGS = [
        LibriVoxIndonesiaConfig(
            name=lang,
            version=STATS["version"],
            language=LANGUAGES[lang],
            release_date=STATS["date"],
            num_clips=lang_stats["clips"],
            num_speakers=lang_stats["users"],
            total_hr=float(lang_stats["totalHrs"]) if lang_stats["totalHrs"] else None,
            size_bytes=int(lang_stats["size"]) if lang_stats["size"] else None,
        )
        for lang, lang_stats in STATS["locales"].items()
    ]
```

!!! info
    é€šå¸¸ï¼Œç”¨æˆ¶éœ€è¦æŒ‡å®šè¦åœ¨ `load_dataset()` ä¸­åŠ è¼‰çš„é…ç½®ï¼Œå¦å‰‡æœƒå¼•ç™¼ `ValueError`ã€‚æ‚¨å¯ä»¥é€šéè¨­ç½®è¦åœ¨ `DEFAULT_CONFIG_NAME` ä¸­åŠ è¼‰çš„é»˜èªæ•¸æ“šé›†é…ç½®ä¾†é¿å…é€™ç¨®æƒ…æ³ã€‚

ç¾åœ¨ï¼Œå¦‚æœç”¨æˆ¶æƒ³è¦åŠ è¼‰ Balineseï¼ˆbalï¼‰é…ç½®ï¼Œä»–å€‘å¯ä»¥ä½¿ç”¨é…ç½®åç¨±ï¼š

```python
from datasets import load_dataset

dataset = load_dataset("indonesian-nlp/librivox-indonesia", "bal", split="train")
```

### Add dataset metadata

æ·»åŠ æœ‰é—œæ•¸æ“šé›†çš„ä¿¡æ¯å¯ä»¥å¹«åŠ©ç”¨æˆ¶äº†è§£æ›´å¤šä¿¡æ¯ã€‚æ­¤ä¿¡æ¯å­˜å„²åœ¨ç”± `info` æ–¹æ³•è¿”å›çš„ `DatasetInfo` é¡åˆ¥ä¸­ã€‚ç”¨æˆ¶å¯ä»¥é€šéä»¥ä¸‹æ–¹å¼è¨ªå•æ­¤ä¿¡æ¯ï¼š

```python
from datasets import load_dataset_builder

ds_builder = load_dataset_builder("vivos")

print(ds_builder.info)
```

æ‚¨å¯ä»¥åŒ…å«å¾ˆå¤šæœ‰é—œæ•¸æ“šé›†çš„ä¿¡æ¯ï¼Œä½†å…¶ä¸­ä¸€äº›é‡è¦ä¿¡æ¯æ˜¯ï¼š

1. `description` æä¾›æ•¸æ“šé›†çš„ç°¡æ½”æè¿°ã€‚
2. `features` æŒ‡å®šæ•¸æ“šé›†åˆ—é¡å‹ã€‚ç”±æ–¼æ‚¨æ­£åœ¨å‰µå»ºéŸ³é »åŠ è¼‰è…³æœ¬ï¼Œå› æ­¤æ‚¨éœ€è¦åŒ…å«éŸ³é »åŠŸèƒ½å’Œæ•¸æ“šé›†çš„ `sampling_rate`ã€‚
3. `homepage` æä¾›æ•¸æ“šé›†ä¸»é çš„éˆæ¥ã€‚
4. `license` æŒ‡å®šä½¿ç”¨è¨±å¯è­‰é¡å‹å®šç¾©çš„æ•¸æ“šé›†çš„æ¬Šé™ã€‚
5. `citation` æ˜¯æ•¸æ“šé›†çš„ BibTeX å¼•ç”¨ã€‚

!!! info
    æ‚¨æœƒæ³¨æ„åˆ°è¨±å¤šæ•¸æ“šé›†ä¿¡æ¯æ˜¯åœ¨ `loading script` è£¡é¢å®šç¾©çš„ï¼Œé€™å¯ä»¥ä½¿ dataset å…¶æ›´æ˜“æ–¼è¢«äººäº†è§£æ­Ÿä½¿ç”¨ã€‚æ‚¨é‚„å¯ä»¥è¼¸å…¥å…¶ä»– `Dataset.Features`ï¼Œå› æ­¤è«‹å‹™å¿…æŸ¥çœ‹å®Œæ•´åˆ—è¡¨å’Œ [features guide](https://huggingface.co/docs/datasets/v2.14.1/en/about_dataset_features) ä»¥äº†è§£æ›´å¤šè©³ç´°ä¿¡æ¯ã€‚

```python
def _info(self):
    return datasets.DatasetInfo(
        description=_DESCRIPTION,
        features=datasets.Features(
            {
                "speaker_id": datasets.Value("string"),
                "path": datasets.Value("string"),
                "audio": datasets.Audio(sampling_rate=16_000),
                "sentence": datasets.Value("string"),
            }
        ),
        supervised_keys=None,
        homepage=_HOMEPAGE,
        license=_LICENSE,
        citation=_CITATION,
    )
```

### Download and define the dataset splits

ç¾åœ¨æ‚¨å·²ç¶“æ·»åŠ äº†æœ‰é—œæ•¸æ“šé›†çš„ä¸€äº›ä¿¡æ¯ï¼Œä¸‹ä¸€æ­¥æ˜¯ä¸‹è¼‰æ•¸æ“šé›†ä¸¦å®šç¾©åˆ†å‰²ã€‚

1. ä½¿ç”¨ `download()` æ–¹æ³•ä¸‹è¼‰ `_PROMPTS_URLS` è™•çš„å…ƒæ•¸æ“šæ–‡ä»¶å’Œ `_DATA_URL` è™•çš„éŸ³é » TAR å­˜æª”ã€‚æ­¤æ–¹æ³•è¿”å›æœ¬åœ°æ–‡ä»¶/å­˜æª”çš„è·¯å¾‘ã€‚åœ¨æµæ¨¡å¼ä¸‹ï¼Œå®ƒä¸æœƒä¸‹è¼‰æ–‡ä»¶ï¼Œåªæ˜¯è¿”å›ä¸€å€‹ URL ä»¥å¾ä¸­æµå¼å‚³è¼¸æ•¸æ“šã€‚è©²æ–¹æ³•æ¥å—ï¼š

    - Hub æ•¸æ“šé›†å­˜å„²åº«å…§æ–‡ä»¶çš„ç›¸å°è·¯å¾‘ï¼ˆä¾‹å¦‚ï¼Œåœ¨ `data/` æ–‡ä»¶å¤¾ä¸­ï¼‰
    - è¨—ç®¡åœ¨å…¶ä»–åœ°æ–¹çš„æ–‡ä»¶çš„ URL
    - æ–‡ä»¶åæˆ– URL çš„ï¼ˆåµŒå¥—ï¼‰åˆ—è¡¨æˆ–å­—å…¸

2. ä¸‹è¼‰æ•¸æ“šé›†å¾Œï¼Œä½¿ç”¨ `SplitGenerator` ä¾†çµ„ç¹”æ¯å€‹åˆ†å‰²ä¸­çš„éŸ³é »æ–‡ä»¶å’Œå¥å­æç¤ºã€‚ä½¿ç”¨æ¨™æº–åç¨±å‘½åæ¯å€‹ splitï¼Œä¾‹å¦‚ï¼š`Split.TRAIN`ã€`Split.TEST` å’Œ `SPLIT.Validation`ã€‚

    åœ¨ `gen_kwargs` åƒæ•¸ä¸­ï¼ŒæŒ‡å®š `prompts_path` å’Œ `path_to_clips` çš„æ–‡ä»¶è·¯å¾‘ã€‚å°æ–¼ `audio_files`ï¼Œæ‚¨éœ€è¦ä½¿ç”¨ `iter_archive()` ä¾†è¿­ä»£ TAR å­˜æª”ä¸­çš„éŸ³é »æ–‡ä»¶ã€‚é€™å°‡ç‚ºæ‚¨çš„æ•¸æ“šé›†å•Ÿç”¨æµå¼å‚³è¼¸ã€‚æ‰€æœ‰é€™äº›æ–‡ä»¶è·¯å¾‘éƒ½å°‡å‚³éåˆ°ä¸‹ä¸€æ­¥ï¼Œæ‚¨å°‡åœ¨å…¶ä¸­å¯¦éš›ç”Ÿæˆæ•¸æ“šé›†ã€‚

    ```python
    def _split_generators(self, dl_manager):
        """Returns SplitGenerators."""
        prompts_paths = dl_manager.download(_PROMPTS_URLS)
        archive = dl_manager.download(_DATA_URL)
        train_dir = "vivos/train"
        test_dir = "vivos/test"

        return [
            datasets.SplitGenerator(
                name=datasets.Split.TRAIN,
                gen_kwargs={
                    "prompts_path": prompts_paths["train"],
                    "path_to_clips": train_dir + "/waves",
                    "audio_files": dl_manager.iter_archive(archive),
                },
            ),
            datasets.SplitGenerator(
                name=datasets.Split.TEST,
                gen_kwargs={
                    "prompts_path": prompts_paths["test"],
                    "path_to_clips": test_dir + "/waves",
                    "audio_files": dl_manager.iter_archive(archive),
                },
            ),
        ]
    ```

!!! tip
    æ­¤å¯¦ç¾ä¸æœƒæå–ä¸‹è¼‰çš„æª”æ¡ˆã€‚å¦‚æœæ‚¨æƒ³åœ¨ä¸‹è¼‰å¾Œè§£å£“æ–‡ä»¶ï¼Œå‰‡éœ€è¦å¦å¤–ä½¿ç”¨ `extract()`ï¼Œè«‹ [(Advanced) Extract TAR archives](https://huggingface.co/docs/datasets/v2.14.1/en/audio_dataset#advanced-extract-tar-archives-locally)ã€‚


### Generate the dataset

`GeneratorBasedBuilder` é¡åˆ¥ä¸­çš„æœ€å¾Œä¸€å€‹æ–¹æ³•å¯¦éš›ä¸Šç”Ÿæˆæ•¸æ“šé›†ä¸­çš„æ¨£æœ¬ã€‚å®ƒæ ¹æ“š `info` æ–¹æ³•ä¸­çš„ç‰¹å¾µä¸­æŒ‡å®šçš„çµæ§‹ç”Ÿæˆæ•¸æ“šé›†ã€‚å¦‚æ‚¨æ‰€è¦‹ï¼Œ`generate_examples` æ¥å—ä¸Šä¸€å€‹æ–¹æ³•ä¸­çš„`prompts_path`ã€`path_to_clips` å’Œ `audio_files` ä½œç‚ºåƒæ•¸ã€‚

TAR å­˜æª”å…§çš„æ–‡ä»¶æ˜¯æŒ‰é †åºè¨ªå•å’Œç”Ÿæˆçš„ã€‚é€™æ„å‘³è‘—æ‚¨éœ€è¦é¦–å…ˆæ“æœ‰èˆ‡ TAR æ–‡ä»¶ä¸­çš„éŸ³é »æ–‡ä»¶é—œè¯çš„å…ƒæ•¸æ“šï¼Œä»¥ä¾¿æ‚¨å¯ä»¥ç”Ÿæˆå®ƒåŠå…¶ç›¸æ‡‰çš„éŸ³é »æ–‡ä»¶ã€‚

```python
examples = {}

with open(prompts_path, encoding="utf-8") as f:
    for row in f:
        data = row.strip().split(" ", 1)
        speaker_id = data[0].split("_")[0]
        audio_path = "/".join([path_to_clips, speaker_id, data[0] + ".wav"])
        examples[audio_path] = {
            "speaker_id": speaker_id,
            "path": audio_path,
            "sentence": data[1],
        }
```

æœ€å¾Œï¼Œè¿­ä»£ `audio_files` ä¸­çš„æ–‡ä»¶ä¸¦ç”Ÿæˆå®ƒå€‘åŠå…¶ç›¸æ‡‰çš„å…ƒæ•¸æ“šã€‚ `iter_archive()` ç”Ÿæˆä¸€å€‹ `(path, f)` å…ƒçµ„ï¼Œå…¶ä¸­ `path` æ˜¯ TAR å­˜æª”å…§æ–‡ä»¶çš„ç›¸å°è·¯å¾‘ï¼Œ`f` æ˜¯æ–‡ä»¶å°è±¡æœ¬èº«ã€‚

```python
inside_clips_dir = False

id_ = 0

for path, f in audio_files:
    if path.startswith(path_to_clips):
        inside_clips_dir = True
        if path in examples:
            audio = {"path": path, "bytes": f.read()}
            yield id_, {**examples[path], "audio": audio}
            id_ += 1
    elif inside_clips_dir:
        break
```

å°‡é€™å…©å€‹æ­¥é©Ÿæ”¾åœ¨ä¸€èµ·ï¼Œæ•´å€‹ `_generate_examples` æ–¹æ³•å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
def _generate_examples(self, prompts_path, path_to_clips, audio_files):
    """Yields examples as (key, example) tuples."""
    examples = {}
    with open(prompts_path, encoding="utf-8") as f:
        for row in f:
            data = row.strip().split(" ", 1)
            speaker_id = data[0].split("_")[0]
            audio_path = "/".join([path_to_clips, speaker_id, data[0] + ".wav"])
            examples[audio_path] = {
                "speaker_id": speaker_id,
                "path": audio_path,
                "sentence": data[1],
            }

    inside_clips_dir = False
    id_ = 0

    for path, f in audio_files:
        if path.startswith(path_to_clips):
            inside_clips_dir = True
            if path in examples:
                audio = {"path": path, "bytes": f.read()}
                yield id_, {**examples[path], "audio": audio}
                id_ += 1
        elif inside_clips_dir:
            break
```

### Upload the dataset to the Hub

è…³æœ¬æº–å‚™å°±ç·’å¾Œï¼Œ[create a dataset card](https://huggingface.co/docs/datasets/v2.14.1/en/dataset_card) ä¸¦å°‡å…¶ [upload it to the Hub](https://huggingface.co/docs/datasets/v2.14.1/en/share)ã€‚

æ­å–œï¼Œæ‚¨ç¾åœ¨å¯ä»¥å¾ Hub åŠ è¼‰æ•¸æ“šé›†ï¼

```python
from datasets import load_dataset

load_dataset("<username>/my_dataset")
```

### (Advanced) Extract TAR archives locally

åœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œæœªæå–ä¸‹è¼‰çš„å­˜æª”ï¼Œå› æ­¤ç¤ºä¾‹ä¸åŒ…å«æœ‰é—œå®ƒå€‘æœ¬åœ°å­˜å„²ä½ç½®çš„ä¿¡æ¯ã€‚ç‚ºäº†è§£é‡‹å¦‚ä½•ä»¥æ”¯æŒæµçš„æ–¹å¼é€²è¡Œæå–ï¼Œæˆ‘å€‘å°‡ç°¡è¦ä»‹ç´¹ [LibriVox Indonesia](https://huggingface.co/datasets/indonesian-nlp/librivox-indonesia/blob/main/librivox-indonesia.py) åŠ è¼‰è…³æœ¬ã€‚

#### Download and define the dataset splits

1. ä½¿ç”¨ [`download()`](https://huggingface.co/docs/datasets/v2.14.1/en/package_reference/builder_classes#datasets.DownloadManager.download) æ–¹æ³•ä¸‹è¼‰ `_AUDIO_URL` çš„éŸ³é »æ•¸æ“šã€‚

2. è¦åœ¨æœ¬åœ°æå–éŸ³é » TAR å­˜æª”ï¼Œè«‹ä½¿ç”¨ `extract()`ã€‚æ‚¨åªèƒ½åœ¨éæµæ¨¡å¼ä¸‹ä½¿ç”¨æ­¤æ–¹æ³•ï¼ˆç•¶ `dl_manager.is_streaming=False` æ™‚ï¼‰ã€‚é€™å°‡è¿”å›æå–çš„å­˜æª”ç›®éŒ„çš„æœ¬åœ°è·¯å¾‘ï¼š

    ```python
    local_extracted_archive = dl_manager.extract(audio_path) if not dl_manager.is_streaming else None
    ```

3. ä½¿ç”¨ `iter_archive()` æ–¹æ³•è¿­ä»£ `audio_path` è™•çš„å­˜æª”ï¼Œå°±åƒä¸Šé¢çš„ Vivos ç¤ºä¾‹ä¸€æ¨£ã€‚`iter_archive()` ä¸æä¾›æœ‰é—œå­˜æª”ä¸­æ–‡ä»¶çš„å®Œæ•´è·¯å¾‘çš„ä»»ä½•ä¿¡æ¯ï¼Œå³ä½¿å®ƒå·²è¢«æå–ã€‚å› æ­¤ï¼Œæ‚¨éœ€è¦å°‡ `local_extracted_archive` è·¯å¾‘å‚³éåˆ° `gen_kwargs` ä¸­çš„ä¸‹ä¸€æ­¥ï¼Œä»¥ä¿ç•™æœ‰é—œå­˜æª”æå–åˆ°çš„ä½ç½®çš„ä¿¡æ¯ã€‚é€™æ˜¯åœ¨ç”Ÿæˆç¤ºä¾‹æ™‚æ§‹é€ æœ¬åœ°æ–‡ä»¶çš„æ­£ç¢ºè·¯å¾‘æ‰€å¿…éœ€çš„ã€‚

    !!! warning
        éœ€è¦çµåˆä½¿ç”¨ `download()` å’Œ `iter_archive()` çš„åŸå› æ˜¯ TAR å­˜æª”ä¸­çš„æ–‡ä»¶ç„¡æ³•é€šéå…¶è·¯å¾‘ç›´æ¥è¨ªå•ã€‚
        
        ç›¸åï¼Œæ‚¨éœ€è¦è¿­ä»£å­˜æª”ä¸­çš„æ–‡ä»¶ï¼æ‚¨åªèƒ½åœ¨éæµæ¨¡å¼ä¸‹å° TAR å­˜æª”ä½¿ç”¨ `download_and_extract()` å’Œ `extract()` ï¼Œå¦å‰‡æœƒå¼•ç™¼éŒ¯èª¤ã€‚

4. ä½¿ç”¨ `download_and_extract()` æ–¹æ³•ä¸‹è¼‰ `_METADATA_URL` ä¸­æŒ‡å®šçš„å…ƒæ•¸æ“šæ–‡ä»¶ã€‚è©²æ–¹æ³•è¿”å›éæµæ¨¡å¼ä¸‹æœ¬åœ°æ–‡ä»¶çš„è·¯å¾‘ã€‚åœ¨æµæ¨¡å¼ä¸‹ï¼Œå®ƒä¸æœƒåœ¨æœ¬åœ°ä¸‹è¼‰æ–‡ä»¶ä¸¦è¿”å›ç›¸åŒçš„ URLã€‚

5. ç¾åœ¨ä½¿ç”¨ `SplitGenerator` ä¾†çµ„ç¹”æ¯å€‹åˆ†å‰²ä¸­çš„éŸ³é »æ–‡ä»¶å’Œå…ƒæ•¸æ“šã€‚ä½¿ç”¨æ¨™æº–åç¨±å‘½åæ¯å€‹ splitï¼Œä¾‹å¦‚ï¼š`Split.TRAIN`ã€`Split.TEST` å’Œ `SPLIT.Validation`ã€‚

    åœ¨ `gen_kwargs` åƒæ•¸ä¸­ï¼ŒæŒ‡å®š `local_extracted_archive`ã€`audio_files`ã€`metadata_path` å’Œ `path_to_clips` çš„æ–‡ä»¶è·¯å¾‘ã€‚è«‹è¨˜ä½ï¼Œå°æ–¼ `audio_files`ï¼Œæ‚¨éœ€è¦ä½¿ç”¨ `iter_archive()` ä¾†è¿­ä»£ TAR å­˜æª”ä¸­çš„éŸ³é »æ–‡ä»¶ã€‚é€™å°‡ç‚ºæ‚¨çš„æ•¸æ“šé›†å•Ÿç”¨æµå¼å‚³è¼¸ï¼æ‰€æœ‰é€™äº›æ–‡ä»¶è·¯å¾‘éƒ½å°‡å‚³éåˆ°ç”Ÿæˆæ•¸æ“šé›†æ¨£æœ¬çš„ä¸‹ä¸€æ­¥ã€‚

```python
def _split_generators(self, dl_manager):
    """Returns SplitGenerators."""
    dl_manager.download_config.ignore_url_params = True

    audio_path = dl_manager.download(_AUDIO_URL)
    local_extracted_archive = dl_manager.extract(audio_path) if not dl_manager.is_streaming else None
    path_to_clips = "librivox-indonesia"

    return [
        datasets.SplitGenerator(
            name=datasets.Split.TRAIN,
            gen_kwargs={
                "local_extracted_archive": local_extracted_archive,
                "audio_files": dl_manager.iter_archive(audio_path),
                "metadata_path": dl_manager.download_and_extract(_METADATA_URL + "/metadata_train.csv.gz"),
                "path_to_clips": path_to_clips,
            },
        ),
        datasets.SplitGenerator(
            name=datasets.Split.TEST,
            gen_kwargs={
                "local_extracted_archive": local_extracted_archive,
                "audio_files": dl_manager.iter_archive(audio_path),
                "metadata_path": dl_manager.download_and_extract(_METADATA_URL + "/metadata_test.csv.gz"),
                "path_to_clips": path_to_clips,
            },
        ),
    ]
```

#### Generate the dataset

é€™è£¡ `_generate_examples` æ¥å—ä¹‹å‰æ–¹æ³•ä¸­çš„ `local_extracted_archive`ã€`audio_files`ã€`metadata_path` å’Œ `path_to_clips` ä½œç‚ºåƒæ•¸ã€‚

1. TAR æ–‡ä»¶æŒ‰é †åºè¨ªå•å’Œç”Ÿæˆã€‚é€™æ„å‘³è‘—æ‚¨éœ€è¦é¦–å…ˆæ“æœ‰èˆ‡ TAR æ–‡ä»¶ä¸­çš„éŸ³é »æ–‡ä»¶é—œè¯çš„ `metadata_path` ä¸­çš„å…ƒæ•¸æ“šï¼Œä»¥ä¾¿æ‚¨å¯ä»¥é€²ä¸€æ­¥ç”Ÿæˆå®ƒåŠå…¶ç›¸æ‡‰çš„éŸ³é »æ–‡ä»¶ï¼š

    ```python
    with open(metadata_path, "r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            if self.config.name == "all" or self.config.name == row["language"]:
                row["path"] = os.path.join(path_to_clips, row["path"])
                # if data is incomplete, fill with empty values
                for field in data_fields:
                    if field not in row:
                        row[field] = ""
                metadata[row["path"]] = row
    ```

2. ç¾åœ¨æ‚¨å¯ä»¥ç”Ÿæˆ `audio_files` å­˜æª”ä¸­çš„æ–‡ä»¶ã€‚ç•¶æ‚¨ä½¿ç”¨ `iter_archive()` æ™‚ï¼Œå®ƒæœƒç”Ÿæˆä¸€å€‹ `(path, f)` å…ƒçµ„ï¼Œå…¶ä¸­ `path` æ˜¯å­˜æª”å…§æ–‡ä»¶çš„ç›¸å°è·¯å¾‘ï¼Œ`f` æ˜¯æ–‡ä»¶å°è±¡æœ¬èº«ã€‚è¦ç²å–æœ¬åœ°æå–æ–‡ä»¶çš„å®Œæ•´è·¯å¾‘ï¼Œè«‹å°‡å­˜æª”æå–åˆ°çš„ç›®éŒ„è·¯å¾‘ `(local_extracted_paâ€‹â€‹th)` èˆ‡ç›¸å°éŸ³é »æ–‡ä»¶è·¯å¾‘ `(path)` ç›¸é€£æ¥ï¼š

    ```python
    for path, f in audio_files:
        if path in metadata:
            result = dict(metadata[path])
            # set the audio feature and the path to the extracted file
            path = os.path.join(local_extracted_archive, path) if local_extracted_archive else path
            result["audio"] = {"path": path, "bytes": f.read()}
            result["path"] = path
            yield id_, result
            id_ += 1    
    ```

å°‡é€™å…©å€‹æ­¥é©Ÿæ”¾åœ¨ä¸€èµ·ï¼Œæ•´å€‹ `_generate_examples` æ–¹æ³•æ‡‰å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
def _generate_examples(
        self,
        local_extracted_archive,
        audio_files,
        metadata_path,
        path_to_clips,
    ):
        """Yields examples."""
        data_fields = list(self._info().features.keys())
        metadata = {}

        with open(metadata_path, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                if self.config.name == "all" or self.config.name == row["language"]:
                    row["path"] = os.path.join(path_to_clips, row["path"])
                    # if data is incomplete, fill with empty values
                    for field in data_fields:
                        if field not in row:
                            row[field] = ""
                    metadata[row["path"]] = row
        id_ = 0

        for path, f in audio_files:
            if path in metadata:
                result = dict(metadata[path])
                # set the audio feature and the path to the extracted file
                path = os.path.join(local_extracted_archive, path) if local_extracted_archive else path
                result["audio"] = {"path": path, "bytes": f.read()}
                result["path"] = path
                yield id_, result
                id_ += 1
```