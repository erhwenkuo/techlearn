# Process text data

æœ¬æŒ‡å—å±•ç¤ºäº†è™•ç†æ–‡æœ¬æ•¸æ“šé›†çš„å…·é«”æ–¹æ³•ã€‚äº†è§£å¦‚ä½•ï¼š

- ä½¿ç”¨ `map()` å°æ•¸æ“šé›†é€²è¡Œ tokenizeã€‚
- å°‡æ•¸æ“šé›†æ¨™ç±¤èˆ‡ NLI æ•¸æ“šé›†çš„æ¨™ç±¤ ID å°é½Šã€‚

æœ‰é—œå¦‚ä½•è™•ç†ä»»ä½•é¡å‹çš„æ•¸æ“šé›†çš„æŒ‡å—ï¼Œè«‹æŸ¥çœ‹[ä¸€èˆ¬æµç¨‹](https://huggingface.co/docs/datasets/v2.14.1/en/process)æŒ‡å—ã€‚

## Map

`map()` å‡½æ•¸æ”¯æŒä¸€æ¬¡è™•ç†æ‰¹é‡ç¤ºä¾‹ï¼Œå¾è€ŒåŠ å¿« tokenization é€Ÿåº¦ã€‚

å¾ ğŸ¤— Transformers åŠ è¼‰ tokenizerï¼š

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```

åœ¨ `map()` å‡½æ•¸ä¸­å°‡ `batched` åƒæ•¸è¨­ç½®ç‚º `True`ï¼Œå°‡ tokenizer æ‡‰ç”¨æ–¼æ‰¹é‡ examplesï¼š

```python
dataset = dataset.map(lambda examples: tokenizer(examples["text"]), batched=True)

dataset[0]
```

çµæœ:

```bash
{'text': 'the rock is destined to be the 21st century\'s new " conan " and that he\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .', 
 'label': 1, 
 'input_ids': [101, 1996, 2600, 2003, 16036, 2000, 2022, 1996, 7398, 2301, 1005, 1055, 2047, 1000, 16608, 1000, 1998, 2008, 2002, 1005, 1055, 2183, 2000, 2191, 1037, 17624, 2130, 3618, 2084, 7779, 29058, 8625, 13327, 1010, 3744, 1011, 18856, 19513, 3158, 5477, 4168, 2030, 7112, 16562, 2140, 1012, 102], 
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

`map()` å‡½æ•¸å°‡è¿”å›çš„å€¼è½‰æ›ç‚º `PyArrow` æ”¯æŒçš„æ ¼å¼ã€‚ä½†é¡¯å¼è¿”å›å¼µé‡ä½œç‚º NumPy æ•¸çµ„é€Ÿåº¦æ›´å¿«ï¼Œå› ç‚ºå®ƒæ˜¯åŸç”Ÿæ”¯æŒçš„ PyArrow æ ¼å¼ã€‚ç•¶æ‚¨æ¨™è¨˜æ–‡æœ¬æ™‚è¨­ç½® `return_tensors="np"`ï¼š

```python
dataset = dataset.map(lambda examples: tokenizer(examples["text"], return_tensors="np"), batched=True)
```

## Align

`align_labels_with_mapping()` å‡½æ•¸å°‡æ•¸æ“šé›†æ¨™ç±¤ id èˆ‡æ¨™ç±¤åç¨±å°é½Šã€‚ä¸¦éæ‰€æœ‰ğŸ¤— Transformers æ¨¡å‹éƒ½éµå¾ªåŸå§‹æ•¸æ“šé›†è¦å®šçš„æ¨™ç±¤æ˜ å°„ï¼Œç‰¹åˆ¥æ˜¯å°æ–¼ NLI æ•¸æ“šé›†ã€‚ä¾‹å¦‚ï¼Œ[MNLI æ•¸æ“šé›†](https://huggingface.co/datasets/glue)ä½¿ç”¨ä»¥ä¸‹æ¨™ç±¤æ˜ å°„ï¼š

```python
label2id = {"entailment": 0, "neutral": 1, "contradiction": 2}
```

è¦å°‡æ•¸æ“šé›†æ¨™ç±¤æ˜ å°„èˆ‡æ¨¡å‹ä½¿ç”¨çš„æ˜ å°„å°é½Šï¼Œè«‹å‰µå»ºè¦å°é½Šçš„æ¨™ç±¤åç¨±å’Œ id çš„å­—å…¸ï¼š

```python
label2id = {"contradiction": 0, "neutral": 1, "entailment": 2}
```

å°‡æ¨™ç±¤æ˜ å°„çš„å­—å…¸å‚³éçµ¦ `align_labels_with_mapping()` å‡½æ•¸ï¼Œä»¥åŠè¦å°é½Šçš„åˆ—ï¼š

```python
from datasets import load_dataset

mnli = load_dataset("glue", "mnli", split="train")

mnli_aligned = mnli.align_labels_with_mapping(label2id, "label")
```

æ‚¨é‚„å¯ä»¥ä½¿ç”¨æ­¤å‡½æ•¸å°‡æ¨™ç±¤çš„è‡ªå®šç¾©æ˜ å°„åˆ†é…çµ¦ idã€‚