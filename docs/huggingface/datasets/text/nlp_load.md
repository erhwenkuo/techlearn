# Load text data

æœ¬æŒ‡å—å‘æ‚¨å±•ç¤ºå¦‚ä½•åŠ è¼‰æ–‡æœ¬æ•¸æ“šé›†ã€‚è¦äº†è§£å¦‚ä½•åŠ è¼‰ä»»ä½•é¡å‹çš„æ•¸æ“šé›†ï¼Œè«‹æŸ¥çœ‹[ä¸€èˆ¬åŠ è¼‰](https://huggingface.co/docs/datasets/v2.14.1/en/loading)æŒ‡å—ã€‚

æ–‡æœ¬æ–‡ä»¶æ˜¯å­˜å„²æ•¸æ“šé›†æœ€å¸¸è¦‹çš„æ–‡ä»¶é¡å‹ä¹‹ä¸€ã€‚é»˜èªæƒ…æ³ä¸‹ï¼ŒğŸ¤— Datasets é€è¡Œå°æ–‡æœ¬æ–‡ä»¶é€²è¡Œæ¡æ¨£ä»¥æ§‹å»ºæ•¸æ“šé›†ã€‚

```python
from datasets import load_dataset

dataset = load_dataset("text", data_files={"train": ["my_text_1.txt", "my_text_2.txt"], "test": "my_test_file.txt"})

# Load from a directory
dataset = load_dataset("text", data_dir="path/to/text/dataset")
```

è¦æŒ‰æ®µè½ç”šè‡³æ•´å€‹æ–‡æª”å°æ–‡æœ¬æ–‡ä»¶é€²è¡Œæ¡æ¨£ï¼Œè«‹ä½¿ç”¨ `sample_by` åƒæ•¸ï¼š

```python
# Sample by paragraph
dataset = load_dataset("text", data_files={"train": "my_train_file.txt", "test": "my_test_file.txt"}, sample_by="paragraph")

# Sample by document
dataset = load_dataset("text", data_files={"train": "my_train_file.txt", "test": "my_test_file.txt"}, sample_by="document")
```

æ‚¨é‚„å¯ä»¥ä½¿ç”¨ `grep` æ¨¡å¼ä¾†åŠ è¼‰ç‰¹å®šæ–‡ä»¶ï¼š

```python
from datasets import load_dataset

c4_subset = load_dataset("allenai/c4", data_files="en/c4-train.0000*-of-01024.json.gz")
```

è¦é€šé HTTP åŠ è¼‰é ç¨‹æ–‡æœ¬æ–‡ä»¶ï¼Œè«‹å‚³é URLï¼š

```python
dataset = load_dataset("text", data_files="https://huggingface.co/datasets/lhoestq/test/resolve/main/some_text.txt")
```

