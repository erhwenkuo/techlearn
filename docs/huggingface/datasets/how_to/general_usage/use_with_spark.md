# Use with Spark

æœ¬æ–‡æª”å¿«é€Ÿä»‹ç´¹ç­å¦‚ä½•å°‡ ğŸ¤— æ•¸æ“šé›†èˆ‡ Spark çµåˆä½¿ç”¨ï¼Œç‰¹åˆ¥é—œæ³¨å¦‚ä½•å°‡ `Spark DataFrame` è½‰æ›æˆ Huggingface çš„ `Dataset` ç‰©ä»¶ã€‚

å¾é‚£è£¡ï¼Œæ‚¨å¯ä»¥å¿«é€Ÿè¨ªå•ä»»ä½•å…ƒç´ ï¼Œä¸¦ä¸”å¯ä»¥å°‡å…¶ç”¨ä½œæ•¸æ“šåŠ è¼‰å™¨ä¾†è¨“ç·´æ¨¡å‹ã€‚

## Load from Spark

Dataset ç‰©ä»¶æ˜¯ Apache Arrow table çš„ wrapperï¼Œå®ƒå…è¨±å¾æ•¸æ“šé›†ä¸­çš„ array å¿«é€Ÿè®€å–åˆ° PyTorchã€TensorFlow å’Œ JAX å¼µé‡ã€‚ Arrow table æ˜¯å¾ç£ç›¤æ˜ å°„çš„å…§å­˜ï¼Œå®ƒå¯ä»¥åŠ è¼‰å¤§æ–¼å¯ç”¨ RAM çš„æ•¸æ“šé›†ã€‚

æ‚¨å¯ä»¥ä½¿ç”¨ `Dataset.from_spark()` å¾ `Spark DataFrame` ç²å–æ•¸æ“šé›†ï¼š

```python
from datasets import Dataset

df = spark.createDataFrame(
    data=[[1, "Elia"], [2, "Teo"], [3, "Fang"]],
    columns=["id", "name"],
)

ds = Dataset.from_spark(df)
```

Spark worker ç·šç¨‹å°‡æ•¸æ“šé›†ä½œç‚º Arrow æ–‡ä»¶å¯«å…¥ç£ç›¤ä¸Šçš„ç·©å­˜ç›®éŒ„ä¸­ï¼Œä¸¦å¾é‚£è£¡åŠ è¼‰æ•¸æ“šé›†ã€‚

æˆ–è€…ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ `IterableDataset.from_spark()` ï¼Œå®ƒè¿”å›ä¸€å€‹ `IterableDataset`ï¼š

```python
from datasets import IterableDataset

df = spark.createDataFrame(
    data=[[1, "Elia"], [2, "Teo"], [3, "Fang"]],
    columns=["id", "name"],
)

ds = IterableDataset.from_spark(df)

print(next(iter(ds)))
```

çµæœ:

```bash
{"id": 1, "name": "Elia"}
```

### Caching

ä½¿ç”¨ `Dataset.from_spark()` æ™‚ï¼Œç”Ÿæˆçš„ Dataset æœƒè¢«ç·©å­˜ï¼›å¦‚æœæ‚¨åœ¨åŒä¸€å€‹ DataFrame ä¸Šå¤šæ¬¡èª¿ç”¨ `Dataset.from_spark()` ï¼Œå®ƒä¸æœƒé‡æ–°é‹è¡Œå°‡æ•¸æ“šé›†ä½œç‚º Arrow æ–‡ä»¶å¯«å…¥ç£ç›¤çš„ Spark jobã€‚

æ‚¨å¯ä»¥é€šéå°‡ `cache_dir=` å‚³éçµ¦ `Dataset.from_spark()` ä¾†è¨­ç½®ç·©å­˜ä½ç½®ã€‚ç¢ºä¿ä½¿ç”¨æ‚¨çš„ worker å’Œç•¶å‰è¨ˆç®—æ©Ÿï¼ˆé©…å‹•ç¨‹åºï¼‰éƒ½å¯ç”¨çš„ diskã€‚

### Feature types

å¦‚æœæ‚¨çš„æ•¸æ“šé›†ç”± imagesã€audio æ•¸æ“šæˆ– N ç¶­é™£åˆ—çµ„æˆï¼Œæ‚¨å¯ä»¥åœ¨ `Dataset.from_spark()`ï¼ˆæˆ– `IterableDataset.from_spark()`ï¼‰ä¸­æŒ‡å®š `features=` åƒæ•¸ï¼š

```python
from datasets import Dataset, Features, Image, Value

data = [(0, open("image.png", "rb").read())]

df = spark.createDataFrame(data, "idx: int, image: binary")

# Also works if you have arrays
# data = [(0, np.zeros(shape=(32, 32, 3), dtype=np.int32).tolist())]
# df = spark.createDataFrame(data, "idx: int, image: array<array<array<int>>>")
features = Features({"idx": Value("int64"), "image": Image()})

dataset = Dataset.from_spark(df, features=features)

print(dataset[0])
```

çµæœ:

```bash
{'idx': 0, 'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32>}
```

æ‚¨å¯ä»¥æŸ¥çœ‹ [Features æ–‡æª”](https://huggingface.co/docs/datasets/v2.14.1/en/package_reference/main_classes#datasets.Features)ä»¥äº†è§£æ‰€æœ‰å¯ç”¨çš„ feature typesã€‚