# Stream

Dataset streaming è™•ç†è®“æ‚¨ç„¡éœ€ä¸‹è¼‰æ•¸æ“šé›†å³å¯ä½¿ç”¨å®ƒã€‚ç•¶æ‚¨è¿­ä»£æ•¸æ“šé›†æ™‚ï¼Œæ•¸æ“šå°‡è¢«æµå¼å‚³è¼¸ã€‚é€™åœ¨ä»¥ä¸‹æƒ…æ³ä¸‹ç‰¹åˆ¥æœ‰ç”¨ï¼š

- æ‚¨ä¸æƒ³ç­‰å¾…ä¸‹è¼‰éå¸¸å¤§çš„æ•¸æ“šé›†ã€‚
- æ•¸æ“šé›†å¤§å°è¶…å‡ºäº†è¨ˆç®—æ©Ÿä¸Šçš„å¯ç”¨ç£ç›¤ç©ºé–“é‡ã€‚
- æ‚¨åªæƒ³å¿«é€Ÿæ¢ç´¢æ•¸æ“šé›†çš„å¹¾å€‹æ¨£æœ¬ã€‚

![](./assets/streaming.gif)

ä¾‹å¦‚ï¼Œ[oscar-corpus/OSCAR-2201](https://huggingface.co/datasets/oscar-corpus/OSCAR-2201) æ•¸æ“šé›†çš„è‹±æ–‡åˆ†å‰²ç‚º `1.2 TB`ï¼Œä½†æ‚¨å¯ä»¥é€šéæµå¼å‚³è¼¸ç«‹å³ä½¿ç”¨å®ƒã€‚é€šéåœ¨`load_dataset()` ä¸­è¨­ç½® `streaming=True` ä¾†æµå¼å‚³è¼¸æ•¸æ“šé›†ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
from datasets import load_dataset

dataset = load_dataset('oscar-corpus/OSCAR-2201', 'en', split='train', streaming=True)

print(next(iter(dataset)))
```

çµæœ:

```bash
{'id': 0, 'text': 'Founded in 2015, Golden Bees is a leading programmatic recruitment platform dedicated to employers, HR agencies and job boards. The company has developed unique HR-custom technologies and predictive algorithms to identify and attract the best candidates for a job opportunity.', ...
```

Dataset streaming é‚„å…è¨±æ‚¨ä½¿ç”¨ç”±æœ¬åœ°æ–‡ä»¶çµ„æˆçš„æ•¸æ“šé›†ï¼Œè€Œç„¡éœ€é€²è¡Œä»»ä½•è½‰æ›ã€‚åœ¨é€™ç¨®æƒ…æ³ä¸‹ï¼Œç•¶æ‚¨è¿­ä»£æ•¸æ“šé›†æ™‚ï¼Œæ•¸æ“šå°‡å¾æœ¬åœ°æ–‡ä»¶æµå¼å‚³è¼¸ã€‚é€™åœ¨ä»¥ä¸‹æƒ…æ³ä¸‹ç‰¹åˆ¥æœ‰ç”¨ï¼š

- æ‚¨ä¸æƒ³ç­‰å¾…éå¸¸å¤§çš„æœ¬åœ°æ•¸æ“šé›†è½‰æ›ç‚º Arrowã€‚
- è½‰æ›å¾Œçš„æ–‡ä»¶å¤§å°å°‡è¶…å‡ºè¨ˆç®—æ©Ÿä¸Šçš„å¯ç”¨ç£ç›¤ç©ºé–“é‡ã€‚
- æ‚¨åªæƒ³å¿«é€Ÿæ¢ç´¢æ•¸æ“šé›†çš„å¹¾å€‹æ¨£æœ¬ã€‚

ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥æµå¼å‚³è¼¸æ•¸ç™¾å€‹å£“ç¸® JSONL æ–‡ä»¶ï¼ˆä¾‹å¦‚ [oscar-corpus/OSCAR-2201](https://huggingface.co/datasets/oscar-corpus/OSCAR-2201)ï¼‰çš„æœ¬åœ°æ•¸æ“šé›†ä»¥ç«‹å³ä½¿ç”¨å®ƒï¼š

```python
from datasets import load_dataset

data_files = {'train': 'path/to/OSCAR-2201/compressed/en_meta/*.jsonl.gz'}

dataset = load_dataset('json', data_files=data_files, split='train', streaming=True)

print(next(iter(dataset)))
```

çµæœ:

```bash
{'id': 0, 'text': 'Founded in 2015, Golden Bees is a leading programmatic recruitment platform dedicated to employers, HR agencies and job boards. The company has developed unique HR-custom technologies and predictive algorithms to identify and attract the best candidates for a job opportunity.', ...
```

ä»¥æµæ¨¡å¼åŠ è¼‰æ•¸æ“šé›†æœƒå‰µå»ºä¸€å€‹æ–°çš„æ•¸æ“šé›†é¡å‹å¯¦ä¾‹ï¼ˆè€Œä¸æ˜¯ç¶“å…¸çš„ `Dataset` ç‰©ä»¶ï¼‰ï¼Œç¨±ç‚º `IterableDataset`ã€‚é€™ç¨®ç‰¹æ®Šé¡å‹çš„æ•¸æ“šé›†æœ‰è‡ªå·±çš„ä¸€å¥— processing data çš„æ–¹æ³•ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚

!!! tip
    `IterableDataset` å°æ–¼è¿­ä»£ä½œæ¥­ï¼ˆä¾‹å¦‚è¨“ç·´æ¨¡å‹ï¼‰éå¸¸æœ‰ç”¨ã€‚æ‚¨ä¸æ‡‰è©²å°‡ `IterableDataset` ç”¨æ–¼éœ€è¦éš¨æ©Ÿè¨ªå•ç¤ºä¾‹çš„ä½œæ¥­ï¼Œå› ç‚ºæ‚¨å¿…é ˆä½¿ç”¨ `for loop` å°å…¶é€²è¡Œè¿­ä»£ã€‚ç²å–å¯è¿­ä»£æ•¸æ“šé›†ä¸­çš„æœ€å¾Œä¸€å€‹ç¤ºä¾‹éœ€è¦è¿­ä»£æ‰€æœ‰å‰é¢çš„ç¤ºä¾‹ã€‚æ‚¨å¯ä»¥åœ¨ [Dataset vs. IterableDataset](https://huggingface.co/docs/datasets/about_mapstyle_vs_iterable) æŒ‡å—ä¸­æ‰¾åˆ°æ›´å¤šè©³ç´°ä¿¡æ¯ã€‚

## Shuffle

èˆ‡å¸¸è¦ `Dataset` ç‰©ä»¶ä¸€æ¨£ï¼Œæ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨ `IterableDataset.shuffle()` å° `IterableDataset` é€²è¡Œæ•¸æ“šæ´—ç‰Œã€‚

`buffer_size` åƒæ•¸æ§åˆ¶å¾ä¸­éš¨æ©Ÿæ¡æ¨£ç¤ºä¾‹çš„ç·©è¡å€çš„å¤§å°ã€‚å‡è¨­æ‚¨çš„æ•¸æ“šé›†æœ‰ä¸€ç™¾è¬å€‹ç¤ºä¾‹ï¼Œä¸¦ä¸”æ‚¨å°‡ `buffer_size` è¨­ç½®ç‚ºä¸€è¬ã€‚ `IterableDataset.shuffle()` å°‡å¾ç·©è¡å€ä¸­çš„å‰ä¸€è¬å€‹ç¤ºä¾‹ä¸­éš¨æ©Ÿé¸æ“‡ç¤ºä¾‹ã€‚ç·©è¡å€ä¸­é¸å®šçš„ç¤ºä¾‹å°‡è¢«æ–°ç¤ºä¾‹æ›¿æ›ã€‚é»˜èªæƒ…æ³ä¸‹ï¼Œç·©è¡å€å¤§å°ç‚º `1,000`ã€‚

```python
from datasets import load_dataset

dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)

shuffled_dataset = dataset.shuffle(seed=42, buffer_size=10_000)
```

## Reshuffle

æœ‰æ™‚æ‚¨å¯èƒ½æƒ³åœ¨æ¯å€‹ epoch å¾Œé‡æ–°æ’åˆ—æ•¸æ“šé›†ã€‚é€™å°‡è¦æ±‚æ‚¨ç‚ºæ¯å€‹æ™‚æœŸè¨­ç½®ä¸åŒçš„ç¨®å­ã€‚åœ¨ epoch ä¹‹é–“ä½¿ç”¨ `IterableDataset.set_epoch()`` ä¾†å‘Šè¨´æ•¸æ“šé›†æ‚¨æ‰€è™•çš„ epochã€‚

ä½ çš„ç¨®å­å¯¦éš›ä¸Šè®Šæˆï¼š`initial seed + current epoch`

```python
for epoch in range(epochs):
    shuffled_dataset.set_epoch(epoch)
    for example in shuffled_dataset:
        ...

```

## Split dataset

æ‚¨å¯ä»¥é€šéä»¥ä¸‹å…©ç¨®æ–¹å¼ä¹‹ä¸€åˆ†å‰²æ•¸æ“šé›†ï¼š

- `IterableDataset.take()` è¿”å›æ•¸æ“šé›†ä¸­çš„å‰ n å€‹ç¤ºä¾‹ï¼š

    ```python
    dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
    dataset_head = dataset.take(2)
    
    print(list(dataset_head))
    ```

    çµæœ:

    ```bash
    [{'id': 0, 'text': 'Mtendere Village was...'}, {'id': 1, 'text': 'Lily James cannot fight the music...'}]
    ```

- `IterableDataset.skip()` å¿½ç•¥æ•¸æ“šé›†ä¸­çš„å‰ n å€‹ç¤ºä¾‹ä¸¦è¿”å›å‰©é¤˜çš„ç¤ºä¾‹ï¼š

    ```python
    train_dataset = shuffled_dataset.skip(1000)
    ```

## Interleave

`interleave_datasets()` å¯ä»¥å°‡ `IterableDataset` èˆ‡å…¶ä»–æ•¸æ“šé›†çµ„åˆèµ·ä¾†ã€‚çµ„åˆæ•¸æ“šé›†è¿”å›æ¯å€‹åŸå§‹æ•¸æ“šé›†çš„äº¤æ›¿ç¤ºä¾‹ã€‚

```python
from datasets import interleave_datasets

en_dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
fr_dataset = load_dataset('oscar', "unshuffled_deduplicated_fr", split='train', streaming=True)

multilingual_dataset = interleave_datasets([en_dataset, fr_dataset])

print(list(multilingual_dataset.take(2)))
```

çµæœ:

```bash
[{'text': 'Mtendere Village was inspired by the vision...'}, {'text': "MÃ©dia de dÃ©bat d'idÃ©es, de culture et de littÃ©rature..."}]
```

å®šç¾©æ¯å€‹åŸå§‹æ•¸æ“šé›†çš„æ¡æ¨£æ¦‚ç‡ï¼Œä»¥ä¾¿æ›´å¥½åœ°æ§åˆ¶æ¯å€‹æ•¸æ“šé›†çš„æ¡æ¨£å’Œçµ„åˆæ–¹å¼ã€‚ä½¿ç”¨æ‰€éœ€çš„æ¡æ¨£æ¦‚ç‡è¨­ç½®æ¦‚ç‡åƒæ•¸ï¼š

```python
multilingual_dataset_with_oversampling = interleave_datasets([en_dataset, fr_dataset], probabilities=[0.8, 0.2], seed=42)

print(list(multilingual_dataset_with_oversampling.take(2)))
```

çµæœ:

```bash
[{'text': 'Mtendere Village was inspired by the vision...'}, {'text': 'Lily James cannot fight the music...'}]
```

æœ€çµ‚æ•¸æ“šé›†çš„å¤§ç´„ `80%` ç”± `en_dataset` çµ„æˆï¼Œ`20%` ç”± `fr_dataset` çµ„æˆã€‚

æ‚¨é‚„å¯ä»¥æŒ‡å®š `stopping_strategy`ã€‚é»˜èªç­–ç•¥ `first_exhausted` æ˜¯äºŒæ¬¡æ¡æ¨£ç­–ç•¥ï¼Œå³ä¸€æ—¦å…¶ä¸­ä¸€å€‹æ•¸æ“šé›†ç”¨å®Œæ¨£æœ¬ï¼Œæ•¸æ“šé›†æ§‹å»ºå°±æœƒåœæ­¢ã€‚

æ‚¨å¯ä»¥æŒ‡å®š `stopping_strategy=all_exhausted` ä¾†åŸ·è¡Œéæ¡æ¨£ç­–ç•¥ã€‚åœ¨é€™ç¨®æƒ…æ³ä¸‹ï¼Œä¸€æ—¦æ¯å€‹æ•¸æ“šé›†ä¸­çš„æ¯å€‹æ¨£æœ¬è¢«æ·»åŠ è‡³å°‘ä¸€æ¬¡ï¼Œæ•¸æ“šé›†æ§‹å»ºå°±æœƒåœæ­¢ã€‚

å¯¦éš›ä¸Šï¼Œé€™æ„å‘³è‘—å¦‚æœæ•¸æ“šé›†è€—ç›¡ï¼Œå®ƒå°‡è¿”å›åˆ°è©²æ•¸æ“šé›†çš„é–‹é ­ï¼Œç›´åˆ°é”åˆ°åœæ­¢æ¨™æº–ã€‚è«‹æ³¨æ„ï¼Œå¦‚æœæœªæŒ‡å®šæ¡æ¨£æ¦‚ç‡ï¼Œå‰‡æ–°æ•¸æ“šé›†å°‡å…·æœ‰ `max_length_datasets * nb_dataset` æ¨£æœ¬ã€‚

## Rename, remove, and cast

ä»¥ä¸‹æ–¹æ³•å…è¨±æ‚¨ä¿®æ”¹æ•¸æ“šé›†çš„åˆ—(column)ã€‚é€™äº›æ–¹æ³•å°æ–¼é‡å‘½åæˆ–åˆªé™¤åˆ—ä»¥åŠå°‡åˆ—æ›´æ”¹ç‚ºä¸€çµ„æ–° feature éå¸¸æœ‰ç”¨ã€‚

### Rename

ç•¶æ‚¨éœ€è¦é‡å‘½åæ•¸æ“šé›†ä¸­çš„åˆ—(column)æ™‚ï¼Œè«‹ä½¿ç”¨ `IterableDataset.rename_column()`ã€‚èˆ‡åŸå§‹åˆ—é—œè¯çš„ç‰¹å¾µå¯¦éš›ä¸Šè¢«ç§»å‹•åˆ°æ–°åˆ—åç¨±ä¸‹ï¼Œè€Œä¸æ˜¯åƒ…åƒ…å°±åœ°æ›¿æ›åŸå§‹åˆ—ã€‚

ç‚º `IterableDataset.rename_column()`` æä¾›åŸå§‹åˆ—çš„åç¨±å’Œæ–°åˆ—çš„åç¨±ï¼š

```python
from datasets import load_dataset

dataset = load_dataset('mc4', 'en', streaming=True, split='train')

dataset = dataset.rename_column("text", "content")
```

### Remove

ç•¶æ‚¨éœ€è¦åˆªé™¤ä¸€åˆ—æˆ–å¤šåˆ—æ™‚ï¼Œè«‹ç‚º `IterableDataset.remove_columns() æŒ‡å®šè¦åˆªé™¤çš„åˆ—çš„åç¨±ã€‚é€šéæä¾›åˆ—åç¨±åˆ—è¡¨ä¾†åˆªé™¤å¤šå€‹åˆ—ï¼š

```python
from datasets import load_dataset

dataset = load_dataset('mc4', 'en', streaming=True, split='train')

dataset = dataset.remove_columns('timestamp')
```

### Cast

`IterableDataset.cast()` æ›´æ”¹ä¸€åˆ—æˆ–å¤šåˆ—çš„ç‰¹å¾µé¡å‹ã€‚æ­¤æ–¹æ³•å°‡æ‚¨çš„æ–°åŠŸèƒ½ä½œç‚ºå…¶åƒæ•¸ã€‚ä»¥ä¸‹ç¤ºä¾‹ä»£ç¢¼å±•ç¤ºç­å¦‚ä½•æ›´æ”¹ `ClassLabel` å’Œ `Value` çš„è¦ç´ é¡å‹ï¼š

```python
from datasets import load_dataset

dataset = load_dataset('glue', 'mrpc', split='train', streaming=True)

print(dataset.features)
```

çµæœ:

```bash
{'sentence1': Value(dtype='string', id=None),
'sentence2': Value(dtype='string', id=None),
'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
'idx': Value(dtype='int32', id=None)}
```

```python
from datasets import ClassLabel, Value

new_features = dataset.features.copy()

new_features["label"] = ClassLabel(names=['negative', 'positive'])

new_features["idx"] = Value('int64')

dataset = dataset.cast(new_features)

print(dataset.features)
```

çµæœ:

```bash
{'sentence1': Value(dtype='string', id=None),
'sentence2': Value(dtype='string', id=None),
'label': ClassLabel(num_classes=2, names=['negative', 'positive'], names_file=None, id=None),
'idx': Value(dtype='int64', id=None)}
```

!!! warn
    åƒ…ç•¶åŸå§‹è¦ç´ é¡å‹å’Œæ–°è¦ç´ é¡å‹å…¼å®¹æ™‚ï¼Œ`cast` æ‰æœ‰æ•ˆã€‚ä¾‹å¦‚ï¼Œå¦‚æœåŸå§‹åˆ—åƒ…åŒ…å« 1 å’Œ 0ï¼Œå‰‡å¯ä»¥å°‡è¦ç´ é¡å‹ `Value('int32')` çš„åˆ—è½‰æ›ç‚º `Value('bool')`ã€‚

ä½¿ç”¨ `IterableDataset.cast_column()` æ›´æ”¹åƒ…ä¸€åˆ—çš„ç‰¹å¾µé¡å‹ã€‚å°‡åˆ—åç¨±åŠå…¶æ–°ç‰¹å¾µé¡å‹ä½œç‚ºåƒæ•¸å‚³éï¼š

```python
print(dataset.features)
```

çµæœ:

```bash
{'audio': Audio(sampling_rate=44100, mono=True, id=None)}
```

```python
dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))

print(dataset.features)
```

çµæœ:

```bash
{'audio': Audio(sampling_rate=16000, mono=True, id=None)}
```

## Map

èˆ‡å¸¸è¦æ•¸æ“šé›†çš„ `Dataset.map()` å‡½æ•¸é¡ä¼¼ï¼ŒğŸ¤— `Datasets` å…·æœ‰ç”¨æ–¼è™•ç† `IterableDataset` çš„ `IterableDataset.map()` åŠŸèƒ½ã€‚ `IterableDataset.map()` åœ¨ç¤ºä¾‹æµå¼å‚³è¼¸æ™‚æ‡‰ç”¨å³æ™‚è™•ç†ã€‚

å®ƒå…è¨±æ‚¨ç¨ç«‹æˆ–æ‰¹é‡åœ°å°‡è™•ç†å‡½æ•¸æ‡‰ç”¨æ–¼æ•¸æ“šé›†ä¸­çš„æ¯å€‹ç¤ºä¾‹ã€‚è©²å‡½æ•¸ç”šè‡³å¯ä»¥å‰µå»ºæ–°çš„è¡Œå’Œåˆ—ã€‚

ä»¥ä¸‹ç¤ºä¾‹æ¼”ç¤ºç­å¦‚ä½•å° `IterableDataset` é€²è¡Œ tokenizeã€‚è©²å‡½æ•¸éœ€è¦æ¥å—ä¸¦è¼¸å‡ºä¸€å€‹ dictï¼š

```python
def add_prefix(example):
    example['text'] = 'My text: ' + example['text']
    return example
```

æ¥ä¸‹ä¾†ï¼Œä½¿ç”¨ `IterableDataset.map()` å°‡æ­¤å‡½æ•¸æ‡‰ç”¨æ–¼æ•¸æ“šé›†ï¼š

```python
from datasets import load_dataset

dataset = load_dataset('oscar', 'unshuffled_deduplicated_en', streaming=True, split='train')

updated_dataset = dataset.map(add_prefix)

print(list(updated_dataset.take(3)))
```

çµæœ:

```bash
[{'id': 0, 'text': 'My text: Mtendere Village was inspired by...'},
 {'id': 1, 'text': 'My text: Lily James cannot fight the music...'},
 {'id': 2, 'text': 'My text: "I\'d love to help kickstart...'}]
```

è®“æˆ‘å€‘çœ‹å¦ä¸€å€‹ç¤ºä¾‹ï¼Œåªä¸éé€™æ¬¡ï¼Œæ‚¨å°‡ä½¿ç”¨ `IterableDataset.map()` åˆªé™¤ä¸€åˆ—ã€‚ç•¶æ‚¨åˆªé™¤åˆ—æ™‚ï¼Œåªæœ‰åœ¨å°‡ç¤ºä¾‹æä¾›çµ¦æ˜ å°„å‡½æ•¸å¾Œæ‰æœƒåˆªé™¤è©²åˆ—ã€‚é€™å…è¨±æ˜ å°„å‡½æ•¸åœ¨åˆªé™¤åˆ—ä¹‹å‰ä½¿ç”¨å®ƒå€‘çš„å…§å®¹ã€‚

ä½¿ç”¨ `IterableDataset.map()`` ä¸­çš„ `remove_columns` åƒæ•¸æŒ‡å®šè¦åˆªé™¤çš„åˆ—ï¼š

```python
updated_dataset = dataset.map(add_prefix, remove_columns=["id"])

print(list(updated_dataset.take(3)))
```

çµæœ:

```bash
[{'text': 'My text: Mtendere Village was inspired by...'},
 {'text': 'My text: Lily James cannot fight the music...'},
 {'text': 'My text: "I\'d love to help kickstart...'}]
```

### Batch processing

`IterableDataset.map()` é‚„æ”¯æŒè™•ç†æ‰¹é‡ç¤ºä¾‹ã€‚é€šéè¨­ç½® `batched=True` ä¾†é€²è¡Œæ‰¹é‡æ“ä½œã€‚é»˜èªæ‰¹é‡å¤§å°ç‚º `1000`ï¼Œä½†æ‚¨å¯ä»¥ä½¿ç”¨ `batch_size` åƒæ•¸é€²è¡Œèª¿æ•´ã€‚é€™ç‚ºè¨±å¤šæœ‰è¶£çš„æ‡‰ç”¨æ‰“é–‹äº†å¤§é–€ï¼Œä¾‹å¦‚ tokenizationã€å°‡é•·å¥å­åˆ†å‰²æˆè¼ƒçŸ­çš„å¡Šä»¥åŠæ•¸æ“šå¢å¼·ã€‚

#### Tokenization

```python
from datasets import load_dataset
from transformers import AutoTokenizer

dataset = load_dataset("mc4", "en", streaming=True, split="train")

tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')

def encode(examples):
    return tokenizer(examples['text'], truncation=True, padding='max_length')

dataset = dataset.map(encode, batched=True, remove_columns=["text", "timestamp", "url"])

print(next(iter(dataset)))
```

çµæœ:

```bash
{'input_ids': 101, 8466, 1018, 1010, 4029, 2475, 2062, 18558, 3100, 2061, ...,1106, 3739, 102],
'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ..., 1, 1]}
```

!!! tip
    è«‹åƒé–± [batched map processing](https://huggingface.co/docs/datasets/process#batch-processing) æ–‡æª”ä¸­çš„å…¶ä»–æ‰¹è™•ç†ç¤ºä¾‹ã€‚å®ƒå€‘å°æ–¼ `iterable datasets` çš„å·¥ä½œåŸç†ç›¸åŒã€‚

### Filter

æ‚¨å¯ä»¥ä½¿ç”¨ `Dataset.filter()` æ ¹æ“šè¬‚è©å‡½æ•¸éæ¿¾æ•¸æ“šé›†ä¸­çš„è¡Œã€‚å®ƒè¿”å›èˆ‡æŒ‡å®šæ¢ä»¶åŒ¹é…çš„è¡Œï¼š

```python
from datasets import load_dataset

dataset = load_dataset('oscar', 'unshuffled_deduplicated_en', streaming=True, split='train')

start_with_ar = dataset.filter(lambda example: example['text'].startswith('Ar'))

print(next(iter(start_with_ar)))
```

çµæœ:

```bash
{'id': 4, 'text': 'Are you looking for Number the Stars (Essential Modern Classics)?...'}
```

å¦‚æœè¨­ç½® `with_indices=True`ï¼Œ`Dataset.filter()` é‚„å¯ä»¥æŒ‰ç´¢å¼•éæ¿¾ï¼š

```python
even_dataset = dataset.filter(lambda example, idx: idx % 2 == 0, with_indices=True)

print(list(even_dataset.take(3)))
```

çµæœ:

```bash
[{'id': 0, 'text': 'Mtendere Village was inspired by the vision of Chief Napoleon Dzombe, ...'},
 {'id': 2, 'text': '"I\'d love to help kickstart continued development! And 0 EUR/month...'},
 {'id': 4, 'text': 'Are you looking for Number the Stars (Essential Modern Classics)? Normally, ...'}]
```

## Stream in a training loop

`IterableDataset` å¯ä»¥é›†æˆåˆ°è¨“ç·´å¾ªç’°ä¸­ã€‚é¦–å…ˆï¼Œå°æ•¸æ“šé›†é€²è¡Œæ´—ç‰Œï¼š

!!! tip "Pytorch"

    ```python
    seed, buffer_size = 42, 10_000

    dataset = dataset.shuffle(seed, buffer_size=buffer_size)
    ```

    æœ€å¾Œï¼Œå‰µå»ºä¸€å€‹ç°¡å–®çš„è¨“ç·´å¾ªç’°ä¸¦é–‹å§‹è¨“ç·´ï¼š

    ```python
    import torch
    from torch.utils.data import DataLoader
    from transformers import AutoModelForMaskedLM, DataCollatorForLanguageModeling
    from tqdm import tqdm

    dataset = dataset.with_format("torch")

    dataloader = DataLoader(dataset, collate_fn=DataCollatorForLanguageModeling(tokenizer))

    device = 'cuda' if torch.cuda.is_available() else 'cpu' 

    model = AutoModelForMaskedLM.from_pretrained("distilbert-base-uncased")

    model.train().to(device)

    optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-5)
    
    for epoch in range(3):
        dataset.set_epoch(epoch)
        for i, batch in enumerate(tqdm(dataloader, total=5)):
            if i == 5:
                break
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**batch)
            loss = outputs[0]
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            if i % 10 == 0:
                print(f"loss: {loss}")
    ```