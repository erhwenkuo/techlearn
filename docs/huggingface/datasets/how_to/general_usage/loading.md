# Load

æ‚¨çš„æ•¸æ“šå¯ä»¥å­˜å„²åœ¨ä¸åŒçš„åœ°æ–¹ï¼›å®ƒå€‘å¯ä»¥ä½æ–¼æœ¬åœ°è¨ˆç®—æ©Ÿçš„ç£ç›¤ä¸Šã€Github å­˜å„²åº«ä¸­ä»¥åŠå…§å­˜ä¸­çš„æ•¸æ“šçµæ§‹ï¼ˆä¾‹å¦‚ Python å­—å…¸å’Œ Pandas DataFramesï¼‰ä¸­ã€‚ç„¡è«–æ•¸æ“šé›†å­˜å„²åœ¨ä½•è™•ï¼ŒğŸ¤— æ•¸æ“šé›†éƒ½å¯ä»¥å¹«åŠ©æ‚¨åŠ è¼‰å®ƒã€‚

æœ¬æŒ‡å—å°‡å‘æ‚¨å±•ç¤ºå¦‚ä½•å¾ä»¥ä¸‹ä½ç½®åŠ è¼‰æ•¸æ“šé›†ï¼š

- The Hub without a dataset loading script
- Local loading script
- Local files
- In-memory data
- Offline
- A specific slice of a split

æœ‰é—œåŠ è¼‰å…¶ä»–æ•¸æ“šé›†æ¨¡å¼çš„æ›´å¤šè©³ç´°ä¿¡æ¯ï¼Œè«‹æŸ¥çœ‹ [load audio dataset guide](https://huggingface.co/docs/datasets/audio_load)ã€[load image dataset guide](https://huggingface.co/docs/datasets/image_load) æˆ– [load text dataset guide](https://huggingface.co/docs/datasets/nlp_load)ã€‚

## Hugging Face Hub

æ•¸æ“šé›†æ˜¯å¾ä¸‹è¼‰çš„ `dataset loading script` æ‰€ç”Ÿæˆçš„ã€‚ä½†æ˜¯ï¼Œæ‚¨ä¹Ÿå¯ä»¥å¾ Hub ä¸Šçš„ä»»ä½•æ•¸æ“šé›†å­˜å„²åº«åŠ è¼‰æ•¸æ“šé›†ï¼Œè€Œç„¡éœ€åŠ è¼‰è…³æœ¬ï¼

é¦–å…ˆåœ¨ Huggingface ä¸Šå‰µå»º dataset repo æ•¸æ“šé›†å­˜å„²åº«ä¸¦ä¸Šå‚³æ•¸æ“šæ–‡ä»¶ã€‚æ¥è‘—æ‚¨å°±å¯ä»¥ä½¿ç”¨ `load_dataset()` å‡½æ•¸åŠ è¼‰æ•¸æ“šé›†ã€‚

ä¾‹å¦‚ï¼Œå˜—è©¦é€šéæä¾›å­˜å„²åº«å‘½åç©ºé–“å’Œæ•¸æ“šé›†åç¨±([demo repository](https://huggingface.co/datasets/lhoestq/demo1))ä¾†åŠ è¼‰æ•¸æ“šã€‚è©²æ•¸æ“šé›†å­˜å„²åº«åŒ…å« CSV æ–‡ä»¶ï¼Œä¸‹é¢çš„ç¨‹å¼ç¢¼å¾ CSV æ–‡ä»¶åŠ è¼‰æ•¸æ“šé›†ï¼š

```python
from datasets import load_dataset

dataset = load_dataset("lhoestq/demo1")

print(dataset)
```

æŸäº›æ•¸æ“šé›†å¯èƒ½æœ‰å¤šå€‹åŸºæ–¼ Git æ¨™ç±¤ã€åˆ†æ”¯æˆ–æäº¤çš„ç‰ˆæœ¬ã€‚ä½¿ç”¨ `revision` åƒæ•¸æŒ‡å®šè¦åŠ è¼‰çš„æ•¸æ“šé›†ç‰ˆæœ¬ï¼š

```python
dataset = load_dataset(
  "lhoestq/custom_squad",
  revision="main"  # tag name, or branch name, or commit hash
)
```

é»˜èªæƒ…æ³ä¸‹ï¼Œæ²’æœ‰ `dataset loading script` çš„æ•¸æ“šé›†æœƒå°‡æ‰€æœ‰æ•¸æ“šåŠ è¼‰åˆ° `train` çš„ã€€`split` ä¸­ã€‚ä½¿ç”¨ `data_files` åƒæ•¸å°‡æ•¸æ“šæ–‡ä»¶æ˜ å°„åˆ° `train`ã€`validation` å’Œ `test`ã€€çš„ splitsï¼š

```python
data_files = {"train": "train.csv", "test": "test.csv"}

dataset = load_dataset("namespace/your_dataset_name", data_files=data_files)
```

!!! tip
    å¦‚æœä¸æŒ‡å®šä½¿ç”¨å“ªäº›æ•¸æ“šæ–‡ä»¶ï¼Œ`load_dataset()` å°‡è¿”å›æ‰€æœ‰æ•¸æ“šæ–‡ä»¶ã€‚å¦‚æœåŠ è¼‰åƒ C4 é€™æ¨£çš„å¤§å‹æ•¸æ“šé›†ï¼ˆå¤§ç´„æœ‰ 13TB çš„æ•¸æ“šï¼‰ï¼Œé€™å¯èƒ½éœ€è¦å¾ˆé•·æ™‚é–“ã€‚

æ‚¨é‚„å¯ä»¥ä½¿ç”¨ `data_files` æˆ– `data_dir` åƒæ•¸åŠ è¼‰ç‰¹å®šçš„æ–‡ä»¶å­é›†ã€‚é€™äº›åƒæ•¸å¯ä»¥æ¥å—ç›¸å°è·¯å¾‘ï¼Œè©²ç›¸å°è·¯å¾‘è§£æç‚ºèˆ‡æ•¸æ“šé›†åŠ è¼‰ä½ç½®ç›¸å°æ‡‰çš„åŸºæœ¬è·¯å¾‘ã€‚

```python
from datasets import load_dataset

# load files that match the grep pattern
c4_subset = load_dataset("allenai/c4", data_files="en/c4-train.0000*-of-01024.json.gz")

# load dataset from the en directory on the Hub
c4_subset = load_dataset("allenai/c4", data_dir="en")
```

`split` åƒæ•¸é‚„å¯ä»¥å°‡æ•¸æ“šæ–‡ä»¶æ˜ å°„åˆ°ç‰¹å®šçš„ `split`ï¼š

```python
data_files = {"validation": "en/c4-validation.*.json.gz"}

c4_validation = load_dataset("allenai/c4", data_files=data_files, split="validation")
```

## Local loading script

æ‚¨çš„æœ¬åœ°è¨ˆç®—æ©Ÿä¸Šå¯èƒ½æœ‰ä¸€å€‹æœ¬åœ° `datasets loading script`ã€‚åœ¨é€™ç¨®æƒ…æ³ä¸‹ï¼Œé€šéå°‡ä»¥ä¸‹è·¯å¾‘ä¹‹ä¸€å‚³éçµ¦ `load_dataset()` ä¾†åŠ è¼‰æ•¸æ“šé›†ï¼š

- `datasets loading script` çš„æœ¬åœ°è·¯å¾‘ã€‚
- åŒ…å« `datasets loading script` çš„ç›®éŒ„çš„æœ¬åœ°è·¯å¾‘ï¼ˆåƒ…ç•¶è…³æœ¬æ–‡ä»¶èˆ‡ç›®éŒ„åŒåæ™‚ï¼‰ã€‚

```python
dataset = load_dataset("path/to/local/loading_script/loading_script.py", split="train")

dataset = load_dataset("path/to/local/loading_script", split="train")  # equivalent because the file has the same name as the directory
```

### Edit loading script

æ‚¨é‚„å¯ä»¥å¾ Hub ç·¨è¼¯åŠ è¼‰è…³æœ¬ä»¥æ·»åŠ æ‚¨è‡ªå·±çš„ä¿®æ”¹ã€‚å°‡æ•¸æ“šé›†å­˜å„²åº«ä¸‹è¼‰åˆ°æœ¬åœ°ï¼Œä»¥ä¾¿å¯ä»¥åŠ è¼‰åŠ è¼‰è…³æœ¬ä¸­ç›¸å°è·¯å¾‘å¼•ç”¨çš„ä»»ä½•æ•¸æ“šæ–‡ä»¶ï¼š

```python
git clone https://huggingface.co/datasets/eli5
```

å° `datasets loading script` é€²è¡Œç·¨è¼¯ï¼Œç„¶å¾Œé€šéå°‡å…¶æœ¬åœ°è·¯å¾‘å‚³éçµ¦ `load_dataset()` ä¾†åŠ è¼‰å®ƒï¼š

```python
from datasets import load_dataset

eli5 = load_dataset("path/to/local/eli5")
```

## Local and remote files

å¯ä»¥å¾è¨ˆç®—æ©Ÿä¸Šå­˜å„²çš„æœ¬åœ°æ–‡ä»¶å’Œé ç¨‹æ–‡ä»¶åŠ è¼‰æ•¸æ“šé›†ã€‚æ•¸æ“šé›†å¾ˆå¯èƒ½å­˜å„²ç‚º `csv`ã€`json`ã€`txt` æˆ– `parquet` æ–‡ä»¶ã€‚ `load_dataset()` å‡½æ•¸å¯ä»¥åŠ è¼‰æ¯ç¨®æ–‡ä»¶é¡å‹ã€‚

### CSV

ğŸ¤— æ•¸æ“šé›†å¯ä»¥è®€å–ç”±ä¸€å€‹æˆ–å¤šå€‹ CSV æ–‡ä»¶çµ„æˆçš„æ•¸æ“šé›†ï¼ˆåœ¨æœ¬ä¾‹ä¸­ï¼Œå°‡ CSV æ–‡ä»¶ä½œç‚ºåˆ—è¡¨å‚³éï¼‰ï¼š

```python
from datasets import load_dataset

dataset = load_dataset("csv", data_files="my_file.csv")
```

è¦é€šé `S3(AWS/Minio)` åŠ è¼‰é ç¨‹ CSV æ–‡ä»¶ï¼Œè«‹è¨­å®š `storage_options` ç‰©ä»¶ï¼š

```python
storage_options = {
        "anon": False,  # for anonymous connection
        "key": "***********",  # access_key
        "secret": "***********",  # secret_key
        "use_ssl": False,  # use https or http
        "client_kwargs": {
            "endpoint_url": "http://localhost:9000"  # minio endpoint
        }
    }


data_files="s3://my_bucket/imdb/titanic/train.csv"

dataset = load_dataset("csv", data_files=data_files, storage_options=storage_options)
```

!!! tip
    æœ‰é—œæ›´å¤šè©³ç´°ä¿¡æ¯ï¼Œè«‹æŸ¥çœ‹ [how to load tabular datasets from CSV files ](https://huggingface.co/docs/datasets/tabular_load#csv-files) æŒ‡å—ã€‚

### JSON

JSON æ–‡ä»¶ç›´æ¥ä½¿ç”¨ `load_dataset()` åŠ è¼‰ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
from datasets import load_dataset

dataset = load_dataset("json", data_files="my_file.json")
```

JSON æ–‡ä»¶æœ‰å¤šç¨®æ ¼å¼ï¼Œä½†æˆ‘å€‘èªç‚ºæœ€æœ‰æ•ˆçš„æ ¼å¼æ˜¯æ“æœ‰å¤šå€‹ JSON å°è±¡ï¼›æ¯è¡Œä»£è¡¨ä¸€è¡Œæ•¸æ“šã€‚ä¾‹å¦‚ï¼š

```json
{"a": 1, "b": 2.0, "c": "foo", "d": false}
{"a": 4, "b": -5.5, "c": null, "d": true}
```

æ‚¨å¯èƒ½é‡åˆ°çš„å¦ä¸€ç¨® JSON æ ¼å¼æ˜¯åµŒå¥—å­—æ®µï¼Œåœ¨é€™ç¨®æƒ…æ³ä¸‹ï¼Œæ‚¨éœ€è¦æŒ‡å®šæ¬„ä½åƒæ•¸ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```json
{"version": "0.1.0",
 "data": [{"a": 1, "b": 2.0, "c": "foo", "d": false},
          {"a": 4, "b": -5.5, "c": null, "d": true}]
}
```

```python
from datasets import load_dataset

dataset = load_dataset("json", data_files="my_file.json", field="data")
```

è¦é€šé HTTP åŠ è¼‰é ç¨‹ JSON æ–‡ä»¶ï¼Œè«‹å‚³é URLï¼š

```python
base_url = "https://rajpurkar.github.io/SQuAD-explorer/dataset/"

dataset = load_dataset("json", data_files={"train": base_url + "train-v1.1.json", "validation": base_url + "dev-v1.1.json"}, field="data")
```

è¦é€šé `S3(AWS/Minio)` åŠ è¼‰é ç¨‹ JSON æ–‡ä»¶ï¼Œè«‹è¨­å®š `storage_options` ç‰©ä»¶ï¼š

```python
storage_options = {
        "anon": False,  # for anonymous connection
        "key": "***********",  # access_key
        "secret": "***********",  # secret_key
        "use_ssl": False,  # use https or http
        "client_kwargs": {
            "endpoint_url": "http://localhost:9000"  # minio endpoint
        }
    }


base_url="s3://my_bucket/squad/"

dataset = load_dataset("json", 
  data_files={"train": base_url + "train-v1.1.json", "validation": base_url + "dev-v1.1.json"}, 
  field="data", 
  storage_options=storage_options)
```

é›–ç„¶é€™äº›æ˜¯æœ€å¸¸è¦‹çš„ JSON æ ¼å¼ï¼Œä½†æ‚¨æœƒçœ‹åˆ°å…¶ä»–æ ¼å¼ä¸åŒçš„æ•¸æ“šé›†ã€‚ ğŸ¤— æ•¸æ“šé›†å¯è­˜åˆ¥é€™äº›å…¶ä»–æ ¼å¼ï¼Œä¸¦å°‡ç›¸æ‡‰åœ°å›é€€åˆ° Python JSON åŠ è¼‰æ–¹æ³•ä¾†è™•ç†å®ƒå€‘ã€‚

### Parquet

`Parquet` æ–‡ä»¶ä»¥åˆ—æ ¼å¼å­˜å„²ï¼Œèˆ‡ `CSV` ç­‰åŸºæ–¼è¡Œçš„æ–‡ä»¶ä¸åŒã€‚å¤§å‹æ•¸æ“šé›†å¯ä»¥å­˜å„²åœ¨ `Parquet` æ–‡ä»¶ä¸­ï¼Œå› ç‚ºå®ƒåœ¨è¿”å›æŸ¥è©¢æ™‚æ›´é«˜æ•ˆã€æ›´å¿«é€Ÿã€‚

è¦åŠ è¼‰ `Parquet` æ–‡ä»¶ï¼š

```python
from datasets import load_dataset

dataset = load_dataset("parquet", data_files={'train': 'train.parquet', 'test': 'test.parquet'})
```

è¦é€šé HTTP åŠ è¼‰é ç¨‹ Parquet æ–‡ä»¶ï¼Œè«‹å‚³é URLï¼š

```python
base_url = "https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/"

data_files = {"train": base_url + "wikipedia-train.parquet"}

wiki = load_dataset("parquet", data_files=data_files, split="train")
```

è¦é€šé `S3(AWS/Minio)` åŠ è¼‰é ç¨‹ JSON æ–‡ä»¶ï¼Œè«‹è¨­å®š `storage_options` ç‰©ä»¶ï¼š

```python
storage_options = {
        "anon": False,  # for anonymous connection
        "key": "***********",  # access_key
        "secret": "***********",  # secret_key
        "use_ssl": False,  # use https or http
        "client_kwargs": {
            "endpoint_url": "http://localhost:9000"  # minio endpoint
        }
    }


base_url="s3://my_bucket/datasets/wikipedia/"

data_files = {"train": base_url + "wikipedia-train.parquet"}

wiki = load_dataset(
  "parquet", 
  data_files={"train": base_url + "wikipedia-train.parquet"},
  storage_options=storage_options)
```


### Arrow

èˆ‡ `CSV` ç­‰åŸºæ–¼è¡Œçš„æ ¼å¼å’Œ `Parquet` ç­‰æœªå£“ç¸®æ ¼å¼ä¸åŒï¼Œ`Arrow` æ–‡ä»¶ä»¥å…§å­˜ä¸­çš„åˆ—æ ¼å¼å­˜å„²ã€‚

è¦åŠ è¼‰ `Arrow` æ–‡ä»¶ï¼š

```python
from datasets import load_dataset

dataset = load_dataset("arrow", data_files={'train': 'train.arrow', 'test': 'test.arrow'})
```

è¦é€šé HTTP åŠ è¼‰é ç¨‹ Arrow æ–‡ä»¶ï¼Œè«‹å‚³é URLï¼š

```python
base_url = "https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/"

data_files = {"train": base_url + "wikipedia-train.arrow"}

wiki = load_dataset("arrow", data_files=data_files, split="train")
```

è¦é€šé `S3(AWS/Minio)` åŠ è¼‰é ç¨‹ JSON æ–‡ä»¶ï¼Œè«‹è¨­å®š `storage_options` ç‰©ä»¶ï¼š

```python
storage_options = {
        "anon": False,  # for anonymous connection
        "key": "***********",  # access_key
        "secret": "***********",  # secret_key
        "use_ssl": False,  # use https or http
        "client_kwargs": {
            "endpoint_url": "http://localhost:9000"  # minio endpoint
        }
    }


base_url="s3://my_bucket/datasets/wikipedia/"

data_files = {"train": base_url + "wikipedia-train.arrow"}

wiki = load_dataset(
  "arrow", 
  data_files={"train": base_url + "wikipedia-train.parquet"},
  storage_options=storage_options)
```

`Arrow` æ˜¯ ğŸ¤— Datasets åœ¨åº•å±¤ä½¿ç”¨çš„æ–‡ä»¶æ ¼å¼ï¼Œå› æ­¤æ‚¨å¯ä»¥ç›´æ¥ä½¿ç”¨ `Dataset.from_file()` åŠ è¼‰æœ¬åœ° `Arrow` æ–‡ä»¶ï¼š

```python
from datasets import Dataset

dataset = Dataset.from_file("data.arrow")
```

èˆ‡ `load_dataset()` ä¸åŒï¼Œ`Dataset.from_file()` å…§å­˜æ˜ å°„ `Arrow` æ–‡ä»¶ï¼Œç„¡éœ€åœ¨ç·©å­˜ä¸­æº–å‚™æ•¸æ“šé›†ï¼Œå¾è€Œç¯€çœç£ç›¤ç©ºé–“ã€‚åœ¨é€™ç¨®æƒ…æ³ä¸‹ï¼Œå­˜å„²ä¸­é–“è™•ç†çµæœçš„ç·©å­˜ç›®éŒ„å°‡æ˜¯ Arrow æ–‡ä»¶ç›®éŒ„ã€‚

ç›®å‰åƒ…æ”¯æŒ Arrow streaming æ ¼å¼ã€‚ä¸æ”¯æŒ Arrow IPC æ–‡ä»¶æ ¼å¼ï¼ˆä¹Ÿç¨±ç‚º Feather V2ï¼‰ã€‚

### SQL

é€šéæŒ‡å®šé€£æ¥åˆ°æ•¸æ“šåº«çš„ URIï¼Œä½¿ç”¨ `from_sql()` è®€å–æ•¸æ“šåº«å…§å®¹ã€‚æ‚¨å¯ä»¥è®€å–è¡¨åå’ŒæŸ¥è©¢ï¼š

```python
from datasets import Dataset

dataset = Dataset.from_sql("data_table_name", con="sqlite:///sqlite_file.db")

dataset = Dataset.from_sql("SELECT text FROM table WHERE length(text) > 100 LIMIT 10", con="sqlite:///sqlite_file.db")
```

!!! tip
    æœ‰é—œæ›´å¤šè©³ç´°ä¿¡æ¯ï¼Œè«‹æŸ¥çœ‹ [how to load tabular datasets from SQL databases](https://huggingface.co/docs/datasets/tabular_load#databases) æŒ‡å—ã€‚

## Multiprocessing

ç•¶æ•¸æ“šé›†ç”±å¤šå€‹æ–‡ä»¶ï¼ˆæˆ‘å€‘ç¨±ç‚ºâ€œåˆ†ç‰‡â€ï¼‰çµ„æˆæ™‚ï¼Œå¯ä»¥é¡¯è‘—åŠ å¿«æ•¸æ“šé›†ä¸‹è¼‰å’Œæº–å‚™æ­¥é©Ÿã€‚

æ‚¨å¯ä»¥ä½¿ç”¨ `num_proc` é¸æ“‡è¦ä½¿ç”¨å¤šå°‘å€‹é€²ç¨‹ä¾†ä¸¦è¡Œæº–å‚™æ•¸æ“šé›†ã€‚åœ¨é€™ç¨®æƒ…æ³ä¸‹ï¼Œæ¯å€‹é€²ç¨‹éƒ½æœƒç²å¾—ä¸€å€‹è¦æº–å‚™çš„åˆ†ç‰‡å­é›†ï¼š

```python
from datasets import load_dataset

oscar_afrikaans = load_dataset("oscar-corpus/OSCAR-2201", "af", num_proc=8)

imagenet = load_dataset("imagenet-1k", num_proc=8)

ml_librispeech_spanish = load_dataset("facebook/multilingual_librispeech", "spanish", num_proc=8)
```

## In-memory data

ğŸ¤— æ•¸æ“šé›†é‚„å…è¨±æ‚¨ç›´æ¥å¾å…§å­˜æ•¸æ“šçµæ§‹ï¼ˆå¦‚ Python å­—å…¸å’Œ Pandas DataFramesï¼‰å‰µå»ºæ•¸æ“šé›†ã€‚

### Python dictionary

ä½¿ç”¨ `from_dict()` åŠ è¼‰ Python å­—å…¸ï¼š

```python
from datasets import Dataset

my_dict = {"a": [1, 2, 3]}

dataset = Dataset.from_dict(my_dict)
```

### Python list of dictionaries

ä½¿ç”¨ `from_list()` åŠ è¼‰ Python å­—å…¸åˆ—è¡¨ï¼š

```python
from datasets import Dataset

my_list = [{"a": 1}, {"a": 2}, {"a": 3}]

dataset = Dataset.from_list(my_list)
```

### Python generator

ä½¿ç”¨ `from_generator()` å¾ Python ç”Ÿæˆå™¨å‰µå»ºæ•¸æ“šé›†ï¼š

```python
from datasets import Dataset

def my_gen():
    for i in range(1, 4):
        yield {"a": i}

dataset = Dataset.from_generator(my_gen)
```

æ­¤æ–¹æ³•æ”¯æŒåŠ è¼‰å¤§æ–¼å¯ç”¨å…§å­˜çš„æ•¸æ“šã€‚

æ‚¨é‚„å¯ä»¥é€šéå°‡åˆ—è¡¨å‚³éçµ¦ `gen_kwargs ä¾†å®šç¾©åˆ†ç‰‡æ•¸æ“šé›†ï¼š

```python
def gen(shards):
    for shard in shards:
        with open(shard) as f:
            for line in f:
                yield {"line": line}

shards = [f"data{i}.txt" for i in range(32)]
ds = IterableDataset.from_generator(gen, gen_kwargs={"shards": shards})
ds = ds.shuffle(seed=42, buffer_size=10_000)  # shuffles the shards order + uses a shuffle buffer

from torch.utils.data import DataLoader

dataloader = DataLoader(ds.with_format("torch"), num_workers=4)  # give each worker a subset of 32/4=8 shards
```

### Pandas DataFrame

ä½¿ç”¨ `from_pandas()` åŠ è¼‰ Pandas DataFrameï¼š

```python
from datasets import Dataset
import pandas as pd

df = pd.DataFrame({"a": [1, 2, 3]})

dataset = Dataset.from_pandas(df)
```

!!! tip
    æœ‰é—œæ›´å¤šè©³ç´°ä¿¡æ¯ï¼Œè«‹æŸ¥çœ‹ [how to load tabular datasets from Pandas DataFrames ](https://huggingface.co/docs/datasets/tabular_load#pandas-dataframes) æŒ‡å—ã€‚

## Offline

å³ä½¿æ‚¨æ²’æœ‰äº’è¯ç¶²é€£æ¥ï¼Œä»ç„¶å¯ä»¥åŠ è¼‰æ•¸æ“šé›†ã€‚åªè¦æ‚¨ä¹‹å‰å¾ Hub å­˜å„²åº«ä¸‹è¼‰éæ•¸æ“šé›†ï¼Œå°±æ‡‰è©²å°å…¶é€²è¡Œç·©å­˜ã€‚é€™æ„å‘³è‘—æ‚¨å¯ä»¥å¾ç·©å­˜ä¸­é‡æ–°åŠ è¼‰æ•¸æ“šé›†ä¸¦é›¢ç·šä½¿ç”¨ã€‚

å¦‚æœæ‚¨çŸ¥é“è‡ªå·±ç„¡æ³•è¨ªå•äº’è¯ç¶²ï¼Œå‰‡å¯ä»¥åœ¨å®Œå…¨é›¢ç·šæ¨¡å¼ä¸‹é‹è¡Œ ğŸ¤— æ•¸æ“šé›†ã€‚é€™å¯ä»¥ç¯€çœæ™‚é–“ï¼Œå› ç‚ºæ•¸æ“šé›†å°‡ç›´æ¥åœ¨ç·©å­˜ä¸­æŸ¥æ‰¾ï¼Œè€Œä¸æ˜¯ç­‰å¾…æ•¸æ“šé›†ç”Ÿæˆå™¨ä¸‹è¼‰è¶…æ™‚ã€‚å°‡ç’°å¢ƒè®Šé‡ `HF_DATASETS_OFFLINE` è¨­ç½®ç‚º `1` ä»¥å•Ÿç”¨å®Œå…¨é›¢ç·šæ¨¡å¼ã€‚

## Slice splits

æ‚¨é‚„å¯ä»¥é¸æ“‡åƒ…åŠ è¼‰æ‹†åˆ†çš„ç‰¹å®šåˆ‡ç‰‡ã€‚æœ‰å…©ç¨®ç”¨æ–¼åˆ†å‰²çš„é¸é …ï¼šä½¿ç”¨å­—ç¬¦ä¸²æˆ– [`ReadInstruction API`](https://huggingface.co/docs/datasets/v2.14.0/en/package_reference/builder_classes#datasets.ReadInstruction)ã€‚å°æ–¼ç°¡å–®æƒ…æ³ï¼Œå­—ç¬¦ä¸²æ›´åŠ ç·Šæ¹Šå’Œå¯è®€ï¼Œè€Œ `ReadInstruction` æ›´æ˜“æ–¼èˆ‡å¯è®Šåˆ‡ç‰‡åƒæ•¸ä¸€èµ·ä½¿ç”¨ã€‚

ä¸²æ¥ `train` å’Œ `test` splitï¼š

=== "String API"

    ```python
    train_test_ds = datasets.load_dataset("bookcorpus", split="train+test")
    ```

=== "ReadInstruction"

    ```python
    ri = datasets.ReadInstruction("train") + datasets.ReadInstruction("test")

    train_test_ds = datasets.load_dataset("bookcorpus", split=ri)
    ```

é¸æ“‡ `train` split çš„ç‰¹å®š rows æ•¸æ“šï¼š

=== "String API"

    ```python
    train_10_20_ds = datasets.load_dataset("bookcorpus", split="train[10:20]")
    ```

=== "ReadInstruction"

    ```python
    train_10_20_ds = datasets.load_dataset("bookcorpu", split=datasets.ReadInstruction("train", from_=10, to=20, unit="abs"))
    ```

æˆ–è€…é¸æ“‡æŸå€‹ split ç‰¹å®šç™¾åˆ†æ¯”çš„æ•¸æ“šï¼š

=== "String API"

    ```python
    train_10pct_ds = datasets.load_dataset("bookcorpus", split="train[:10%]")
    ```

=== "ReadInstruction"

    ```python
    train_10_20_ds = datasets.load_dataset("bookcorpus", split=datasets.ReadInstruction("train", to=10, unit="%"))
    ```

å¾æ¯å€‹ split ä¸­é¸æ“‡ç‰¹å®šç™¾åˆ†æ¯”çš„çµ„åˆæ•¸æ“šï¼š

=== "String API"

    ```python
    train_10_80pct_ds = datasets.load_dataset("bookcorpus", split="train[:10%]+train[-80%:]")
    ```

=== "ReadInstruction"

    ```python
    ri = (datasets.ReadInstruction("train", to=10, unit="%") + datasets.ReadInstruction("train", from_=-80, unit="%"))

    train_10_80pct_ds = datasets.load_dataset("bookcorpus", split=ri)
    ```

æœ€å¾Œï¼Œæ‚¨ç”šè‡³å¯ä»¥å‰µå»ºäº¤å‰é©—è­‰çš„åˆ†å‰²ã€‚ä¸‹é¢çš„ç¯„ä¾‹å‰µå»º 10-fold cross-validated splitsã€‚æ¯å€‹é©—è­‰æ•¸æ“šé›†éƒ½æ˜¯ 10% çš„é‡ï¼Œè¨“ç·´æ•¸æ“šé›†æ§‹æˆå‰©é¤˜çš„ 90% çš„é‡ï¼š

=== "String API"

    ```python
    val_ds = datasets.load_dataset("bookcorpus", split=[f"train[{k}%:{k+10}%]" for k in range(0, 100, 10)])

    train_ds = datasets.load_dataset("bookcorpus", split=[f"train[:{k}%]+train[{k+10}%:]" for k in range(0, 100, 10)])
    ```

=== "ReadInstruction"

    ```python
    val_ds = datasets.load_dataset("bookcorpus", [datasets.ReadInstruction("train", from_=k, to=k+10, unit="%") for k in range(0, 100, 10)])

    train_ds = datasets.load_dataset("bookcorpus", [(datasets.ReadInstruction("train", to=k, unit="%") + datasets.ReadInstruction("train", from_=k+10, unit="%")) for k in range(0, 100, 10)])
    ```

###ã€€Percent slicing and rounding

å°æ–¼è«‹æ±‚çš„åˆ‡ç‰‡é‚Šç•Œç„¡æ³•è¢« `100` æ•´é™¤çš„æ•¸æ“šé›†ï¼Œé»˜èªè¡Œç‚ºæ˜¯å°‡é‚Šç•Œèˆå…¥ç‚ºæœ€æ¥è¿‘çš„æ•´æ•¸ã€‚å¦‚ä¸‹æ‰€ç¤ºï¼ŒæŸäº›åˆ‡ç‰‡å¯èƒ½æ¯”å…¶ä»–åˆ‡ç‰‡åŒ…å«æ›´å¤š examples æ•¸æ“šã€‚ä¾‹å¦‚ï¼Œå¦‚æœä»¥ä¸‹ `train` åˆ†å‰²åŒ…å« 999 æ¢è¨˜éŒ„ï¼š

```python
# 19 records, from 500 (included) to 519 (excluded).
train_50_52_ds = datasets.load_dataset("bookcorpus", split="train[50%:52%]")

# 20 records, from 519 (included) to 539 (excluded).
train_52_54_ds = datasets.load_dataset("bookcorpus", split="train[52%:54%]")
```

å¦‚æœæ‚¨æƒ³è¦ç›¸åŒå¤§å°çš„åˆ†å‰²ï¼Œè«‹ä½¿ç”¨ `pct1_dropremainder` èˆå…¥ä»£æ›¿ã€‚é€™å°‡æŒ‡å®šçš„ç™¾åˆ†æ¯”é‚Šç•Œè¦–ç‚º 1% çš„å€æ•¸ã€‚

```python
# 18 records, from 450 (included) to 468 (excluded).
train_50_52pct1_ds = datasets.load_dataset("bookcorpus", split=datasets.ReadInstruction("train", from_=50, to=52, unit="%", rounding="pct1_dropremainder"))

# 18 records, from 468 (included) to 486 (excluded).
train_52_54pct1_ds = datasets.load_dataset("bookcorpus", split=datasets.ReadInstruction("train",from_=52, to=54, unit="%", rounding="pct1_dropremainder"))

# Or equivalently:
train_50_52pct1_ds = datasets.load_dataset("bookcorpus", split="train[50%:52%](pct1_dropremainder)")
train_52_54pct1_ds = datasets.load_dataset("bookcorpus", split="train[52%:54%](pct1_dropremainder)")
```

!!! info
    å¦‚æœæ•¸æ“šé›†ä¸­çš„ç¤ºä¾‹æ•¸é‡ä¸èƒ½è¢« 100 æ•´é™¤ï¼Œå‰‡ `pct1_dropremainder` èˆå…¥å¯èƒ½æœƒæˆªæ–·æ•¸æ“šé›†ä¸­çš„æœ€å¾Œä¸€å€‹ç¤ºä¾‹ã€‚

## Troubleshooting

æœ‰æ™‚ï¼ŒåŠ è¼‰æ•¸æ“šé›†æ™‚å¯èƒ½æœƒå¾—åˆ°æ„æƒ³ä¸åˆ°çš„çµæœã€‚æ‚¨å¯èƒ½é‡åˆ°çš„å…©å€‹æœ€å¸¸è¦‹çš„å•é¡Œæ˜¯æ‰‹å‹•ä¸‹è¼‰æ•¸æ“šé›†å’ŒæŒ‡å®šæ•¸æ“šé›†çš„ç‰¹å¾µã€‚

### Manual download

ç”±æ–¼è¨±å¯ä¸å…¼å®¹æˆ–è€…æ–‡ä»¶éš±è—åœ¨ç™»éŒ„é é¢å¾Œé¢ï¼ŒæŸäº›æ•¸æ“šé›†è¦æ±‚æ‚¨æ‰‹å‹•ä¸‹è¼‰æ•¸æ“šé›†æ–‡ä»¶ã€‚é€™æœƒå°è‡´ `load_dataset()` æ‹‹å‡ºéŒ¯èª¤ã€‚ä½†æ˜¯ğŸ¤—æ•¸æ“šé›†æä¾›äº†ä¸‹è¼‰ä¸Ÿå¤±æ–‡ä»¶çš„è©³ç´°èªªæ˜ã€‚ä¸‹è¼‰æ–‡ä»¶å¾Œï¼Œä½¿ç”¨ `data_dir` åƒæ•¸æŒ‡å®šå‰›å‰›ä¸‹è¼‰çš„æ–‡ä»¶çš„è·¯å¾‘ã€‚

ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨å˜—è©¦å¾ [MATINF æ•¸æ“šé›†](https://huggingface.co/datasets/matinf)ä¸‹è¼‰æŸä¸€å€‹é…ç½®ï¼š

```python
dataset = load_dataset("matinf", "summarization")
```

çµæœ:

```bash
Downloading and preparing dataset matinf/summarization (download: Unknown size, generated: 246.89 MiB, post-processed: Unknown size, total: 246.89 MiB) to /root/.cache/huggingface/datasets/matinf/summarization/1.0.0/82eee5e71c3ceaf20d909bca36ff237452b4e4ab195d3be7ee1c78b53e6f540e...

AssertionError: The dataset matinf with config summarization requires manual data. 

Please follow the manual download instructions: To use MATINF you have to download it manually. Please fill this google form (https://forms.gle/nkH4LVE4iNQeDzsc9). You will receive a download link and a password once you complete the form. Please extract all files in one folder and load the dataset with: *datasets.load_dataset('matinf', data_dir='path/to/folder/folder_name')*. 

Manual data can be loaded with `datasets.load_dataset(matinf, data_dir='<path/to/manual/data>') 
```

å¦‚æœæ‚¨å·²ç¶“ä½¿ç”¨ `loading script` å¾ Hub ä¸‹è¼‰äº†æ•¸æ“šé›†åˆ°æ‚¨çš„è¨ˆç®—æ©Ÿï¼Œé‚£éº¼æ‚¨éœ€è¦å°‡çµ•å°è·¯å¾‘å‚³éçµ¦ `data_dir` æˆ– `data_files` åƒæ•¸ä¾†åŠ è¼‰è©²æ•¸æ“šé›†ã€‚å¦å‰‡ï¼Œå¦‚æœå‚³éç›¸å°è·¯å¾‘ï¼Œ`load_dataset()` å°‡å¾ Hub ä¸Šçš„å­˜å„²åº«åŠ è¼‰ç›®éŒ„ï¼Œè€Œä¸æ˜¯æœ¬åœ°ç›®éŒ„ã€‚

### Specify features

ç•¶æ‚¨å¾æœ¬åœ°æ–‡ä»¶å‰µå»ºæ•¸æ“šé›†æ™‚ï¼ŒApache Arrow æœƒè‡ªå‹•æ¨æ–·ç‰¹å¾µã€‚ç„¶è€Œï¼Œæ•¸æ“šé›†çš„ç‰¹å¾µå¯èƒ½ä¸¦ä¸ç¸½æ˜¯ç¬¦åˆæ‚¨çš„æœŸæœ›ï¼Œæˆ–è€…æ‚¨å¯èƒ½æƒ³è‡ªå·±å®šç¾©é€™äº›ç‰¹å¾µã€‚ä»¥ä¸‹ç¤ºä¾‹é¡¯ç¤ºå¦‚ä½•ä½¿ç”¨ [`ClassLabel`](https://huggingface.co/docs/datasets/v2.14.0/en/package_reference/main_classes#datasets.ClassLabel) åŠŸèƒ½æ·»åŠ è‡ªå®šç¾©æ¨™ç±¤ã€‚

é¦–å…ˆä½¿ç”¨ [Features](https://huggingface.co/docs/datasets/v2.14.0/en/package_reference/main_classes#datasets.Features) class å®šç¾©æ‚¨è‡ªå·±çš„æ¨™ç±¤ï¼š

```python
class_names = ["sadness", "joy", "love", "anger", "fear", "surprise"]

emotion_features = Features({'text': Value('string'), 'label': ClassLabel(names=class_names)})
```

æ¥ä¸‹ä¾†ï¼Œä½¿ç”¨æ‚¨å‰›å‰›å‰µå»ºçš„ç‰¹å¾µæŒ‡å®š `load_dataset()` ä¸­çš„ `features` åƒæ•¸ï¼š

```python
dataset = load_dataset('csv', data_files=file_dict, delimiter=';', column_names=['text', 'label'], features=emotion_features)
```

ç¾åœ¨ï¼Œç•¶æ‚¨æŸ¥çœ‹æ•¸æ“šé›†åŠŸèƒ½æ™‚ï¼Œæ‚¨å¯ä»¥çœ‹åˆ°å®ƒä½¿ç”¨æ‚¨å®šç¾©çš„è‡ªå®šç¾©æ¨™ç±¤ï¼š

```python
print(dataset['train'].features)
```

çµæœ:

```bash
{'text': Value(dtype='string', id=None),
'label': ClassLabel(num_classes=6, names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'], names_file=None, id=None)}
```