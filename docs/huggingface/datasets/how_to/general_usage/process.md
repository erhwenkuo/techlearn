# Process

ğŸ¤— Datasets æä¾›äº†è¨±å¤šç”¨æ–¼ä¿®æ”¹æ•¸æ“šé›†çš„çµæ§‹å’Œå…§å®¹çš„å·¥å…·ã€‚é€™äº›å·¥å…·å°æ–¼æ•´ç†æ•¸æ“šé›†ã€å‰µå»ºé™„åŠ åˆ—ã€åœ¨åŠŸèƒ½å’Œæ ¼å¼ä¹‹é–“é€²è¡Œè½‰æ›ç­‰éå¸¸é‡è¦ã€‚

æœ¬æŒ‡å—å°‡å‘æ‚¨å±•ç¤ºå¦‚ä½•ï¼š

- å°è¡Œ(rows)é‡æ–°æ’åºä¸¦æ‹†åˆ†æ•¸æ“šé›†ã€‚
- é‡å‘½åå’Œåˆªé™¤åˆ—(columns)ä»¥åŠå…¶ä»–å¸¸è¦‹çš„åˆ—æ“ä½œã€‚
- å°‡ processing function æ‡‰ç”¨æ–¼æ•¸æ“šé›†ä¸­çš„æ¯å€‹ç¤ºä¾‹è¡Œ(row)ã€‚
- ä¸²æ¥æ•¸æ“šé›†ã€‚
- æ‡‰ç”¨è‡ªå®šç¾©æ ¼å¼è½‰æ›ã€‚
- ä¿å­˜ä¸¦å°å‡ºè™•ç†å¾Œçš„æ•¸æ“šé›†ã€‚

æœ‰é—œè™•ç†å…¶ä»–æ•¸æ“šé›†æ¨¡æ…‹çš„æ›´å¤šè©³ç´°ä¿¡æ¯ï¼Œè«‹æŸ¥çœ‹ [process audio dataset](https://huggingface.co/docs/datasets/audio_process) æŒ‡å—ã€[process image dataset](https://huggingface.co/docs/datasets/image_process) æŒ‡å—æˆ– [process text dataset](https://huggingface.co/docs/datasets/nlp_process) æŒ‡å—ã€‚

æœ¬æŒ‡å—ä¸­çš„ç¤ºä¾‹ä½¿ç”¨ [MRPC]() æ•¸æ“šé›†ï¼Œä½†è«‹éš¨æ„åŠ è¼‰æ‚¨é¸æ“‡çš„ä»»ä½•æ•¸æ“šé›†ä¸¦ç¹¼çºŒæ“ä½œï¼

```python
from datasets import load_dataset

dataset = load_dataset("glue", "mrpc", split="train")
```

!!! tip
    æœ¬æŒ‡å—ä¸­çš„æ‰€æœ‰è™•ç†æ–¹æ³•éƒ½æœƒè¿”å›ä¸€å€‹æ–°çš„ [Dataset](https://huggingface.co/docs/datasets/v2.14.0/en/package_reference/main_classes#datasets.Dataset) ç‰©ä»¶ã€‚

## Sort, shuffle, select, split, and shard

æœ‰å¹¾å€‹å‡½æ•¸å¯ç”¨æ–¼é‡æ–°æ’åˆ—æ•¸æ“šé›†çš„çµæ§‹ã€‚é€™äº›å‡½æ•¸å°æ–¼åƒ…é¸æ“‡æ‰€éœ€çš„è¡Œã€å‰µå»ºè¨“ç·´å’Œæ¸¬è©¦åˆ†å‰²ä»¥åŠå°‡éå¸¸å¤§çš„æ•¸æ“šé›†åˆ†å‰²æˆè¼ƒå°çš„åˆ†å¡Šéå¸¸æœ‰ç”¨ã€‚

###ã€€Sort

ä½¿ç”¨ `sort()` æ ¹æ“šåˆ—å€¼(column)çš„æ•¸å€¼é€²è¡Œæ’åºã€‚æä¾›çš„åˆ—çš„æ¬„ä½å(column name)å¿…é ˆèˆ‡ NumPy å…¼å®¹ã€‚

```python
dataset["label"][:10]
sorted_dataset = dataset.sort("label")
sorted_dataset["label"][:10]
sorted_dataset["label"][-10:]
```

çµæœ:

```bash
[1, 0, 1, 0, 1, 1, 0, 1, 0, 0]
```

```python
sorted_dataset = dataset.sort("label")

sorted_dataset["label"][:10]
```

çµæœ:

```bash
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
```

```python
sorted_dataset["label"][-10:]
```

çµæœ:

```bash
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

åœ¨åº•å±¤ï¼Œé€™æœƒå‰µå»ºä¸€å€‹æ ¹æ“šåˆ—å€¼æ’åºçš„ç´¢å¼•åˆ—è¡¨ã€‚ç„¶å¾Œï¼Œè©²ç´¢å¼•æ˜ å°„ç”¨æ–¼è¨ªå•åŸºç¤ Arrow è¡¨ä¸­çš„æ­£ç¢ºè¡Œã€‚

### Shuffle

`shuffle()` å‡½æ•¸éš¨æ©Ÿé‡æ–°æ’åˆ—åˆ—å€¼(column values)ã€‚å¦‚æœæ‚¨æƒ³æ›´å¥½åœ°æ§åˆ¶ç”¨æ–¼æ´—ç‰Œæ•¸æ“šé›†çš„ç®—æ³•ï¼Œæ‚¨å¯ä»¥åœ¨æ­¤å‡½æ•¸ä¸­æŒ‡å®š `generator` åƒæ•¸ä¾†ä½¿ç”¨ä¸åŒçš„ `numpy.random.Generator`ã€‚

```python
shuffled_dataset = sorted_dataset.shuffle(seed=42)

shuffled_dataset["label"][:10]
```

çµæœ:

```bash
[1, 1, 1, 0, 1, 1, 1, 1, 1, 0]
```

Shuffling ç²å–ç´¢å¼•åˆ—è¡¨ [0:len(my_dataset)] ä¸¦å‰µå»ºä¸€å€‹æ–°çš„ shuffling çš„ç´¢å¼•æ˜ å°„ã€‚ç„¶è€Œï¼Œä¸€æ—¦æ‚¨çš„æ•¸æ“šé›†å…·æœ‰ç´¢å¼•æ˜ å°„ï¼Œé€Ÿåº¦å°±æœƒè®Šæ…¢ 10 å€ã€‚

é€™æ˜¯å› ç‚ºéœ€è¦ä¸€å€‹é¡å¤–çš„æ­¥é©Ÿä¾†ä½¿ç”¨ç´¢å¼•æ˜ å°„ä¾†è®€å–è¦è®€å–çš„è¡Œç´¢å¼•ï¼Œæœ€é‡è¦çš„æ˜¯ï¼Œæ‚¨ä¸å†è®€å–é€£çºŒçš„æ•¸æ“šå¡Šã€‚

è¦æ¢å¾©é€Ÿåº¦ï¼Œæ‚¨éœ€è¦ä½¿ç”¨ [`Dataset.flatten_indices()`](https://huggingface.co/docs/datasets/v2.14.0/en/package_reference/main_classes#datasets.Dataset.flatten_indices) å†æ¬¡é‡å¯«ç£ç›¤ä¸Šçš„æ•´å€‹æ•¸æ“šé›†ï¼Œé€™æœƒåˆªé™¤ç´¢å¼•æ˜ å°„ã€‚æˆ–è€…ï¼Œæ‚¨å¯ä»¥åˆ‡æ›åˆ° [IterableDataset](https://huggingface.co/docs/datasets/v2.14.0/en/package_reference/main_classes#datasets.IterableDataset) ä¸¦åˆ©ç”¨å…¶å¿«é€Ÿè¿‘ä¼¼çš„æ–¹æ³• [IterableDataset.shuffle()](https://huggingface.co/docs/datasets/v2.14.0/en/package_reference/main_classes#datasets.IterableDataset.shuffle)ï¼š

```python
iterable_dataset = dataset.to_iterable_dataset(num_shards=128)

shuffled_iterable_dataset = iterable_dataset.shuffle(seed=42, buffer_size=1000)
```

### Select and Filter

æœ‰å…©å€‹é¸é …å¯ç”¨æ–¼éæ¿¾æ•¸æ“šé›†ä¸­çš„è¡Œï¼š`select()` å’Œ `filter()`ã€‚

- `select()` æ ¹æ“šç´¢å¼•åˆ—è¡¨è¿”å›è¡Œ(rows)ï¼š

    ```python
    small_dataset = dataset.select([0, 10, 20, 30, 40, 50])

    len(small_dataset)
    ```

    çµæœ:

    ```bash
    6
    ```

- `filter()` è¿”å›ç¬¦åˆæŒ‡å®šæ¢ä»¶çš„è¡Œ(rows)ï¼š

    ```python
    start_with_ar = dataset.filter(lambda example: example["sentence1"].startswith("Ar"))

    print(len(start_with_ar))

    print(start_with_ar["sentence1"])
    ```

    çµæœ:

    ```bash
    6

    ['Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at A $ 4.56 , having earlier set a record high of A $ 4.57 .',
    'Arison said Mann may have been one of the pioneers of the world music movement and he had a deep love of Brazilian music .',
    'Arts helped coach the youth on an eighth-grade football team at Lombardi Middle School in Green Bay .',
    'Around 9 : 00 a.m. EDT ( 1300 GMT ) , the euro was at $ 1.1566 against the dollar , up 0.07 percent on the day .',
    "Arguing that the case was an isolated example , Canada has threatened a trade backlash if Tokyo 's ban is not justified on scientific grounds .",
    'Artists are worried the plan would harm those who need help most - performers who have a difficult time lining up shows .'
    ]    
    ```

    å¦‚æœè¨­å®š `with_indices=True`ï¼Œ`filter()` é‚„å¯ä»¥æŒ‰ç´¢å¼•ä¾†é€²è¡Œéæ¿¾ï¼š

    ```python
    even_dataset = dataset.filter(lambda example, idx: idx % 2 == 0, with_indices=True)

    print(len(even_dataset))

    print(len(dataset) / 2)
    ```

    ```bash
    1834
    
    1834.0
    ```

    é™¤éè¦ä¿ç•™â€‹â€‹çš„ç´¢å¼•åˆ—è¡¨æ˜¯é€£çºŒçš„ï¼Œå¦å‰‡é€™äº›æ–¹æ³•é‚„æœƒåœ¨å¹•å¾Œå‰µå»ºç´¢å¼•æ˜ å°„ã€‚


### Split

å¦‚æœæ‚¨çš„æ•¸æ“šé›†é‚„æ²’æœ‰å€éš” `train` å’Œ `test` çš„ splitsï¼Œ[`train_test_split()`](https://huggingface.co/docs/datasets/v2.14.0/en/package_reference/main_classes#datasets.Dataset.train_test_split) å‡½æ•¸æœƒå‰µå»ºå®ƒå€‘ã€‚é€™å…è¨±æ‚¨èª¿æ•´æ¯å€‹åˆ†å‰²ä¸­æ¨£æœ¬çš„ç›¸å°æ¯”ä¾‹æˆ–çµ•å°æ•¸é‡ã€‚åœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œä½¿ç”¨ `test_size` åƒæ•¸å‰µå»ºä¸€å€‹ä½”åŸå§‹æ•¸æ“šé›† 10% çš„æ¸¬è©¦åˆ†å‰²ï¼š

```python
new_dataset = dataset.train_test_split(test_size=0.1)

print(new_dataset)
```

çµæœ:

```bash
{'train': Dataset(schema: {'sentence1': 'string', 'sentence2': 'string', 'label': 'int64', 'idx': 'int32'}, num_rows: 3301),
'test': Dataset(schema: {'sentence1': 'string', 'sentence2': 'string', 'label': 'int64', 'idx': 'int32'}, num_rows: 367)}
```

æ¯”å°ä¸€ä¸‹åŸæœ‰çš„ dataset:

```python
print(0.1 * len(dataset))
```

çµæœ:

```bash
366.8
```

é»˜èªæƒ…æ³ä¸‹ï¼Œåˆ†å‰²æ˜¯éš¨æ©Ÿæ’åˆ—çš„ï¼Œä½†æ‚¨å¯ä»¥è¨­ç½® `shuffle=False` ä¾†é˜²æ­¢éš¨æ©Ÿæ’åˆ—ã€‚

### Shard

ğŸ¤— æ•¸æ“šé›†æ”¯æŒåˆ†ç‰‡(sharding)ï¼Œå°‡éå¸¸å¤§çš„æ•¸æ“šé›†åŠƒåˆ†ç‚ºé å®šç¾©æ•¸é‡çš„å¡Šã€‚åœ¨ `shard()` ä¸­æŒ‡å®š `num_shards` åƒæ•¸ä¾†ç¢ºå®šå°‡æ•¸æ“šé›†æ‹†åˆ†ç‚ºçš„åˆ†ç‰‡æ•¸é‡ã€‚æ‚¨é‚„éœ€è¦æä¾›è¦ä½¿ç”¨ç´¢å¼•åƒæ•¸è¿”å›çš„åˆ†ç‰‡ã€‚

ä¾‹å¦‚ï¼Œ[imdb](https://huggingface.co/datasets/imdb) æ•¸æ“šé›†æœ‰ `25000` å€‹ç¤ºä¾‹ï¼š

```python
from datasets import load_dataset

datasets = load_dataset("imdb", split="train")

print(dataset)
```

çµæœ:

```bash
Dataset({
    features: ['text', 'label'],
    num_rows: 25000
})
```

å°‡æ•¸æ“šé›†åˆ†ç‰‡ç‚ºå››å€‹å¡Š(chunks)å¾Œï¼Œç¬¬ä¸€å€‹åˆ†ç‰‡(shard)å°‡åªæœ‰ `6250` å€‹ç¤ºä¾‹ï¼š

```python
first_shard_dataset = dataset.shard(num_shards=4, index=0)

print(first_shard_dataset)

print(25000/4)
```

çµæœ:

```bash
Dataset({
    features: ['text', 'label'],
    num_rows: 6250
})

6250.0
```

## Rename, remove, cast, and flatten

ä»¥ä¸‹å‡½æ•¸å…è¨±æ‚¨ä¿®æ”¹æ•¸æ“šé›†çš„åˆ—(column)ã€‚é€™äº›å‡½æ•¸å°æ–¼é‡å‘½åæˆ–åˆªé™¤åˆ—ã€å°‡åˆ—æ›´æ”¹ç‚ºä¸€çµ„æ–°åŠŸèƒ½ä»¥åŠå±•å¹³åµŒå¥—åˆ—çµæ§‹éå¸¸æœ‰ç”¨ã€‚

### Rename

ç•¶æ‚¨éœ€è¦é‡å‘½åæ•¸æ“šé›†ä¸­çš„åˆ—(column)æ™‚ï¼Œè«‹ä½¿ç”¨ `rename_column()`ã€‚èˆ‡åŸå§‹åˆ—é—œè¯çš„ç‰¹å¾µå¯¦éš›ä¸Šè¢«ç§»å‹•åˆ°æ–°åˆ—åç¨±ä¸‹ï¼Œè€Œä¸æ˜¯åƒ…åƒ…å°±åœ°æ›¿æ›åŸå§‹åˆ—ã€‚

ç‚º `rename_column()` æä¾›åŸå§‹åˆ—çš„åç¨±å’Œæ–°åˆ—çš„åç¨±ï¼š

```python
print(dataset)

dataset = dataset.rename_column("sentence1", "sentenceA")
dataset = dataset.rename_column("sentence2", "sentenceB")

print(dataset)
```

çµæœ:

```bash
# before
Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx'],
    num_rows: 3668
})

# after
Dataset({
    features: ['sentenceA', 'sentenceB', 'label', 'idx'],
    num_rows: 3668
})
```

### Remove

ç•¶æ‚¨éœ€è¦åˆªé™¤ä¸€åˆ—æˆ–å¤šåˆ—æ™‚ï¼Œè«‹å‘ `remove_columns()` å‡½æ•¸æä¾›è¦åˆªé™¤çš„åˆ—åç¨±ã€‚é€šéæä¾›åˆ—åç¨±åˆ—è¡¨ä¾†åˆªé™¤å¤šå€‹åˆ—ï¼š

```python
dataset = dataset.remove_columns("label")

print(dataset)
```

çµæœ:

```bash
Dataset({
    features: ['sentence1', 'sentence2', 'idx'],
    num_rows: 3668
})
```

```python
dataset = dataset.remove_columns(["sentence1", "sentence2"])

print(dataset)
```

çµæœ:

```bash
Dataset({
    features: ['idx'],
    num_rows: 3668
})
```

### Cast

`cast()` å‡½æ•¸è½‰æ›ä¸€åˆ—æˆ–å¤šåˆ—çš„ç‰¹å¾µé¡å‹ã€‚è©²å‡½æ•¸æ¥å—æ‚¨çš„æ–°åŠŸèƒ½ä½œç‚ºå…¶åƒæ•¸ã€‚ä¸‹é¢çš„ç¤ºä¾‹æ¼”ç¤ºç­å¦‚ä½•æ›´æ”¹ `ClassLabel` å’Œ `Value` é€™å…©å€‹åœ¨ dataset è£¡çš„ feature æ•¸æ“šï¼š

```python
print(dataset.features)
```

çµæœ:

```bash
{'sentence1': Value(dtype='string', id=None),
'sentence2': Value(dtype='string', id=None),
'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
'idx': Value(dtype='int32', id=None)}
```

```python
from datasets import ClassLabel, Value

new_features = dataset.features.copy()

new_features["label"] = ClassLabel(names=["negative", "positive"])

new_features["idx"] = Value("int64")

dataset = dataset.cast(new_features)

print(dataset.features)
```

çµæœ:

```bash
{'sentence1': Value(dtype='string', id=None),
'sentence2': Value(dtype='string', id=None),
'label': ClassLabel(num_classes=2, names=['negative', 'positive'], names_file=None, id=None),
'idx': Value(dtype='int64', id=None)}
```

!!! tip
    åƒ…ç•¶åŸå§‹ feature type å’Œæ–° feature type å…¼å®¹æ™‚ï¼Œ`cast()` æ‰æœ‰æ•ˆã€‚ä¾‹å¦‚ï¼Œå¦‚æœåŸå§‹åˆ—åƒ…åŒ…å« 1 å’Œ 0ï¼Œå‰‡å¯ä»¥å°‡è¦ç´ é¡å‹ `Value("int32")` çš„åˆ—è½‰æ›ç‚º `Value("bool")`ã€‚

ä½¿ç”¨ [`cast_column()`](https://huggingface.co/docs/datasets/v2.14.0/en/package_reference/main_classes#datasets.Dataset.cast_column) å‡½æ•¸æ›´æ”¹å–®å€‹åˆ—çš„è¦ç´ é¡å‹ã€‚å°‡ `column name` åŠ `new feature type` ä½œç‚ºåƒæ•¸å‚³éï¼š

```python
print(dataset.features)
```

çµæœ:

```bash
{'audio': Audio(sampling_rate=44100, mono=True, id=None)}
```

```python
dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))

print(dataset.features)
```

çµæœ:

```bash
{'audio': Audio(sampling_rate=16000, mono=True, id=None)}
```

### Flatten

æœ‰æ™‚ï¼Œåˆ—å¯ä»¥æ˜¯å¤šç¨®é¡å‹çš„åµŒå¥—çµæ§‹ã€‚çœ‹ä¸€ä¸‹ [SQuAD](https://huggingface.co/datasets/squad) æ•¸æ“šé›†ä¸­çš„åµŒå¥—çµæ§‹ï¼š

```python
from datasets import load_dataset

dataset = load_dataset("squad", split="train")

print(dataset.features)
```

çµæœ:

```bash hl_lines="1"
{'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None),
'context': Value(dtype='string', id=None),
'id': Value(dtype='string', id=None),
'question': Value(dtype='string', id=None),
'title': Value(dtype='string', id=None)}
```

`answers` æ¬„ä½åŒ…å«å…©å€‹å­æ¬„ä½ï¼š `text` å’Œ `answer_start`ã€‚ä½¿ç”¨ `flatten()` å‡½æ•¸å°‡å­æ¬„ä½æå–åˆ°å®ƒå€‘è‡ªå·±çš„å–®ç¨åˆ—ä¸­ï¼š

```python
flat_dataset = dataset.flatten()

print(flat_dataset)
```

çµæœ:

```bash hl_lines="1"
Dataset({
    features: ['id', 'title', 'context', 'question', 'answers.text', 'answers.answer_start'],
 num_rows: 87599
})
```

è«‹æ³¨æ„ï¼Œå­æ¬„ä½ç¾åœ¨æ˜¯å®ƒå€‘è‡ªå·±çš„ç¨ç«‹æ¬„ä½äº†ï¼š`answers.text` å’Œ `answer.answer_start`ã€‚

## Map

ğŸ¤— æ•¸æ“šé›†çš„ä¸€äº›æ›´å¼·å¤§çš„æ‡‰ç”¨ä¾†è‡ªæ–¼ä½¿ç”¨ [`map()`](https://huggingface.co/docs/datasets/v2.14.0/en/package_reference/main_classes#datasets.Dataset.map) å‡½æ•¸ã€‚ `map()` çš„ä¸»è¦ç›®çš„æ˜¯åŠ é€Ÿæ•¸æ“šè™•ç†è½‰æ›çš„å‡½æ•¸ã€‚å®ƒå…è¨±æ‚¨ç¨ç«‹æˆ–æ‰¹é‡åœ°å°‡è™•ç†å‡½æ•¸æ‡‰ç”¨æ–¼æ•¸æ“šé›†ä¸­çš„æ¯å€‹ç¤ºä¾‹ã€‚è©²å‡½æ•¸ç”šè‡³å¯ä»¥å‰µå»ºæ–°çš„è¡Œå’Œåˆ—ã€‚

åœ¨ä»¥ä¸‹ç¤ºä¾‹ä¸­ï¼Œåœ¨æ•¸æ“šé›†ä¸­çš„æ¯å€‹ Sentence1 å€¼å‰é¢æ·»åŠ  `My Sentence:` å‰ç¶´ã€‚

é¦–å…ˆå‰µå»ºä¸€å€‹å‡½æ•¸ï¼Œå°‡ 'My sentence: ' æ·»åŠ åˆ°æ¯å€‹ `sentence1` çš„é–‹é ­ã€‚è©²å‡½æ•¸éœ€è¦æ¥å—ä¸¦è¼¸å‡ºä¸€å€‹ `dict`ï¼š

```python
def add_prefix(example):
    example["sentence1"] = 'My sentence: ' + example["sentence1"]
    return example
```

ç¾åœ¨ä½¿ç”¨ `map()` å°‡ `add_prefix` å‡½æ•¸æ‡‰ç”¨åˆ°æ•´å€‹æ•¸æ“šé›†ï¼š

```python
updated_dataset = small_dataset.map(add_prefix)

print(updated_dataset["sentence1"][:5])
```

çµæœ:

```bash
['My sentence: Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
"My sentence: Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .",
'My sentence: They had published an advertisement on the Internet on June 10 , offering the cargo for sale , he added .',
'My sentence: Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at A $ 4.56 , having earlier set a record high of A $ 4.57 .',
]
```

è®“æˆ‘å€‘çœ‹å¦ä¸€å€‹ç¤ºä¾‹ï¼Œåªä¸éé€™æ¬¡æ‚¨å°‡ä½¿ç”¨ `map()` åˆªé™¤ä¸€åˆ—(column)ã€‚ç•¶æ‚¨åˆªé™¤åˆ—æ™‚ï¼Œåªæœ‰åœ¨å°‡ç¤ºä¾‹æä¾›çµ¦æ˜ å°„å‡½æ•¸å¾Œæ‰æœƒåˆªé™¤è©²åˆ—ã€‚é€™å…è¨±æ˜ å°„å‡½æ•¸åœ¨åˆªé™¤åˆ—ä¹‹å‰ä½¿ç”¨å®ƒå€‘çš„å…§å®¹ã€‚

ä½¿ç”¨ `map()` ä¸­çš„ `remove_columns` åƒæ•¸æŒ‡å®šè¦åˆªé™¤çš„åˆ—ï¼š

```python
updated_dataset = dataset.map(lambda example: {"new_sentence": example["sentence1"]}, remove_columns=["sentence1"])

print(updated_dataset.column_names)
```

çµæœ:

```bash
['sentence2', 'label', 'idx', 'new_sentence']
```

!!! tip
    ğŸ¤— æ•¸æ“šé›†é‚„æœ‰ä¸€å€‹ `remove_columns()` å‡½æ•¸ï¼Œè©²å‡½æ•¸é€Ÿåº¦æ›´å¿«ï¼Œå› ç‚ºå®ƒä¸æœƒå¾©åˆ¶å‰©é¤˜åˆ—çš„æ•¸æ“šã€‚

å¦‚æœè¨­ç½® `with_indices=True`ï¼Œæ‚¨é‚„å¯ä»¥å°‡ `map()` èˆ‡ç´¢å¼•ä¸€èµ·ä½¿ç”¨ã€‚ä¸‹é¢çš„ç¤ºä¾‹å°‡ç´¢å¼•æ·»åŠ åˆ°æ¯å€‹å¥å­çš„é–‹é ­ï¼š

```python
updated_dataset = dataset.map(lambda example, idx: {"sentence2": f"{idx}: " + example["sentence2"]}, with_indices=True)

updated_dataset["sentence2"][:5]
```

çµæœ:

```bash
['0: Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .',
 "1: Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .",
 "2: On June 10 , the ship 's owners had published an advertisement on the Internet , offering the explosives for sale .",
 '3: Tab shares jumped 20 cents , or 4.6 % , to set a record closing high at A $ 4.57 .',
 '4: PG & E Corp. shares jumped $ 1.63 or 8 percent to $ 21.03 on the New York Stock Exchange on Friday .'
]
```

å¦‚æœè¨­ç½® `with_rank=True`ï¼Œ`map()` é‚„å¯ä»¥è™•ç†é€²ç¨‹çš„ç­‰ç´šã€‚é€™é¡ä¼¼æ–¼ `with_indices` åƒæ•¸ã€‚å¦‚æœæ˜ å°„å‡½æ•¸ä¸­çš„ `with_rank` åƒæ•¸å·²ç¶“å­˜åœ¨ï¼Œå‰‡è©²åƒæ•¸ä½æ–¼ç´¢å¼• one ä¹‹å¾Œã€‚

```python
from multiprocess import set_start_method
import torch
import os

set_start_method("spawn")

def gpu_computation(example, rank):
    os.environ["CUDA_VISIBLE_DEVICES"] = str(rank % torch.cuda.device_count())
    # Your big GPU call goes here
    return examples

updated_dataset = dataset.map(gpu_computation, with_rank=True)
```

Rank çš„ä¸»è¦ç”¨ä¾‹æ˜¯è·¨å¤šå€‹ GPU ä¸¦è¡Œè¨ˆç®—ã€‚é€™éœ€è¦è¨­ç½® `multiprocess.set_start_method("spawn")`ã€‚å¦‚æœä¸é€™æ¨£åšï¼Œæ‚¨å°‡æ”¶åˆ°ä»¥ä¸‹ CUDA éŒ¯èª¤ï¼š

```bash
RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method.
```

### Multiprocessing

Multiprocessing é€šé CPU ä¸Šçš„ä¸¦è¡Œé€²ç¨‹é¡¯è‘—åŠ å¿«è™•ç†é€Ÿåº¦ã€‚åœ¨ `map()` ä¸­è¨­ç½® `num_proc` åƒæ•¸ä¾†è¨­ç½®è¦ä½¿ç”¨çš„é€²ç¨‹æ•¸ï¼š

```python
updated_dataset = dataset.map(lambda example, idx: {"sentence2": f"{idx}: " + example["sentence2"]}, num_proc=4)
```

### Batch processing

`map()` å‡½æ•¸æ”¯æŒè™•ç† {==æ‰¹é‡==} ç¤ºä¾‹ã€‚é€šéè¨­ç½® `batched=True` ä¾†é€²è¡Œæ‰¹é‡æ“ä½œã€‚é»˜èªæ‰¹é‡å¤§å°ç‚º `1000`ï¼Œä½†æ‚¨å¯ä»¥ä½¿ç”¨ `batch_size` åƒæ•¸é€²è¡Œèª¿æ•´ã€‚æ‰¹é‡è™•ç†å¯ä»¥å¯¦ç¾æœ‰è¶£çš„æ‡‰ç”¨ï¼Œä¾‹å¦‚å°‡é•·å¥å­åˆ†å‰²æˆè¼ƒçŸ­çš„å¡Šå’Œæ•¸æ“šå¢å¼·ã€‚

####ã€€Split long examples

ç•¶ example æ•¸æ“šå¤ªé•·æ™‚ï¼Œæ‚¨å¯èƒ½éœ€è¦å°‡å®ƒå€‘åˆ†æˆå¹¾å€‹è¼ƒå°çš„ chunkã€‚é¦–å…ˆå‰µå»ºä¸€å€‹å‡½æ•¸ï¼š

1. å°‡ Sentence1 æ¬„ä½æ‹†åˆ†ç‚º 50 å€‹å­—ç¬¦çš„å¡Šã€‚
2. å°‡æ‰€æœ‰ chunks å †ç–Šåœ¨ä¸€èµ·ä»¥å‰µå»ºæ–°çš„æ•¸æ“šé›†ã€‚

```python
def chunk_examples(examples):
    chunks = []
    for sentence in examples["sentence1"]:
        chunks += [sentence[i:i + 50] for i in range(0, len(sentence), 50)]
    return {"chunks": chunks}
```

ä½¿ç”¨ `map()` æ‡‰ç”¨è©²å‡½æ•¸ï¼š

```python
chunked_dataset = dataset.map(chunk_examples, batched=True, remove_columns=dataset.column_names)

chunked_dataset[:10]
```

çµæœ:

```bash
{'chunks': ['Amrozi accused his brother , whom he called " the ',
            'witness " , of deliberately distorting his evidenc',
            'e .',
            "Yucaipa owned Dominick 's before selling the chain",
            ' to Safeway in 1998 for $ 2.5 billion .',
            'They had published an advertisement on the Interne',
            't on June 10 , offering the cargo for sale , he ad',
            'ded .',
            'Around 0335 GMT , Tab shares were up 19 cents , or',
            ' 4.4 % , at A $ 4.56 , having earlier set a record']}
```

è«‹æ³¨æ„ç¾åœ¨å¥å­æ˜¯å¦‚ä½•åˆ†å‰²æˆè¼ƒçŸ­çš„å¡Šçš„ï¼Œä¸¦ä¸”æ•¸æ“šé›†ä¸­æœ‰æ›´å¤šçš„è¡Œã€‚

```python
print(dataset)
```

çµæœ:

```bash
Dataset({
 features: ['sentence1', 'sentence2', 'label', 'idx'],
 num_rows: 3668
})
```

```python
print(chunked_dataset)
```

çµæœ:

```bash
Dataset(schema: {'chunks': 'string'}, num_rows: 10470)
```

#### Data augmentation

`map()` å‡½æ•¸ä¹Ÿå¯ç”¨æ–¼æ•¸æ“šå¢å¼·ã€‚ä»¥ä¸‹ç¤ºä¾‹ç‚ºå¥å­ä¸­çš„å±è”½æ¨™è¨˜ç”Ÿæˆé™„åŠ å–®è©ã€‚

åœ¨ ğŸ¤— Transformers çš„ [FillMaskPipeline](https://huggingface.co/transformers/main_classes/pipelines#transformers.FillMaskPipeline) ä¸­åŠ è¼‰ä¸¦ä½¿ç”¨ [RoBERTA](https://huggingface.co/roberta-base) æ¨¡å‹ï¼š

```python
from random import randint
from transformers import pipeline

fillmask = pipeline("fill-mask", model="roberta-base")

mask_token = fillmask.tokenizer.mask_token

smaller_dataset = dataset.filter(lambda e, i: i<100, with_indices=True)
```

å‰µå»ºä¸€å€‹å‡½æ•¸ä¾†éš¨æ©Ÿé¸æ“‡è¦åœ¨å¥å­ä¸­å±è”½çš„å–®è©ã€‚è©²å‡½æ•¸é‚„æ‡‰è¿”å›åŸå§‹å¥å­å’Œ RoBERTA ç”Ÿæˆçš„å‰å…©å€‹æ›¿æ›å¥å­ã€‚

```python
def augment_data(examples):
    outputs = []
    for sentence in examples["sentence1"]:
        words = sentence.split(' ')
        K = randint(1, len(words)-1)
        masked_sentence = " ".join(words[:K]  + [mask_token] + words[K+1:])
        predictions = fillmask(masked_sentence)
        augmented_sequences = [predictions[i]["sequence"] for i in range(3)]
        outputs += [sentence] + augmented_sequences
    return {"data": outputs}
```

ä½¿ç”¨ `map()` å°‡å‡½æ•¸æ‡‰ç”¨æ–¼æ•´å€‹æ•¸æ“šé›†ï¼š

```python
augmented_dataset = smaller_dataset.map(augment_data, batched=True, remove_columns=dataset.column_names, batch_size=8)

augmented_dataset[:9]["data"]
```

çµæœ:

```bash
['Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
 'Amrozi accused his brother, whom he called " the witness ", of deliberately withholding his evidence.',
 'Amrozi accused his brother, whom he called " the witness ", of deliberately suppressing his evidence.',
 'Amrozi accused his brother, whom he called " the witness ", of deliberately destroying his evidence.',
 "Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .",
 'Yucaipa owned Dominick Stores before selling the chain to Safeway in 1998 for $ 2.5 billion.',
 "Yucaipa owned Dominick's before selling the chain to Safeway in 1998 for $ 2.5 billion.",
 'Yucaipa owned Dominick Pizza before selling the chain to Safeway in 1998 for $ 2.5 billion.'
]
```

### Process multiple splits

è¨±å¤šæ•¸æ“šé›†éƒ½æœ‰å¯ä»¥ä½¿ç”¨ `DatasetDict.map()` åŒæ™‚è™•ç†çš„åˆ†å‰²(split)ã€‚ä¾‹å¦‚ï¼Œå°‡ `train` å’Œ `test` ä¸­çš„ Sentence1 æ¬„ä½é€²è¡Œ tokenizingï¼š

```python
from datasets import load_dataset

# load all the splits
dataset = load_dataset('glue', 'mrpc')

encoded_dataset = dataset.map(lambda examples: tokenizer(examples["sentence1"]), batched=True)

encoded_dataset["train"][0]
```

çµæœ:

```bash
{'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .',
'label': 1,
'idx': 0,
'input_ids': [  101,  7277,  2180,  5303,  4806,  1117,  1711,   117,  2292, 1119,  1270,   107,  1103,  7737,   107,   117,  1104,  9938, 4267, 12223, 21811,  1117,  2554,   119,   102],
'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}
```

### Distributed usage

ç•¶ä½ åœ¨åˆ†ä½ˆå¼ç’°å¢ƒä¸­ä½¿ç”¨ `map()` æ™‚ï¼Œä½ é‚„æ‡‰è©²ä½¿ç”¨ `torch.distributed.barrier`ã€‚é€™ç¢ºä¿ä¸»é€²ç¨‹åŸ·è¡Œæ˜ å°„ï¼Œè€Œå…¶ä»–é€²ç¨‹åŠ è¼‰çµæœï¼Œå¾è€Œé¿å…é‡è¤‡å·¥ä½œã€‚

ä»¥ä¸‹ç¤ºä¾‹å±•ç¤ºç­å¦‚ä½•ä½¿ç”¨ `torch.distributed.barrier` ä¾†åŒæ­¥é€²ç¨‹ï¼š

```python
from datasets import Dataset
import torch.distributed

dataset1 = Dataset.from_dict({"a": [0, 1, 2]})

if training_args.local_rank > 0:
    print("Waiting for main process to perform the mapping")
    torch.distributed.barrier()

dataset2 = dataset1.map(lambda x: {"a": x["a"] + 1})

if training_args.local_rank == 0:
    print("Loading results from main process")
    torch.distributed.barrier()
```

## Concatenate

å¦‚æœå–®ç¨çš„æ•¸æ“šé›†å…±äº«ç›¸åŒçš„åˆ—é¡å‹(column type)ï¼Œå‰‡å¯ä»¥å°‡å®ƒå€‘é€£æ¥èµ·ä¾†ã€‚ä½¿ç”¨ `concatenate_datasets()` é€£æ¥æ•¸æ“šé›†ï¼š

```python
from datasets import concatenate_datasets, load_dataset

bookcorpus = load_dataset("bookcorpus", split="train")

wiki = load_dataset("wikipedia", "20220301.en", split="train")

wiki = wiki.remove_columns([col for col in wiki.column_names if col != "text"])  # only keep the 'text' column

assert bookcorpus.features.type == wiki.features.type

bert_dataset = concatenate_datasets([bookcorpus, wiki])
```

æ‚¨é‚„å¯ä»¥é€šéè¨­ç½® `axis=1` æ°´å¹³é€£æ¥å…©å€‹æ•¸æ“šé›†ï¼Œåªè¦æ•¸æ“šé›†å…·æœ‰ç›¸åŒçš„è¡Œæ•¸å³å¯ï¼š

```python
from datasets import Dataset

bookcorpus_ids = Dataset.from_dict({"ids": list(range(len(bookcorpus)))})

bookcorpus_with_ids = concatenate_datasets([bookcorpus, bookcorpus_ids], axis=1)
```

### Interleave

æ‚¨é‚„å¯ä»¥é€šéäº¤æ›¿ä½¿ç”¨æ¯å€‹æ•¸æ“šé›†çš„ç¤ºä¾‹ä¾†å°‡å¤šå€‹æ•¸æ“šé›†æ··åˆåœ¨ä¸€èµ·ï¼Œä»¥å‰µå»ºä¸€å€‹æ–°çš„æ•¸æ“šé›†ã€‚é€™ç¨±ç‚ºäº¤éŒ¯(interleaving)ï¼Œç”± `interleave_datasets()` å‡½æ•¸ä¾†é©…å‹•ã€‚ `interleave_datasets()` å’Œ `concatenate_datasets()` éƒ½é©ç”¨æ–¼å¸¸è¦ `Dataset` å’Œ `IterableDataset` ç‰©ä»¶ã€‚æœ‰é—œå¦‚ä½•äº¤éŒ¯ `IterableDataset` ç‰©ä»¶çš„ç¤ºä¾‹ï¼Œè«‹åƒé–± [Stream](https://huggingface.co/docs/datasets/stream#interleave) æŒ‡å—ã€‚

æ‚¨å¯ä»¥ç‚ºæ¯å€‹åŸå§‹æ•¸æ“šé›†å®šç¾©æ¡æ¨£æ¦‚ç‡ï¼Œä»¥æŒ‡å®šå¦‚ä½•äº¤éŒ¯æ•¸æ“šé›†ã€‚åœ¨é€™ç¨®æƒ…æ³ä¸‹ï¼Œæ–°æ•¸æ“šé›†æ˜¯é€šéå¾éš¨æ©Ÿæ•¸æ“šé›†ä¸­é€ä¸€ç²å–ç¤ºä¾‹ä¾†æ§‹å»ºçš„ï¼Œç›´åˆ°å…¶ä¸­ä¸€å€‹æ•¸æ“šé›†ç”¨å®Œæ¨£æœ¬ç‚ºæ­¢ã€‚

```python
seed = 42
probabilities = [0.3, 0.5, 0.2]

d1 = Dataset.from_dict({"a": [0, 1, 2]})
d2 = Dataset.from_dict({"a": [10, 11, 12, 13]})
d3 = Dataset.from_dict({"a": [20, 21, 22]})

dataset = interleave_datasets([d1, d2, d3], probabilities=probabilities, seed=seed)

print(dataset["a"])
```

çµæœ:

```bash
[10, 11, 20, 12, 0, 21, 13]
```

æ‚¨é‚„å¯ä»¥æŒ‡å®š `stopping_strategy`ã€‚é»˜èªç­–ç•¥ `first_exhausted` æ˜¯äºŒæ¬¡æ¡æ¨£ç­–ç•¥ï¼Œå³ä¸€æ—¦å…¶ä¸­ä¸€å€‹æ•¸æ“šé›†ç”¨å®Œæ¨£æœ¬ï¼Œæ•¸æ“šé›†æ§‹å»ºå°±æœƒåœæ­¢ã€‚

æ‚¨å¯ä»¥æŒ‡å®š `stopping_strategy=all_exhausted` ä¾†åŸ·è¡Œéæ¡æ¨£ç­–ç•¥ã€‚åœ¨é€™ç¨®æƒ…æ³ä¸‹ï¼Œä¸€æ—¦æ¯å€‹æ•¸æ“šé›†ä¸­çš„æ¯å€‹æ¨£æœ¬è¢«æ·»åŠ è‡³å°‘ä¸€æ¬¡ï¼Œæ•¸æ“šé›†æ§‹å»ºå°±æœƒåœæ­¢ã€‚

å¯¦éš›ä¸Šï¼Œé€™æ„å‘³è‘—å¦‚æœæ•¸æ“šé›†è€—ç›¡ï¼Œå®ƒå°‡è¿”å›åˆ°è©²æ•¸æ“šé›†çš„é–‹é ­ï¼Œç›´åˆ°é”åˆ°åœæ­¢æ¨™æº–ã€‚è«‹æ³¨æ„ï¼Œå¦‚æœæœªæŒ‡å®šæ¡æ¨£æ¦‚ç‡ï¼Œå‰‡æ–°æ•¸æ“šé›†å°‡å…·æœ‰ `max_length_datasets*nb_dataset` æ¨£æœ¬ã€‚

```python
d1 = Dataset.from_dict({"a": [0, 1, 2]})
d2 = Dataset.from_dict({"a": [10, 11, 12, 13]})
d3 = Dataset.from_dict({"a": [20, 21, 22]})

dataset = interleave_datasets([d1, d2, d3], stopping_strategy="all_exhausted")

print(dataset["a"])
```

çµæœ:

```bash
[0, 10, 20, 1, 11, 21, 2, 12, 22, 0, 13, 20]
```

## Format

`set_format()` å‡½æ•¸æ›´æ”¹åˆ—(column)çš„æ ¼å¼ä»¥èˆ‡ä¸€äº›å¸¸è¦‹çš„æ•¸æ“šæ ¼å¼å…¼å®¹ã€‚åœ¨é¡å‹åƒæ•¸ä¸­æŒ‡å®šæ‚¨æƒ³è¦çš„è¼¸å‡ºä»¥åŠè¦è¨­ç½®æ ¼å¼çš„åˆ—ã€‚æ ¼å¼åŒ–æ˜¯å³æ™‚ç™¼ç”Ÿçš„ã€‚

ä¾‹å¦‚ï¼Œé€šéè¨­ç½® `type="torch"` å‰µå»º `PyTorch tensor`ï¼š

```python
import torch

dataset.set_format(type="torch", columns=["input_ids", "token_type_ids", "attention_mask", "label"])
```

`with_format()` å‡½æ•¸é‚„å¯ä»¥æ›´æ”¹åˆ—çš„æ ¼å¼ï¼Œä½†å®ƒè¿”å›ä¸€å€‹æ–°çš„ Dataset ç‰©ä»¶ï¼š

```python
dataset = dataset.with_format(type="torch", columns=["input_ids", "token_type_ids", "attention_mask", "label"])
```

!!! tip
    ğŸ¤— Datasets é‚„æä¾›å°å…¶ä»–å¸¸è¦‹æ•¸æ“šæ ¼å¼çš„æ”¯æŒï¼Œä¾‹å¦‚ `NumPy`ã€`Pandas` å’Œ `JAX`ã€‚æŸ¥çœ‹ [Using Datasets with TensorFlow](https://huggingface.co/docs/datasets/master/en/use_with_tensorflow#using-totfdataset) æŒ‡å—ï¼Œäº†è§£æœ‰é—œå¦‚ä½•é«˜æ•ˆå‰µå»º TensorFlow æ•¸æ“šé›†çš„æ›´å¤šè©³ç´°ä¿¡æ¯ã€‚

å¦‚æœéœ€è¦å°‡æ•¸æ“šé›†é‡ç½®ç‚ºå…¶åŸå§‹æ ¼å¼ï¼Œè«‹ä½¿ç”¨ `reset_format()` å‡½æ•¸ï¼š

```python
print(dataset.format)
```

çµæœ:

```bash
{'type': 'torch', 'format_kwargs': {}, 'columns': ['label'], 'output_all_columns': False}
```

```python
dataset.reset_format()
```

```python
dataset.format
```

çµæœ:

```bash
{'type': 'python', 'format_kwargs': {}, 'columns': ['idx', 'label', 'sentence1', 'sentence2'], 'output_all_columns': False}
```

### Format transform

`set_transform()` å‡½æ•¸å³æ™‚æ‡‰ç”¨è‡ªå®šç¾©æ ¼å¼è½‰æ›ã€‚æ­¤å‡½æ•¸æ›¿æ›ä»»ä½•å…ˆå‰æŒ‡å®šçš„æ ¼å¼ã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ­¤å‡½æ•¸å» tokenize å’Œ pad tokensã€‚åƒ…åœ¨è¨ªå•ç¤ºä¾‹æ™‚æ‰æ‡‰ç”¨ Tokenizationï¼š

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

def encode(batch):
    return tokenizer(batch["sentence1"], padding="longest", truncation=True, max_length=512, return_tensors="pt")

dataset.set_transform(encode)

print(dataset.format)
```

çµæœ:

```bash
{'type': 'custom', 'format_kwargs': {'transform': <function __main__.encode(batch)>}, 'columns': ['idx', 'label', 'sentence1', 'sentence2'], 'output_all_columns': False}
```

æ‚¨é‚„å¯ä»¥ä½¿ç”¨ `set_transform()` å‡½æ•¸ä¾†è§£ç¢¼åŠŸèƒ½ä¸æ”¯æŒçš„æ ¼å¼ã€‚ä¾‹å¦‚ï¼ŒéŸ³é »åŠŸèƒ½ä½¿ç”¨è²éŸ³æ–‡ä»¶ï¼ˆä¸€å€‹å®‰è£å¿«é€Ÿä¸”ç°¡å–®çš„åº«ï¼‰ï¼Œä½†å®ƒä¸æä¾›å°ä¸å¤ªå¸¸è¦‹çš„éŸ³é »æ ¼å¼çš„æ”¯æŒã€‚æ‚¨å¯ä»¥åœ¨æ­¤è™•ä½¿ç”¨ `set_transform()`` å‹•æ…‹æ‡‰ç”¨è‡ªå®šç¾©è§£ç¢¼è½‰æ›ã€‚æ‚¨å¯ä»¥éš¨æ„ä½¿ç”¨ä»»ä½•æ‚¨å–œæ­¡çš„åº«ä¾†è§£ç¢¼éŸ³é »æ–‡ä»¶ã€‚

ä¸‹é¢çš„ç¤ºä¾‹ä½¿ç”¨ [pydub](http://pydub.com/) å¥—ä»¶æ‰“é–‹ `soundfile` ä¸æ”¯æŒçš„éŸ³é »æ ¼å¼ï¼š

```python
import numpy as np
from pydub import AudioSegment

audio_dataset_amr = Dataset.from_dict({"audio": ["audio_samples/audio.amr"]})

def decode_audio_with_pydub(batch, sampling_rate=16_000):
    def pydub_decode_file(audio_path):
        sound = AudioSegment.from_file(audio_path)
        if sound.frame_rate != sampling_rate:
            sound = sound.set_frame_rate(sampling_rate)
        channel_sounds = sound.split_to_mono()
        samples = [s.get_array_of_samples() for s in channel_sounds]
        fp_arr = np.array(samples).T.astype(np.float32)
        fp_arr /= np.iinfo(samples[0].typecode).max
        return fp_arr
    batch["audio"] = [pydub_decode_file(audio_path) for audio_path in batch["audio"]]
    return batch

audio_dataset_amr.set_transform(decode_audio_with_pydub)
```

## Save

è™•ç†å®Œæ•¸æ“šé›†å¾Œï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ [`save_to_disk()`](https://huggingface.co/docs/datasets/v2.14.0/en/package_reference/main_classes#datasets.Dataset.save_to_disk) ä¿å­˜ä¸¦ç¨å¾Œé‡è¤‡ä½¿ç”¨å®ƒã€‚

é€šéæä¾›æ‚¨å¸Œæœ›å°‡å…¶ä¿å­˜åˆ°çš„ç›®éŒ„çš„è·¯å¾‘ä¾†ä¿å­˜æ•¸æ“šé›†ï¼š

```python
encoded_dataset.save_to_disk("path/of/my/dataset/directory")
```

ä½¿ç”¨ `load_from_disk()` å‡½æ•¸é‡æ–°åŠ è¼‰æ•¸æ“šé›†ï¼š

```python
from datasets import load_from_disk

reloaded_dataset = load_from_disk("path/of/my/dataset/directory")
```

!!! tip
    æƒ³è¦å°‡æ•¸æ“šé›†ä¿å­˜åˆ°é›²å­˜å„²ç©ºé–“å—ï¼Ÿé–±è®€æˆ‘å€‘çš„ [Cloud Storage](https://huggingface.co/docs/datasets/filesystems) æŒ‡å—ï¼Œäº†è§£å¦‚ä½•å°‡æ•¸æ“šé›†ä¿å­˜åˆ° AWS(S3) æˆ– Google é›²å­˜å„²ã€‚

## Export

ğŸ¤— æ•¸æ“šé›†ä¹Ÿæ”¯æŒå°å‡ºï¼Œå› æ­¤æ‚¨å¯ä»¥åœ¨å…¶ä»–æ‡‰ç”¨ç¨‹åºä¸­ä½¿ç”¨æ‚¨çš„æ•¸æ“šé›†ã€‚ä¸‹è¡¨é¡¯ç¤ºäº†ç•¶å‰æ”¯æŒçš„å°å‡ºæ–‡ä»¶æ ¼å¼ï¼š

|File type	|Export method|
|-----------|-------------|
|`CSV`	|[`Dataset.to_csv()`](https://huggingface.co/docs/datasets/v2.14.0/en/package_reference/main_classes#datasets.Dataset.to_csv)|
|`JSON`	|[`Dataset.to_json()`](https://huggingface.co/docs/datasets/v2.14.0/en/package_reference/main_classes#datasets.Dataset.to_json)|
|`Parquet`	|[`Dataset.to_parquet()`](https://huggingface.co/docs/datasets/v2.14.0/en/package_reference/main_classes#datasets.Dataset.to_parquet)|
|`SQL`	|[`Dataset.to_sql()`](https://huggingface.co/docs/datasets/v2.14.0/en/package_reference/main_classes#datasets.Dataset.to_sql)|
|`In-memory` |Python object	[`Dataset.to_pandas()`](https://huggingface.co/docs/datasets/v2.14.0/en/package_reference/main_classes#datasets.Dataset.to_pandas) or [`Dataset.to_dict()`](https://huggingface.co/docs/datasets/v2.14.0/en/package_reference/main_classes#datasets.Dataset.to_dict)|

ä¾‹å¦‚ï¼Œå°‡æ•¸æ“šé›†å°å‡ºåˆ° CSV æ–‡ä»¶ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
encoded_dataset.to_csv("path/of/my/dataset.csv")
```