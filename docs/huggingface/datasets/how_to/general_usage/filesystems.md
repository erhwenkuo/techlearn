# Cloud storage

ğŸ¤— æ•¸æ“šé›†æ”¯æŒé€šé `fsspec` æ–‡ä»¶ç³»çµ±å¯¦ç¾è¨ªå•é›²å­˜å„²æœå‹™çš„å­˜å–ã€‚æ‚¨å¯ä»¥ä»¥ Python æ–¹å¼å¾ä»»ä½•é›²å­˜å„²æœå‹™ä¾†ä¿å­˜å’ŒåŠ è¼‰æ•¸æ“šé›†ã€‚è«‹æŸ¥çœ‹ä¸‹è¡¨ï¼Œäº†è§£å—æ”¯æŒçš„é›²å­˜å„²æœå‹™çš„ä¸€äº›ç¯„ä¾‹ï¼š

|Storage provider	|Filesystem implementation|
|-----------------|-------------------------|
|`Amazon S3`	|[s3fs](https://s3fs.readthedocs.io/en/latest/)|
|`Google Cloud Storage`	|[gcsfs](https://gcsfs.readthedocs.io/en/latest/)|
|`Azure Blob/DataLake`	|[adlfs](https://github.com/fsspec/adlfs)|
|`Dropbox`	|[dropboxdrivefs](https://github.com/MarineChap/dropboxdrivefs)|
|`Google Drive`	|[gdrivefs](https://github.com/intake/gdrivefs)|
|`Oracle Cloud Storage`	|[ocifs](https://ocifs.readthedocs.io/en/latest/)|

æœ¬æŒ‡å—å°‡å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨ä»»ä½•é›²å­˜å„²æœå‹™ä¿å­˜å’ŒåŠ è¼‰æ•¸æ“šé›†ã€‚ä»¥ä¸‹æ˜¯ `S3`ã€`Google é›²å­˜å„²`ã€`Azure Blob å­˜å„²`å’Œ `Oracle é›² Object å­˜å„²`çš„ç¯„ä¾‹ã€‚

##ã€€Set up your cloud storage FileSystem

###ã€€Amazon S3

1. å®‰è£ S3 FileSystem implementationï¼š

    ```python
     pip install s3fs
    ```

2. å®šç¾©æ‚¨çš„ credentials

    è¦ä½¿ç”¨åŒ¿åé€£æ¥ï¼Œè«‹ä½¿ç”¨ `anon=True`ã€‚å¦å‰‡ï¼Œæ¯ç•¶æ‚¨èˆ‡ç§æœ‰ S3 å­˜å„²æ¡¶äº¤äº’æ™‚ï¼Œè«‹åŒ…å«æ‚¨çš„ `aws_access_key_id` å’Œ `aws_secret_access_key`ã€‚

    === "AWS S3"

        ```python
        storage_options = {"anon": True}  # for anonymous connection
        storage_options = {"key": aws_access_key_id, "secret": aws_secret_access_key}  # for private buckets

        import aiobotocore.session

        s3_session = aiobotocore.session.AioSession(profile="my_profile_name")

        storage_options = {"session": s3_session}
        ```

    === "Minio"

        ```python
        storage_options = {
            "anon": False, # for anonymous connection
            "key": "*********", # access_key
            "secret": "*********", # secret_key
            "use_ssl": False, # use https or http
            "client_kwargs": {
                "endpoint_url": "http://localhost:9000" # minio endpoint
            }
        }
        ```

3. å‰µå»ºä½ çš„ `FileSystem instance`

    ```python
    import s3fs
    fs = s3fs.S3FileSystem(**storage_options)
    ```

#### Class: `datasets.filesystems.S3FileSystem`

Parameters:

- `anon` (bool, default to False) â€” Whether to use anonymous connection (public buckets only). If False, uses the key/secret given, or botoâ€™s credential - resolver (client_kwargs, environment, variables, config files, EC2 IAM server, in that order).
- `key` (str) â€” If not anonymous, use this access key ID, if specified.
- `secret` (str) â€” If not anonymous, use this secret access key, if specified.
- `token` (str) â€” If not anonymous, use this security token, if specified.
- `use_ssl` (bool, defaults to True) â€” Whether to use SSL in connections to S3; may be faster without, but insecure. If use_ssl is also set in client_kwargs, the value set in client_kwargs will take priority.
- `s3_additional_kwargs` (dict) â€” Parameters that are used when calling S3 API methods. Typically used for things like ServerSideEncryption.
- `client_kwargs` (dict) â€” Parameters for the botocore client.
- `requester_pays` (bool, defaults to False) â€” Whether RequesterPays buckets are supported.
- `default_block_size` (int) â€” If given, the default block size value used for open(), if no specific value is given at all time. The built-in default is 5MB.
- `default_fill_cache` (bool, defaults to True) â€” Whether to use cache filling with open by default. Refer to S3File.open.
- `default_cache_type` (str, defaults to bytes) â€” If given, the default cache_type value used for open(). Set to none if no caching is desired. See fsspecâ€™s documentation for other available cache_type values.
- `version_aware` (bool, defaults to False) â€” Whether to support bucket versioning. If enable this will require the user to have the necessary IAM permissions for dealing with versioned objects.
- `cache_regions` (bool, defaults to False) â€” Whether to cache bucket regions. Whenever a new bucket is used, it will first find out which region it belongs to and then use the client for that region.
- `asynchronous` (bool, defaults to False) â€” Whether this instance is to be used from inside coroutines.
- `config_kwargs` (dict) â€” Parameters passed to botocore.client.Config. **kwargs â€” Other parameters for core session.
- `session` (aiobotocore.session.AioSession) â€” Session to be used for all connections. This session will be used inplace of creating a new session inside S3FileSystem. For example: aiobotocore.session.AioSession(profile='test_user').
- `skip_instance_cache` (bool) â€” Control reuse of instances. Passed on to fsspec.
- `use_listings_cache` (bool) â€” Control reuse of directory listings. Passed on to fsspec.
- `listings_expiry_time` (int or float) â€” Control reuse of directory listings. Passed on to fsspec.
- `max_paths` (int) â€” Control reuse of directory listings. Passed on to fsspec.

`datasets.filesystems.S3FileSystem` æ˜¯ `s3fs.S3FileSystem` çš„å­é¡ã€‚

ç”¨æˆ¶å¯ä»¥ä½¿ç”¨æ­¤ class ä¾†è¨ªå• S3ï¼Œå°±åƒå®ƒæ˜¯ä¸€å€‹æ–‡ä»¶ç³»çµ±ä¸€æ¨£ã€‚å®ƒåœ¨ S3 å­˜å„²ä¹‹ä¸Šå…¬é–‹äº†é¡ä¼¼æ–‡ä»¶ç³»çµ±çš„ APIï¼ˆ`ls`ã€`cp`ã€`open` ç­‰ï¼‰ã€‚é¡¯å¼æä¾›æ†‘æ“šã€€`(key=ã€secret=)` æˆ–ä½¿ç”¨ `boto`` çš„æ†‘æ“šæ–¹æ³•ã€‚è«‹åƒé–± `botocore` æ–‡æª”ä»¥ç²å–æ›´å¤šä¿¡æ¯ã€‚å¦‚æœæ²’æœ‰å¯ç”¨çš„æ†‘æ“šï¼Œè«‹ä½¿ç”¨ `anon=True`ã€‚

ç¯„ä¾‹:

**Listing files from public S3 bucket**

```python
import datasets

s3 = datasets.filesystems.S3FileSystem(anon=True)

s3.ls('public-datasets/imdb/train')
```

çµæœ:

```bash
['dataset_info.json.json','dataset.arrow','state.json']
```

**Listing files from private S3 bucket using aws_access_key_id and aws_secret_access_key.**

```python
import datasets

s3 = datasets.filesystems.S3FileSystem(key=aws_access_key_id, secret=aws_secret_access_key)

s3.ls('my-private-datasets/imdb/train')
```

çµæœ:

```bash
['dataset_info.json.json','dataset.arrow','state.json']
```

**Using S3Filesystem with `botocore.session.Session` and custom aws_profile.**

```python
import botocore
from datasets.filesystems import S3Filesystem

s3_session = botocore.session.Session(profile_name='my_profile_name')

s3 = S3FileSystem(session=s3_session)
```

**Loading dataset from S3 using S3Filesystem and `load_from_disk()`.**

=== "AWS S3"

    ```python
    from datasets import load_from_disk
    from datasets.filesystems import S3Filesystem

    s3 = S3FileSystem(key=aws_access_key_id, secret=aws_secret_access_key)

    dataset = load_from_disk('s3://my-private-datasets/imdb/train', storage_options=s3.storage_options)

    print(len(dataset))
    ```

    çµæœ:

    ```bash
    25000
    ```

=== "Minio"

    ```python
    from datasets import load_from_disk

    storage_options = {
            "anon": False, # for anonymous connection
            "key": "minioadmin", # access_key
            "secret": "minioadmin", # secret_key
            "use_ssl": False, # use https or http
            "client_kwargs": {
                "endpoint_url": "http://localhost:9000" # minio endpoint
            }
        }

    dataset = load_from_disk('s3://my-private-datasets/imdb/train', storage_options=storage_options)

    print(len(dataset))
    ```

    çµæœ:

    ```bash
    25000
    ```

**Saving dataset to S3 using S3Filesystem and `Dataset.save_to_disk()`.**

=== "AWS S3"

```python
from datasets import load_dataset
from datasets.filesystems import S3Filesystem

dataset = load_dataset("imdb")

s3 = S3FileSystem(key=aws_access_key_id, secret=aws_secret_access_key)

dataset.save_to_disk('s3://my-private-datasets/imdb/train', storage_options=s3.storage_options)
```

=== "Minio"

    ```python
    from datasets import load_dataset

    dataset = load_dataset("imdb")

    storage_options = {
                "anon": False, # for anonymous connection
                "key": "minioadmin", # access_key
                "secret": "minioadmin", # secret_key
                "use_ssl": False, # use https or http
                "client_kwargs": {
                    "endpoint_url": "http://localhost:9000" # minio endpoint
                }
            }

    dataset.save_to_disk('s3://my-private-datasets/imdb/train', storage_options=storage_options)
    ```

### Google Cloud Storage

1. å®‰è£ Google Cloud Storage implementationï¼š

    ```python
    conda install -c conda-forge gcsfs

    # or install using pip
    pip install gcsfs
    ```

2. å®šç¾©æ‚¨çš„ credentials

    ```python
    storage_options={"token": "anon"}  # for anonymous connection

    # or use your credentials of your default gcloud credentials or from the google metadata service
    storage_options={"project": "my-google-project"}

    # or use your credentials from elsewhere, see the documentation at https://gcsfs.readthedocs.io/
    storage_options={"project": "my-google-project", "token": TOKEN}
    ```

3. å‰µå»ºä½ çš„ `FileSystem instance`

    ```python
    import gcsfs

    fs = gcsfs.GCSFileSystem(**storage_options)
    ```

### Azure Blob Storage

1. å®‰è£ Azure Blob Storage implementationï¼š

    ```python
    conda install -c conda-forge adlfs

    # or install using pip
    pip install adlfs
    ```

2. å®šç¾©æ‚¨çš„ credentials

    ```python
    storage_options = {"anon": True}  # for anonymous connection

    # or use your credentials
    storage_options = {"account_name": ACCOUNT_NAME, "account_key": ACCOUNT_KEY}  # gen 2 filesystem

    # or use your credentials with the gen 1 filesystem
    storage_options={"tenant_id": TENANT_ID, "client_id": CLIENT_ID, "client_secret": CLIENT_SECRET}
    ```

3. å‰µå»ºä½ çš„ `FileSystem instance`

    ```python
    import adlfs

    fs = adlfs.AzureBlobFileSystem(**storage_options)
    ```

### Oracle Cloud Object Storage

1. å®‰è£ OCI FileSystem implementationï¼š

    ```python
    conda install -c conda-forge ocifs

    # or install using pip
    pip install ocifs
    ```

2. å®šç¾©æ‚¨çš„ credentials

    ```python
    storage_options = {"config": "~/.oci/config", "region": "us-ashburn-1"} 
    ```

3. å‰µå»ºä½ çš„ `FileSystem instance`

    ```python
    import ocifs

    fs = ocifs.OCIFileSystem(**storage_options)
    ```

## Load and Save your datasets using your cloud storage FileSystem

### Download and prepare a dataset into a cloud storage

æ‚¨å¯ä»¥é€šéåœ¨ `download_and_prepare` ä¸­æŒ‡å®šé ç¨‹ `output_dir` å°‡æ•¸æ“šé›†ä¸‹è¼‰ä¸¦æº–å‚™åˆ°é›²å­˜å„²æœå‹™ä¸­ã€‚ä¸è¦å¿˜è¨˜ä½¿ç”¨ä¹‹å‰å®šç¾©çš„åŒ…å«æ‚¨çš„æ†‘æ“šçš„ `storage_options`ã€‚

`download_and_prepare` æ–¹æ³•åˆ†å…©å€‹æ­¥é©Ÿé€²è¡Œï¼š

1. å®ƒé¦–å…ˆä¸‹è¼‰æœ¬åœ°ç·©å­˜ä¸­çš„åŸå§‹æ•¸æ“šæ–‡ä»¶ï¼ˆå¦‚æœæœ‰ï¼‰ã€‚æ‚¨å¯ä»¥é€šéå°‡ `cache_dir` å‚³éçµ¦ `load_dataset_builder()` ä¾†è¨­ç½®ç·©å­˜ç›®éŒ„
2. ç„¶å¾Œå®ƒé€šéè¿­ä»£åŸå§‹æ•¸æ“šæ–‡ä»¶åœ¨é›²å­˜å„²ä¸­ç”Ÿæˆ `Arrow` æˆ– `Parquet` æ ¼å¼çš„æ•¸æ“šé›†ã€‚

ç¯„ä¾‹:

å¾ Hugging Face Hub åŠ è¼‰æ•¸æ“šé›†æ§‹å»ºå™¨(äº†è§£[how to load from the Hugging Face Hub](https://huggingface.co/docs/datasets/loading#hugging-face-hub)):

```python
from datasets import load_dataset_builder

output_dir = "s3://my-bucket/imdb"

builder = load_dataset_builder("imdb")

builder.download_and_prepare(output_dir, storage_options=storage_options, file_format="parquet")
```

å¦å¤–ä¸€ç¨®æ‰‹æ³•æ˜¯ä½¿ç”¨æœ¬åœ°çš„ `loading script` ä¾†åŠ è¼‰ dataset builderï¼ˆè«‹åƒé–± [how to load a local loading script](https://huggingface.co/docs/datasets/loading#local-loading-script)):

```python
output_dir = "s3://my-bucket/imdb"

builder = load_dataset_builder("path/to/local/loading_script/loading_script.py")

builder.download_and_prepare(output_dir, storage_options=storage_options, file_format="parquet")
```

ä½¿ç”¨æ‚¨è‡ªå·±çš„æ•¸æ“šæ–‡ä»¶ (è«‹åƒé–± [how to load local and remote files](https://huggingface.co/docs/datasets/loading#local-and-remote-files)):

```python
data_files = {"train": ["path/to/train.csv"]}

output_dir = "s3://my-bucket/imdb"

builder = load_dataset_builder("csv", data_files=data_files)

builder.download_and_prepare(output_dir, storage_options=storage_options, file_format="parquet")
```

å¼·çƒˆå»ºè­°å°‡æ–‡ä»¶ä¿å­˜ç‚ºå£“ç¸®çš„ `Parquet` æ–‡ä»¶ï¼Œé€šéæŒ‡å®š `file_format="parquet"` ä¾†å„ªåŒ– I/Oã€‚å¦å‰‡ï¼Œæ•¸æ“šé›†å°‡ä¿å­˜ç‚ºæœªå£“ç¸®çš„ `Arrow` æ–‡ä»¶ã€‚

æ‚¨é‚„å¯ä»¥ä½¿ç”¨ `max_shard_size` æŒ‡å®šåˆ†ç‰‡çš„å¤§å°ï¼ˆé»˜èªç‚º `500MB`ï¼‰ï¼š

```python
builder.download_and_prepare(output_dir, storage_options=storage_options, file_format="parquet", max_shard_size="1GB")
```

### Dask

Dask æ˜¯ä¸€å€‹ä¸¦è¡Œè¨ˆç®—åº«ï¼Œå®ƒæœ‰ä¸€å€‹é¡ä¼¼ pandas çš„ APIï¼Œç”¨æ–¼ä¸¦è¡Œè™•ç†å¤§æ–¼å…§å­˜çš„ Parquet æ•¸æ“šé›†ã€‚ Dask å¯ä»¥åœ¨å–®å°æ©Ÿå™¨æˆ–æ©Ÿå™¨é›†ç¾¤ä¸Šä½¿ç”¨å¤šå€‹ç·šç¨‹æˆ–é€²ç¨‹ä¾†ä¸¦è¡Œè™•ç†æ•¸æ“šã€‚ Dask æ”¯æŒæœ¬åœ°æ•¸æ“šï¼Œä¹Ÿæ”¯æŒä¾†è‡ªé›²å­˜å„²çš„æ•¸æ“šã€‚

å› æ­¤ï¼Œæ‚¨å¯ä»¥åœ¨ Dask ä¸­åŠ è¼‰ä¿å­˜ç‚ºåˆ†ç‰‡ Parquet æ–‡ä»¶çš„æ•¸æ“šé›†ï¼š

```python
import dask.dataframe as dd

df = dd.read_parquet(output_dir, storage_options=storage_options)

# or if your dataset is split into train/valid/test
df_train = dd.read_parquet(output_dir + f"/{builder.name}-train-*.parquet", storage_options=storage_options)
df_valid = dd.read_parquet(output_dir + f"/{builder.name}-validation-*.parquet", storage_options=storage_options)
df_test = dd.read_parquet(output_dir + f"/{builder.name}-test-*.parquet", storage_options=storage_options)
```

æ‚¨å¯ä»¥åœ¨å…¶[æ–‡æª”](https://docs.dask.org/en/stable/dataframe.html)ä¸­æ‰¾åˆ°æœ‰é—œ dask dataframesçš„æ›´å¤šä¿¡æ¯ã€‚

## Saving serialized datasets

è™•ç†å®Œæ•¸æ“šé›†å¾Œï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ [`Dataset.save_to_disk()`](https://huggingface.co/docs/datasets/v2.14.1/en/package_reference/main_classes#datasets.Dataset.save_to_disk) å°‡å…¶ä¿å­˜åˆ°é›²å­˜å„²ä¸­ï¼š

```python
encoded_dataset.save_to_disk("s3://my-private-datasets/imdb/train", storage_options=storage_options)

encoded_dataset.save_to_disk("gcs://my-private-datasets/imdb/train", storage_options=storage_options)

encoded_dataset.save_to_disk("adl://my-private-datasets/imdb/train", storage_options=storage_options)
```

!!! info
    æ¯ç•¶æ‚¨èˆ‡ç§æœ‰é›²å­˜å„²äº¤äº’æ™‚ï¼Œè«‹è¨˜ä½åœ¨[æ–‡ä»¶ç³»çµ±](https://huggingface.co/docs/datasets/filesystems#set-up-your-cloud-storage-filesystem)å¯¦ä¾‹ fs ä¸­å®šç¾©æ‚¨çš„ credentialsã€‚

ç¯„ä¾‹:

```python
# load dataset from Hugging Hub
dataset = load_dataset("imdb")

# setup storage options for remote file system
storage_options = {
    "anon": False, # for anonymous connection
    "key": "minioadmin", # access_key
    "secret": "minioadmin", # secret_key
    "use_ssl": False, # use https or http
    "client_kwargs": {
        "endpoint_url": "http://localhost:9000" # minio endpoint
    }
}

# serialize dataset to remote file system
dataset.save_to_disk("s3://my-private-datasets/imdb", storage_options=storage_options)
```

###ã€€Listing serialized datasets

ä½¿ç”¨ `fs.ls` åˆ—å‡ºå¸¶æœ‰æ–‡ä»¶ç³»çµ±å¯¦ä¾‹ fs çš„é›²å­˜å„²ä¸­çš„æ–‡ä»¶ï¼š

```python
fs.ls("my-private-datasets/imdb", detail=False)
```

çµæœ:

```bash
["dataset_info.json.json","dataset.arrow","state.json"]
```

## Load serialized datasets

ç•¶æ‚¨æº–å‚™å¥½å†æ¬¡ä½¿ç”¨æ•¸æ“šé›†æ™‚ï¼Œè«‹ä½¿ç”¨ [`Dataset.load_from_disk()`](https://huggingface.co/docs/datasets/v2.14.1/en/package_reference/main_classes#datasets.Dataset.load_from_disk) é‡æ–°åŠ è¼‰å®ƒï¼š

```python
from datasets import load_from_disk

dataset = load_from_disk("s3://my-private-datasets/imdb", storage_options=storage_options)

print(len(dataset))
```

ç¯„ä¾‹:

```python
import datasets

# setup storage options for remote file system
storage_options = {
        "anon": False, # for anonymous connection
        "key": "minioadmin", # access_key
        "secret": "minioadmin", # secret_key
        "use_ssl": False, # use https or http
        "client_kwargs": {
            "endpoint_url": "http://localhost:9000" # minio endpoint
        }
    }

# load dataset from remote file system
dataset = datasets.load_from_disk("s3://my-private-datasets/imdb", storage_options=storage_options)

print(dataset)
```

çµæœ:

```bash
DatasetDict({
    train: Dataset({
        features: ['text', 'label'],
        num_rows: 25000
    })
    test: Dataset({
        features: ['text', 'label'],
        num_rows: 25000
    })
    unsupervised: Dataset({
        features: ['text', 'label'],
        num_rows: 50000
    })
})
```