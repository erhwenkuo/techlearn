# Create an image dataset

æœ‰å…©ç¨®å‰µå»ºå’Œå…±äº«åœ–åƒæ•¸æ“šé›†çš„æ–¹æ³•ã€‚æœ¬æŒ‡å—å°‡å‘æ‚¨å±•ç¤ºå¦‚ä½•ï¼š

- ä½¿ç”¨ `ImageFolder` å’Œä¸€äº› `metadata` ä¾†å‰µå»º image datasetã€‚é€™æ˜¯ä¸€å€‹ no-code è§£æ±ºæ–¹æ¡ˆï¼Œç”¨æ–¼å¿«é€Ÿå‰µå»ºåŒ…å«æ•¸åƒå¼µåœ–åƒçš„ image datasetã€‚
- é€šéç·¨å¯« `loading script` å‰µå»ºåœ–åƒæ•¸æ“šé›†ã€‚æ­¤æ–¹æ³•ç¨å¾®è¤‡é›œä¸€äº›ï¼Œä½†æ‚¨å¯ä»¥æ›´éˆæ´»åœ°å®šç¾©ã€ä¸‹è¼‰å’Œç”Ÿæˆæ•¸æ“šé›†ï¼Œé€™å°æ–¼æ›´è¤‡é›œæˆ–å¤§è¦æ¨¡çš„åœ–åƒæ•¸æ“šé›†éå¸¸æœ‰ç”¨ã€‚

## ImageFolder

ImageFolder æ˜¯ä¸€å€‹æ•¸æ“šé›†ç”Ÿæˆå™¨ï¼Œæ—¨åœ¨å¿«é€ŸåŠ è¼‰åŒ…å«æ•¸åƒå¼µåœ–åƒçš„åœ–åƒæ•¸æ“šé›†ï¼Œè€Œç„¡éœ€æ‚¨ç·¨å¯«ä»»ä½•ä»£ç¢¼ã€‚

!!! tip
    ğŸ’¡ æŸ¥çœ‹ [Split pattern hierarchy](https://huggingface.co/docs/datasets/v2.14.1/en/repository_structure#split-pattern-hierarchy) çµæ§‹ï¼Œäº†è§£æœ‰é—œ ImageFolder å¦‚ä½•æ ¹æ“šæ•¸æ“šé›†å­˜å„²åº«çµæ§‹å‰µå»ºæ•¸æ“šé›†åˆ†å‰²çš„æ›´å¤šä¿¡æ¯ã€‚

ImageFolder æ ¹æ“šç›®éŒ„åç¨±è‡ªå‹•æ¨æ–·æ•¸æ“šé›†çš„é¡åˆ¥æ¨™ç±¤ã€‚å°‡æ•¸æ“šé›†å­˜å„²åœ¨å¦‚ä¸‹ç›®éŒ„çµæ§‹ä¸­ï¼š

```bash
folder/train/dog/golden_retriever.png
folder/train/dog/german_shepherd.png
folder/train/dog/chihuahua.png

folder/train/cat/maine_coon.png
folder/train/cat/bengal.png
folder/train/cat/birman.png
```

ç„¶å¾Œç”¨æˆ¶å¯ä»¥é€šéåœ¨ `load_dataset()` ä¸­æŒ‡å®š `imagefolder` å’Œ `data_dir` ä¸­æŒ‡å®šç›®éŒ„ä¾†åŠ è¼‰æ•¸æ“šé›†ï¼š

```python
from datasets import load_dataset

dataset = load_dataset("imagefolder", data_dir="/path/to/folder")
```

æ‚¨é‚„å¯ä»¥ä½¿ç”¨ imagefolder åŠ è¼‰æ¶‰åŠå¤šå€‹ split çš„æ•¸æ“šé›†ã€‚ç‚ºæ­¤ï¼Œæ‚¨çš„æ•¸æ“šé›†ç›®éŒ„æ‡‰å…·æœ‰ä»¥ä¸‹çµæ§‹ï¼š

```bash
folder/train/dog/golden_retriever.png
folder/train/cat/maine_coon.png
folder/test/dog/german_shepherd.png
folder/test/cat/bengal.png
```

!!! tip
    å¦‚æœæ‰€æœ‰åœ–åƒæ–‡ä»¶éƒ½åŒ…å«åœ¨ä¸€å€‹ç›®éŒ„ä¸­æˆ–è€…å®ƒå€‘ä¸åœ¨åŒä¸€ç´šåˆ¥çš„ç›®éŒ„çµæ§‹ä¸­ï¼Œå‰‡ä¸æœƒè‡ªå‹•æ·»åŠ æ¨™ç±¤åˆ—ã€‚å¦‚æœéœ€è¦ï¼Œè«‹æ˜ç¢ºè¨­ç½® `drop_labels=False` ã€‚

å¦‚æœæ‚¨æƒ³è¦åŒ…å«æœ‰é—œæ•¸æ“šé›†çš„å…¶ä»–ä¿¡æ¯ï¼ˆä¾‹å¦‚æ–‡æœ¬æ¨™é¡Œæˆ–é‚Šç•Œæ¡†ï¼‰ï¼Œè«‹å°‡å…¶ä½œç‚º `metadata.csv` æ–‡ä»¶æ·»åŠ åˆ°æ‚¨çš„æ–‡ä»¶å¤¾ä¸­ã€‚é€™ä½¿æ‚¨å¯ä»¥å¿«é€Ÿç‚ºä¸åŒçš„è¨ˆç®—æ©Ÿè¦–è¦ºä»»å‹™ï¼ˆä¾‹å¦‚æ–‡æœ¬å­—å¹•æˆ–å°è±¡æª¢æ¸¬ï¼‰å‰µå»ºæ•¸æ“šé›†ã€‚æ‚¨é‚„å¯ä»¥ä½¿ç”¨ JSONL æ–‡ä»¶ `metadata.jsonl`ã€‚

```bash
folder/train/metadata.csv
folder/train/0001.png
folder/train/0002.png
folder/train/0003.png
```

æ‚¨é‚„å¯ä»¥å£“ç¸®åœ–åƒï¼š

```bash
folder/metadata.csv
folder/train.zip
folder/test.zip
folder/valid.zip
```

æ‚¨çš„ `metadata.csv` æ–‡ä»¶å¿…é ˆæœ‰ä¸€å€‹ `file_name` åˆ—(column)ï¼Œå®ƒå°‡ image file èˆ‡å…¶ metadata éˆæ¥èµ·ä¾†ï¼š

```csv
file_name,additional_feature
0001.png,This is a first value of a text feature you added to your images
0002.png,This is a second value of a text feature you added to your images
0003.png,This is a third value of a text feature you added to your images
```

æˆ–ä½¿ç”¨ `metadata.jsonl`ï¼š

```bash
{"file_name": "0001.png", "additional_feature": "This is a first value of a text feature you added to your images"}
{"file_name": "0002.png", "additional_feature": "This is a second value of a text feature you added to your images"}
{"file_name": "0003.png", "additional_feature": "This is a third value of a text feature you added to your images"}
```

!!! info
    å¦‚æœå­˜åœ¨ metadata æ–‡ä»¶ï¼Œå‰‡é»˜èªæƒ…æ³ä¸‹æœƒåˆªé™¤åŸºæ–¼ç›®éŒ„åç¨±æ¨æ–·çš„æ¨™ç±¤ã€‚è¦åŒ…å«é€™äº›æ¨™ç±¤ï¼Œè«‹åœ¨ `load_dataset()` ä¸­è¨­ç½® `drop_labels=False`ã€‚

### Image captioning

Image captioning datasets åŒ…å«æè¿°åœ–åƒçš„æ–‡æœ¬ã€‚ç¯„ä¾‹ `metadata.csv` å¯èƒ½å¦‚ä¸‹æ‰€ç¤ºï¼š

```csv
file_name,text
0001.png,This is a golden retriever playing with a ball
0002.png,A german shepherd
0003.png,One chihuahua
```

ä½¿ç”¨ `ImageFolder` åŠ è¼‰æ•¸æ“šé›†ï¼Œå®ƒå°‡ç‚ºåœ–åƒæ¨™é¡Œå‰µå»ºä¸€å€‹æ–‡æœ¬åˆ—ï¼š

```python
dataset = load_dataset("imagefolder", data_dir="/path/to/folder", split="train")

print(dataset[0]["text"])
```

çµæœ:

```bash
"This is a golden retriever playing with a ball"
```

### Object detection

Object detection datasets å…·æœ‰è­˜åˆ¥åœ–åƒä¸­ç‰©ä»¶çš„é‚Šç•Œæ¡†å’Œé¡åˆ¥ã€‚ç¤ºä¾‹ `metadata.jsonl` å¯èƒ½å¦‚ä¸‹æ‰€ç¤ºï¼š

```jsonl
{"file_name": "0001.png", "objects": {"bbox": [[302.0, 109.0, 73.0, 52.0]], "categories": [0]}}
{"file_name": "0002.png", "objects": {"bbox": [[810.0, 100.0, 57.0, 28.0]], "categories": [1]}}
{"file_name": "0003.png", "objects": {"bbox": [[160.0, 31.0, 248.0, 616.0], [741.0, 68.0, 202.0, 401.0]], "categories": [2, 2]}}
```

ä½¿ç”¨ ImageFolder åŠ è¼‰æ•¸æ“šé›†ï¼Œå®ƒå°‡å‰µå»ºä¸€å€‹åŒ…å«é‚Šç•Œæ¡†å’Œé¡åˆ¥çš„ç‰©ä»¶åˆ—ï¼š

```python
dataset = load_dataset("imagefolder", data_dir="/path/to/folder", split="train")

print(dataset[0]["objects"])
```

çµæœ:

```bash
{"bbox": [[302.0, 109.0, 73.0, 52.0]], "categories": [0]}
```

### Upload dataset to the Hub

å‰µå»ºæ•¸æ“šé›†å¾Œï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ `push_to_hub()` æ–¹æ³•å°‡å…¶å…±äº«åˆ° Hubã€‚ç¢ºä¿æ‚¨å·²å®‰è£ `huggingface_hub` å¥—ä»¶ä¸¦ä¸”å·²ç™»å…¥æ‚¨çš„ Hugging Face å¸³æˆ¶ï¼ˆæœ‰é—œæ›´å¤šè©³ç´°ä¿¡æ¯ï¼Œè«‹åƒé–± [Upload with Python tutorial](https://huggingface.co/docs/datasets/v2.14.1/en/upload_dataset#upload-with-python)ï¼‰ã€‚

ä½¿ç”¨ `push_to_hub()` ä¸Šå‚³æ•¸æ“šé›†ï¼š

```python
from datasets import load_dataset

dataset = load_dataset("imagefolder", data_dir="/path/to/folder", split="train")

dataset.push_to_hub("stevhliu/my-image-captioning-dataset")
```

## Loading script

ç·¨å¯«æ•¸æ“šé›†åŠ è¼‰è…³æœ¬ä¾†å…±äº«æ•¸æ“šé›†ã€‚å®ƒå®šç¾©æ•¸æ“šé›†çš„åˆ†å‰²å’Œé…ç½®ï¼Œä¸¦è™•ç†æ•¸æ“šé›†çš„ä¸‹è¼‰å’Œç”Ÿæˆã€‚è©²è…³æœ¬ä½æ–¼èˆ‡æ•¸æ“šé›†ç›¸åŒçš„æ–‡ä»¶å¤¾æˆ–å­˜å„²åº«ä¸­ï¼Œä¸¦ä¸”æ‡‰å…·æœ‰ç›¸åŒçš„åç¨±ã€‚

```bash
my_dataset/
â”œâ”€â”€ README.md
â”œâ”€â”€ my_dataset.py
â””â”€â”€ data/  # optional, may contain your images or TAR archives
```

é€™ç¨®çµæ§‹å…è¨±æ‚¨çš„æ•¸æ“šé›†ä½¿ç”¨ä¸€è¡Œç¨‹å¼ç¢¼ä¸­åŠ è¼‰ï¼š

```python
from datasets import load_dataset

dataset = load_dataset("path/to/my_dataset")
```

æœ¬æŒ‡å—å°‡å‘æ‚¨å±•ç¤ºå¦‚ä½•ç‚ºåœ–åƒæ•¸æ“šé›†å‰µå»ºæ•¸æ“šé›†åŠ è¼‰è…³æœ¬ï¼Œé€™èˆ‡ [creating a loading script for text datasets](https://huggingface.co/docs/datasets/v2.14.1/en/dataset_script)ã€€æœ‰é»ä¸åŒã€‚æ‚¨å°‡å­¸ç¿’å¦‚ä½•ï¼š

- æ§‹å»º dataset builder class
- æ§‹å»º dataset configurations
- å¢åŠ  dataset metadata
- ä¸‹è¼‰èˆ‡å®šç¾© dataset splits
- å‰µå»º dataset
- å‰µå»º dataset metadata (optional)
- ä¸Šå‚³ dataset åˆ° Hub

æœ€å¥½çš„å­¸ç¿’æ–¹æ³•æ˜¯æ‰“é–‹ç¾æœ‰çš„åœ–åƒæ•¸æ“šé›†åŠ è¼‰è…³æœ¬ï¼Œä¾‹å¦‚ [Food-101](https://huggingface.co/datasets/food101/blob/main/food101.py)ï¼Œç„¶å¾Œè·Ÿè‘—æ“ä½œï¼

!!! tip
    ç‚ºäº†å¹«åŠ©æ‚¨å…¥é–€ï¼ŒHuggingface å‰µå»ºäº†ä¸€å€‹ [loading script template](https://github.com/huggingface/datasets/blob/main/templates/new_dataset_script.py)ï¼Œæ‚¨å¯ä»¥è¤‡è£½ä¸¦ç”¨ä¾†ä½œç‚ºé–‹ç™¼çš„èµ·é»ï¼

###ã€€Create a dataset builder class

[`GeneratorBasedBuilder`](https://huggingface.co/docs/datasets/v2.14.1/en/package_reference/builder_classes#datasets.GeneratorBasedBuilder) æ˜¯å­—å…¸ç”Ÿæˆå™¨ç”Ÿæˆçš„æ•¸æ“šé›†çš„åŸºé¡ã€‚åœ¨æ­¤é¡ä¸­ï¼Œæä¾›äº†ä¸‰ç¨®æ–¹æ³•ä¾†å¹«åŠ©å‰µå»ºæ•¸æ“šé›†ï¼š

- `info` å­˜å„²æœ‰é—œæ•¸æ“šé›†çš„ä¿¡æ¯ï¼Œä¾‹å¦‚å…¶æè¿°ã€è¨±å¯è­‰å’Œ featuresã€‚
- `split_generators` ä¸‹è¼‰æ•¸æ“šé›†ä¸¦å®šç¾©å…¶åˆ†å‰²ã€‚
- `generate_examples` ç‚ºæ¯å€‹åˆ†å‰²ç”Ÿæˆåœ–åƒå’Œæ¨™ç±¤ã€‚

é¦–å…ˆå‰µå»ºæ•¸æ“šé›†é¡ä½œç‚º `GeneratorBasedBuilder` çš„å­é¡ä¸¦æ·»åŠ ä¸‰å€‹æ–¹æ³•ã€‚ä¸ç”¨æ“”å¿ƒå¡«å¯«é€™äº›æ–¹æ³•ï¼Œæ‚¨å°‡åœ¨æ¥ä¸‹ä¾†çš„ç« ç¯€ä¸­é–‹ç™¼é€™äº›æ–¹æ³•ï¼š

```python
class Food101(datasets.GeneratorBasedBuilder):
    """Food-101 Images dataset"""

    def _info(self):

    def _split_generators(self, dl_manager):

    def _generate_examples(self, images, metadata_path):
```

#### Multiple configurations

åœ¨æŸäº›æƒ…æ³ä¸‹ï¼Œæ•¸æ“šé›†å¯èƒ½å…·æœ‰å¤šå€‹é…ç½®ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æŸ¥çœ‹ [Imagenette æ•¸æ“šé›†](https://huggingface.co/datasets/frgfm/imagenette)ï¼Œæ‚¨æœƒæ³¨æ„åˆ°å­˜åœ¨ä¸‰å€‹å­é›†ã€‚

è¦å‰µå»ºä¸åŒçš„é…ç½®ï¼Œè«‹ä½¿ç”¨ [BuilderConfig](https://huggingface.co/docs/datasets/v2.14.1/en/package_reference/builder_classes#datasets.BuilderConfig) é¡ç‚ºæ•¸æ“šé›†å‰µå»ºå­é¡ã€‚åœ¨ `data_url` å’Œ `metadata_urls` ä¸­æä¾›ä¸‹è¼‰åœ–åƒå’Œæ¨™ç±¤çš„éˆæ¥ï¼š

```python
class Food101Config(datasets.BuilderConfig):
    """Builder Config for Food-101"""
 
    def __init__(self, data_url, metadata_urls, **kwargs):
        """BuilderConfig for Food-101.
        Args:
          data_url: `string`, url to download the zip file from.
          metadata_urls: dictionary with keys 'train' and 'validation' containing the archive metadata URLs
          **kwargs: keyword arguments forwarded to super.
        """
        super(Food101Config, self).__init__(version=datasets.Version("1.0.0"), **kwargs)
        self.data_url = data_url
        self.metadata_urls = metadata_urls
```

ç¾åœ¨æ‚¨å¯ä»¥åœ¨ `GeneratorBasedBuilder` çš„é ‚éƒ¨å®šç¾©æ‚¨çš„å­é›†ã€‚æƒ³åƒä¸€ä¸‹ï¼Œæ‚¨æƒ³è¦æ ¹æ“š `Food-101` æ•¸æ“šé›†æ˜¯æ—©é¤é‚„æ˜¯æ™šé¤é£Ÿç‰©ä¾†å‰µå»ºå…©å€‹å­é›†ã€‚

1. ä½¿ç”¨ `BUILDER_CONFIGS` åˆ—è¡¨ä¸­çš„ `Food101Config` å®šç¾©æ‚¨çš„å­é›†ã€‚
2. å°æ–¼æ¯å€‹é…ç½®ï¼Œè«‹æä¾›åç¨±ã€æè¿°ä»¥åŠåœ–åƒå’Œæ¨™ç±¤çš„ä¸‹è¼‰ä½ç½®ã€‚

```python
class Food101(datasets.GeneratorBasedBuilder):
    """Food-101 Images dataset"""
 
    BUILDER_CONFIGS = [
        Food101Config(
            name="breakfast",
            description="Food types commonly eaten during breakfast.",
            data_url="https://link-to-breakfast-foods.zip",
            metadata_urls={
                "train": "https://link-to-breakfast-foods-train.txt", 
                "validation": "https://link-to-breakfast-foods-validation.txt"
            },
        ,
        Food101Config(
            name="dinner",
            description="Food types commonly eaten during dinner.",
            data_url="https://link-to-dinner-foods.zip",
            metadata_urls={
                "train": "https://link-to-dinner-foods-train.txt", 
                "validation": "https://link-to-dinner-foods-validation.txt"
            },
        )...
    ]
```

ç¾åœ¨ï¼Œå¦‚æœç”¨æˆ¶æƒ³è¦åŠ è¼‰ `breakfast` é…ç½®ï¼Œä»–å€‘å¯ä»¥ä½¿ç”¨é…ç½®åç¨±ï¼š

```python
from datasets import load_dataset

ds = load_dataset("food101", "breakfast", split="train")
```

### Add dataset metadata

æ·»åŠ æœ‰é—œæ•¸æ“šé›†çš„ä¿¡æ¯å°æ–¼ç”¨æˆ¶äº†è§£æ›´å¤šä¿¡æ¯å¾ˆæœ‰ç”¨ã€‚æ­¤ä¿¡æ¯å­˜å„²åœ¨ç”± `info` æ–¹æ³•è¿”å›çš„ `DatasetInfo` é¡åˆ¥ä¸­ã€‚ç”¨æˆ¶å¯ä»¥é€šéä»¥ä¸‹æ–¹å¼è¨ªå•æ­¤ä¿¡æ¯ï¼š

```python
from datasets import load_dataset_builder

ds_builder = load_dataset_builder("food101")

print(ds_builder.info)
```

æ‚¨å¯ä»¥æŒ‡å®šæœ‰é—œæ•¸æ“šé›†çš„å¤§é‡ä¿¡æ¯ï¼Œä½†éœ€è¦åŒ…å«çš„ä¸€äº›é‡è¦ä¿¡æ¯åŒ…æ‹¬ï¼š

1. `description` æä¾›æ•¸æ“šé›†çš„ç°¡æ½”æè¿°ã€‚
2. `features` æŒ‡å®šæ•¸æ“šé›†åˆ—é¡å‹ã€‚ç”±æ–¼æ‚¨æ­£åœ¨å‰µå»ºåœ–åƒåŠ è¼‰è…³æœ¬ï¼Œå› æ­¤æ‚¨éœ€è¦åŒ…å«åœ–åƒåŠŸèƒ½ã€‚
3. `supervised_keys` æŒ‡å®šè¼¸å…¥ç‰¹å¾µå’Œæ¨™ç±¤ã€‚
4. `homepage` æä¾›æ•¸æ“šé›†ä¸»é çš„éˆæ¥ã€‚
5. `citation` æ˜¯æ•¸æ“šé›†çš„ BibTeX å¼•ç”¨ã€‚
6. `license` èªªæ˜æ•¸æ“šé›†çš„è¨±å¯è­‰ã€‚

!!! info
    æ‚¨æœƒæ³¨æ„åˆ°è¨±å¤šæ•¸æ“šé›†ä¿¡æ¯æ˜¯åœ¨åŠ è¼‰è…³æœ¬çš„å‰é¢å®šç¾©çš„ï¼Œé€™ä½¿å¾—å®ƒæ›´æ˜“æ–¼é–±è®€ã€‚æ‚¨é‚„å¯ä»¥è¼¸å…¥å…¶ä»– Datasets.Featuresï¼Œå› æ­¤è«‹å‹™å¿…æŸ¥çœ‹å®Œæ•´åˆ—è¡¨ä»¥äº†è§£æ›´å¤šè©³ç´°ä¿¡æ¯ã€‚

```python
def _info(self):
    return datasets.DatasetInfo(
        description=_DESCRIPTION,
        features=datasets.Features(
            {
                "image": datasets.Image(),
                "label": datasets.ClassLabel(names=_NAMES),
            }
        ),
        supervised_keys=("image", "label"),
        homepage=_HOMEPAGE,
        citation=_CITATION,
        license=_LICENSE,
        task_templates=[ImageClassification(image_column="image", label_column="label")],
    )
```

### Download and define the dataset splits

ç¾åœ¨æ‚¨å·²ç¶“æ·»åŠ äº†æœ‰é—œæ•¸æ“šé›†çš„ä¸€äº›ä¿¡æ¯ï¼Œä¸‹ä¸€æ­¥æ˜¯ä¸‹è¼‰æ•¸æ“šé›†ä¸¦ç”Ÿæˆæ‹†åˆ†ã€‚

1. ä½¿ç”¨ `DownloadManager.download()` æ–¹æ³•ä¸‹è¼‰æ•¸æ“šé›†ä»¥åŠæ‚¨æƒ³è¦èˆ‡å…¶é—œè¯çš„ä»»ä½•å…¶ä»– metadataã€‚è©²æ–¹æ³•æ¥å—ï¼š

    - ä¸€å€‹åœ¨ Hub dataset repository çš„è³‡æ–™é›†åç¨± (`data/folder`)
    - ä¸€å€‹ URL å‡å¦‚ç›¸é—œæª”æ¡ˆè¢«ç½®æ”¾åœ¨å…¶å®ƒåœ°æ–¹
    - ä¸€å€‹ list æˆ– dictionary ä¾†è¡¨ç¤º file names æˆ– URLs

    åœ¨ Food-101 åŠ è¼‰è…³æœ¬ä¸­ï¼Œæ‚¨æœƒå†æ¬¡æ³¨æ„åˆ° URL æ˜¯åœ¨è…³æœ¬çš„å‰é¢å®šç¾©çš„ã€‚

2. ä¸‹è¼‰æ•¸æ“šé›†å¾Œï¼Œä½¿ç”¨ [SplitGenerator](https://huggingface.co/docs/datasets/v2.14.1/en/package_reference/builder_classes#datasets.SplitGenerator) ä¾†çµ„ç¹”æ¯å€‹åˆ†å‰²ä¸­çš„åœ–åƒå’Œæ¨™ç±¤ã€‚ä½¿ç”¨æ¨™æº–åç¨±å‘½åæ¯å€‹ splitï¼Œä¾‹å¦‚ï¼š`Split.TRAIN`ã€`Split.TEST` å’Œ `SPLIT.Validation`ã€‚

    åœ¨ `gen_kwargs` åƒæ•¸ä¸­ï¼ŒæŒ‡å®šè¦è¿­ä»£å’ŒåŠ è¼‰çš„åœ–åƒçš„æ–‡ä»¶è·¯å¾‘ã€‚å¦‚æœ‰å¿…è¦ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ `DownloadManager.iter_archive()` è¿­ä»£ TAR å­˜æª”ä¸­çš„åœ–åƒã€‚æ‚¨é‚„å¯ä»¥åœ¨ `metadata_path` ä¸­æŒ‡å®šé—œè¯çš„æ¨™ç±¤ã€‚åœ–åƒå’Œ `metadata_path` å¯¦éš›ä¸Šæœƒå‚³éåˆ°ä¸‹ä¸€æ­¥ï¼Œæ‚¨å°‡åœ¨å…¶ä¸­å¯¦éš›ç”Ÿæˆæ•¸æ“šé›†ã€‚

```python
def _split_generators(self, dl_manager):
    archive_path = dl_manager.download(_BASE_URL)
    split_metadata_paths = dl_manager.download(_METADATA_URLS)
    return [
        datasets.SplitGenerator(
            name=datasets.Split.TRAIN,
            gen_kwargs={
                "images": dl_manager.iter_archive(archive_path),
                "metadata_path": split_metadata_paths["train"],
            },
        ),
        datasets.SplitGenerator(
            name=datasets.Split.VALIDATION,
            gen_kwargs={
                "images": dl_manager.iter_archive(archive_path),
                "metadata_path": split_metadata_paths["test"],
            },
        ),
    ]
```

### Generate the dataset

`GeneratorBasedBuilder` é¡åˆ¥ä¸­çš„æœ€å¾Œä¸€å€‹æ–¹æ³•å¯¦éš›ä¸Šç”Ÿæˆæ•¸æ“šé›†ä¸­çš„åœ–åƒå’Œæ¨™ç±¤ã€‚å®ƒæ ¹æ“š `info` æ–¹æ³•ä¸­çš„ features ä¸­æŒ‡å®šçš„çµæ§‹ç”Ÿæˆæ•¸æ“šé›†ã€‚å¦‚æ‚¨æ‰€è¦‹ï¼Œ`generate_examples` æ¥å—ä¸Šä¸€å€‹æ–¹æ³•ä¸­çš„åœ–åƒå’Œ `metadata_path` ä½œç‚ºåƒæ•¸ã€‚

ç¾åœ¨æ‚¨å¯ä»¥ç·¨å¯«ä¸€å€‹å‡½æ•¸ä¾†æ‰“é–‹å’ŒåŠ è¼‰æ•¸æ“šé›†ä¸­çš„ç¯„ä¾‹ï¼š

```python
def _generate_examples(self, images, metadata_path):
    """Generate images and labels for splits."""
    with open(metadata_path, encoding="utf-8") as f:
        files_to_keep = set(f.read().split("\n"))

    for file_path, file_obj in images:
        if file_path.startswith(_IMAGES_DIR):
            if file_path[len(_IMAGES_DIR) : -len(".jpg")] in files_to_keep:
                label = file_path.split("/")[2]
                yield file_path, {
                    "image": {"path": file_path, "bytes": file_obj.read()},
                    "label": label,
                }
```

###ã€€Generate the dataset metadata (optional)

å¯ä»¥ç”Ÿæˆ `dataset metadata` ä¸¦å°‡å…¶å­˜å„²åœ¨æ•¸æ“šé›†å¡ï¼ˆ`README.md` æ–‡ä»¶ï¼‰ä¸­ã€‚

é‹è¡Œä»¥ä¸‹å‘½ä»¤åœ¨ `README.md` ä¸­ç”Ÿæˆæ•¸æ“šé›†å…ƒæ•¸æ“šä¸¦ç¢ºä¿æ–°çš„åŠ è¼‰è…³æœ¬æ­£å¸¸å·¥ä½œï¼š

```bash
datasets-cli test path/to/<your-dataset-loading-script> --save_info --all_configs
```

å¦‚æœæ‚¨çš„åŠ è¼‰è…³æœ¬é€šéäº†æ¸¬è©¦ï¼Œæ‚¨çš„æ•¸æ“šé›†æ–‡ä»¶å¤¾ä¸­çš„ `README.md` æ–‡ä»¶çš„æ¨™é ­ä¸­ç¾åœ¨æ‡‰è©²åŒ…å« `dataset_info` YAML æ¬„ä½ã€‚

###ã€€Upload the dataset to the Hub

è…³æœ¬æº–å‚™å°±ç·’å¾Œï¼Œå‰µå»ºæ•¸æ“šé›†å¡ä¸¦å°‡å…¶ä¸Šå‚³åˆ° Hubã€‚

æ­å–œï¼Œæ‚¨ç¾åœ¨å¯ä»¥å¾ Hub åŠ è¼‰æ•¸æ“šé›†ï¼

```python
from datasets import load_dataset

load_dataset("<username>/my_dataset")
```