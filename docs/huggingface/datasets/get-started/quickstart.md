# Quickstart

æœ¬å¿«é€Ÿå…¥é–€é©ç”¨æ–¼æº–å‚™æ·±å…¥ç ”ç©¶ç¨‹å¼ç¢¼ä¸¦æŸ¥çœ‹å¦‚ä½•å°‡ ğŸ¤— æ•¸æ“šé›†é›†æˆåˆ°æ¨¡å‹è¨“ç·´å·¥ä½œæµç¨‹ä¸­çš„ç¯„ä¾‹çš„é–‹ç™¼äººå“¡ã€‚å¦‚æœæ‚¨æ˜¯åˆå­¸è€…ï¼Œæˆ‘å€‘å»ºè­°æ‚¨å¾ Huggingface çš„æ•™ç¨‹é–‹å§‹ï¼Œæ‚¨å°‡ç²å¾—æ›´å…¨é¢çš„ä»‹ç´¹ã€‚

æ¯å€‹æ•¸æ“šé›†éƒ½æ˜¯å”¯ä¸€çš„ï¼Œä¸¦ä¸”æ ¹æ“šä»»å‹™çš„ä¸åŒï¼ŒæŸäº›æ•¸æ“šé›†å¯èƒ½éœ€è¦é¡å¤–çš„æ­¥é©Ÿä¾†æº–å‚™è¨“ç·´ã€‚ä½†æ‚¨å§‹çµ‚å¯ä»¥ä½¿ç”¨ ğŸ¤— æ•¸æ“šé›†å·¥å…·ä¾†åŠ è¼‰å’Œè™•ç†æ•¸æ“šé›†ã€‚æœ€å¿«ã€æœ€ç°¡å–®çš„å…¥é–€æ–¹æ³•æ˜¯å¾ Hugging Face Hub åŠ è¼‰ç¾æœ‰æ•¸æ“šé›†ã€‚æœ‰æ•¸åƒå€‹æ•¸æ“šé›†å¯ä¾›é¸æ“‡ï¼Œæ¶µè“‹è¨±å¤šä»»å‹™ã€‚é¸æ“‡æ‚¨æƒ³è¦ä½¿ç”¨çš„æ•¸æ“šé›†é¡å‹ï¼Œç„¶å¾Œé–‹å§‹å§ï¼

![](./assets/dataset-grouping.png)

é¦–å…ˆå®‰è£ğŸ¤—æ•¸æ“šé›†å¥—ä»¶ï¼š

```bash
pip install datasets
```

è¦ä½¿ç”¨éŸ³é »æ•¸æ“šé›†ï¼Œè«‹å®‰è£é¡å¤– `Audio`åŠŸèƒ½ï¼š

```bash
pip install datasets[audio]
```

è¦ä½¿ç”¨åœ–åƒæ•¸æ“šé›†ï¼Œè«‹å®‰è£é¡å¤– `Image` åŠŸèƒ½ï¼š

```bash
pip install datasets[vision]
```

## Audio

éŸ³é »æ•¸æ“šé›†çš„åŠ è¼‰æ–¹å¼èˆ‡æ–‡æœ¬æ•¸æ“šé›†ä¸€æ¨£ã€‚ç„¶è€Œï¼ŒéŸ³é »æ•¸æ“šé›†çš„é è™•ç†æœ‰é»ä¸åŒã€‚æ‚¨éœ€è¦ä¸€å€‹ **ç‰¹å¾µæå–å™¨**ï¼Œè€Œä¸æ˜¯ **åˆ†è©å™¨**ã€‚éŸ³é »è¼¸å…¥å¯èƒ½é‚„éœ€è¦é‡æ–°æ¡æ¨£å…¶æ¡æ¨£ç‡ï¼Œä»¥åŒ¹é…æ‚¨æ­£åœ¨ä½¿ç”¨çš„é è¨“ç·´æ¨¡å‹çš„æ¡æ¨£ç‡ã€‚åœ¨æœ¬å¿«é€Ÿå…¥é–€ä¸­ï¼Œæ‚¨å°‡æº–å‚™ [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) æ•¸æ“šé›†ç”¨æ–¼æ¨¡å‹è¨“ç·´ä¸¦å°å®¢æˆ¶é‡åˆ°çš„éŠ€è¡Œå•é¡Œé€²è¡Œåˆ†é¡ã€‚

1. é€šéå‘ `load_dataset()` å‡½æ•¸æä¾›æ•¸æ“šé›†åç¨±ã€æ•¸æ“šé›†é…ç½®ï¼ˆä¸¦éæ‰€æœ‰æ•¸æ“šé›†éƒ½æœ‰é…ç½®ï¼‰å’Œæ•¸æ“šé›†æ‹†åˆ†ä¾†åŠ è¼‰ MInDS-14 æ•¸æ“šé›†ï¼š

    ```python
    from datasets import load_dataset, Audio

    dataset = load_dataset("PolyAI/minds14", "en-US", split="train")
    ```

2. æ¥ä¸‹ä¾†ï¼Œå¾ ğŸ¤— Transformers å¥—ä»¶åŠ è¼‰é è¨“ç·´çš„ [Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base) æ¨¡å‹åŠå…¶ç›¸æ‡‰çš„ç‰¹å¾µæå–å™¨ã€‚åŠ è¼‰æ¨¡å‹å¾Œçœ‹åˆ°æœ‰é—œæŸäº›æ¬Šé‡æœªåˆå§‹åŒ–çš„è­¦å‘Šæ˜¯å®Œå…¨æ­£å¸¸çš„ã€‚é€™æ˜¯é æœŸçš„ï¼Œå› ç‚ºæ‚¨æ­£åœ¨åŠ è¼‰æ­¤æ¨¡å‹ check-point ä»¥ç”¨æ–¼å¦ä¸€ä»»å‹™çš„è¨“ç·´ã€‚

    ```python
    from transformers import AutoModelForAudioClassification, AutoFeatureExtractor

    model = AutoModelForAudioClassification.from_pretrained("facebook/wav2vec2-base")
    feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base")
    ```

3. [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) æ•¸æ“šé›†è³‡è¨Šé¡¯ç¤ºæ¡æ¨£ç‡ç‚º 8kHzï¼Œä½† Wav2Vec2 æ¨¡å‹æ˜¯åœ¨ 16kHZ æ¡æ¨£ç‡ä¸Šé è¨“ç·´çš„ã€‚æ‚¨éœ€è¦ä½¿ç”¨`cast_column()` å‡½æ•¸å’ŒéŸ³é »åŠŸèƒ½å°éŸ³é »åˆ—é€²è¡Œä¸Šæ¡æ¨£ï¼Œä»¥åŒ¹é…æ¨¡å‹çš„æ¡æ¨£ç‡ã€‚

    ```python
    dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
    dataset[0]["audio"]
    ```

    çµæœ:

    ```
    {'array': array([ 2.3443763e-05,  2.1729663e-04,  2.2145823e-04, ...,
            3.8356509e-05, -7.3497440e-06, -2.1754686e-05], dtype=float32),
    'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav',
    'sampling_rate': 16000}
    ```

4. å‰µå»ºä¸€å€‹å‡½æ•¸ä¾†ä½¿ç”¨ç‰¹å¾µæå–å™¨é è™•ç†éŸ³é »æ•¸çµ„ï¼Œä¸¦å°‡åºåˆ—æˆªæ–·ä¸¦å¡«å……ç‚ºæ•´é½Šçš„çŸ©å½¢å¼µé‡ã€‚è¦è¨˜ä½çš„æœ€é‡è¦çš„äº‹æƒ…æ˜¯åœ¨ç‰¹å¾µæå–å™¨ä¸­èª¿ç”¨éŸ³é »æ•¸çµ„ï¼Œå› ç‚ºè©²æ•¸çµ„ï¼ˆå¯¦éš›çš„èªéŸ³ä¿¡è™Ÿï¼‰æ˜¯æ¨¡å‹è¼¸å…¥ã€‚

    æ“æœ‰é è™•ç†å‡½æ•¸å¾Œï¼Œå¯ä»¥ä½¿ç”¨ `map()` å‡½æ•¸å°‡è©²å‡½æ•¸æ‡‰ç”¨æ–¼æ•¸æ“šé›†ä¸­çš„æ‰¹é‡æ•¸æ“šä¾†åŠ å¿«è™•ç†é€Ÿåº¦ã€‚

    ```python
    def preprocess_function(examples):
        audio_arrays = [x["array"] for x in examples["audio"]]
        inputs = feature_extractor(
            audio_arrays,
            sampling_rate=16000,
            padding=True,
            max_length=100000,
            truncation=True,
        )
        return inputs

    dataset = dataset.map(preprocess_function, batched=True)
    ```

5. ä½¿ç”¨ `rename_column()` å‡½æ•¸å°‡ `intent_class` åˆ—é‡å‘½åç‚º `labels`ï¼Œé€™æ˜¯ **Wav2Vec2ForSequenceClassification** ä¸­é æœŸçš„è¼¸å…¥åç¨±ï¼š

    ```python
    dataset = dataset.rename_column("intent_class", "labels")
    ```

6. æ ¹æ“šæ‚¨ä½¿ç”¨çš„æ©Ÿå™¨å­¸ç¿’æ¡†æ¶è¨­ç½®æ•¸æ“šé›†æ ¼å¼ã€‚

    ??? info "Pytorch"

        ä½¿ç”¨ `set_format()` å‡½æ•¸å°‡æ•¸æ“šé›†æ ¼å¼è¨­ç½®ç‚º `torch` ä¸¦æŒ‡å®šè¦è¨­ç½®æ ¼å¼çš„åˆ—ã€‚æ­¤å‡½æ•¸å³æ™‚æ‡‰ç”¨æ ¼å¼ã€‚è½‰æ›ç‚º PyTorch å¼µé‡å¾Œï¼Œå°‡æ•¸æ“šé›†åŒ…è£åœ¨ `torch.utils.data.DataLoader` ä¸­ï¼š

        ```python
        from torch.utils.data import DataLoader

        dataset.set_format(type="torch", columns=["input_values", "labels"])
        dataloader = DataLoader(dataset, batch_size=4)
        ```

    ??? info "Tensorflow"

        ä½¿ç”¨ `to_tf_dataset()` å‡½æ•¸è¨­ç½®æ•¸æ“šé›†æ ¼å¼ä»¥èˆ‡ TensorFlow å…¼å®¹ã€‚æ‚¨é‚„éœ€è¦å¾ ğŸ¤— Transformers å°å…¥æ•¸æ“šæ•´ç†å™¨ï¼Œä»¥å°‡ä¸åŒçš„åºåˆ—é•·åº¦çµ„åˆæˆä¸€æ‰¹ç›¸åŒé•·åº¦çš„ï¼š

        ```python
        import tensorflow as tf

        tf_dataset = dataset.to_tf_dataset(
            columns=["input_values"],
            label_cols=["labels"],
            batch_size=4,
            shuffle=True)
        ```

7. é–‹å§‹ä½¿ç”¨æ©Ÿå™¨å­¸ç¿’æ¡†æ¶é€²è¡Œè¨“ç·´ï¼æŸ¥çœ‹ ğŸ¤— Transformers [audio classification guide](https://huggingface.co/docs/transformers/tasks/audio_classification)ï¼Œäº†è§£å¦‚ä½•åœ¨éŸ³é »æ•¸æ“šé›†ä¸Šè¨“ç·´æ¨¡å‹çš„ç«¯åˆ°ç«¯ç¤ºä¾‹ã€‚

## Vision

åœ–åƒæ•¸æ“šé›†çš„åŠ è¼‰å°±åƒæ–‡æœ¬æ•¸æ“šé›†ä¸€æ¨£ã€‚ä½†æ˜¯ï¼Œæ‚¨éœ€è¦ä¸€å€‹ **feature extractor** ä¾†é è™•ç†æ•¸æ“šé›†ï¼Œè€Œä¸æ˜¯æ¨™è¨˜å™¨ã€‚å°‡æ•¸æ“šå¢å¼·æ‡‰ç”¨æ–¼åœ–åƒåœ¨è¨ˆç®—æ©Ÿè¦–è¦ºä¸­å¾ˆå¸¸è¦‹ï¼Œä»¥ä½¿æ¨¡å‹æ›´ç©©å¥åœ°é˜²æ­¢éåº¦æ“¬åˆã€‚æ‚¨å¯ä»¥è‡ªç”±ä½¿ç”¨ä»»ä½•æ‚¨æƒ³è¦çš„æ•¸æ“šå¢å¼·åº«ï¼Œç„¶å¾Œæ‚¨å¯ä»¥é€šéğŸ¤— æ•¸æ“šé›†æ‡‰ç”¨å¢å¼·ã€‚åœ¨æœ¬å¿«é€Ÿå…¥é–€ä¸­ï¼Œæ‚¨å°‡åŠ è¼‰ [Beans æ•¸æ“šé›†](https://huggingface.co/datasets/beans)ä¸¦æº–å‚™å¥½è®“æ¨¡å‹è¨“ç·´ä¸¦å¾è‘‰å­åœ–åƒä¸­è­˜åˆ¥ç–¾ç—…ã€‚

1. é€šéå‘`load_dataset()` å‡½æ•¸æä¾›æ•¸æ“šé›†åç¨±å’Œæ•¸æ“šé›†æ‹†åˆ†ä¾†åŠ è¼‰ Beans æ•¸æ“šé›†ï¼š

    ```python
    from datasets import load_dataset, Image

    dataset = load_dataset("beans", split="train")
    ```

2. ç¾åœ¨æ‚¨å¯ä»¥ä½¿ç”¨æ‚¨å–œæ­¡çš„ä»»ä½•å¥—ä»¶ï¼ˆ[Albumentations](https://albumentations.ai/)ã€[imgaug](https://imgaug.readthedocs.io/en/latest/)ã€[Kornia](https://kornia.readthedocs.io/en/latest/ï¼‰ä¾†æ·»åŠ ä¸€äº›æ•¸æ“šå¢å¼·ã€‚åœ¨é€™è£¡ï¼Œæ‚¨å°‡ä½¿ç”¨ [torchvision](https://pytorch.org/vision/stable/transforms.html) éš¨æ©Ÿæ›´æ”¹åœ–åƒçš„é¡è‰²å±¬æ€§ï¼š

    ```python
    from torchvision.transforms import Compose, ColorJitter, ToTensor

    jitter = Compose(
        [ColorJitter(brightness=0.5, hue=0.5), ToTensor()]
    )
    ```

3. å‰µå»ºä¸€å€‹å‡½æ•¸ä»¥å°‡è®Šæ›æ‡‰ç”¨åˆ°æ•¸æ“šé›†ä¸¦ç”Ÿæˆæ¨¡å‹è¼¸å…¥ï¼š`pixel_values`ã€‚

    ```python
    def transforms(examples):
        examples["pixel_values"] = [jitter(image.convert("RGB")) for image in examples["image"]]
        return examples
    ```

4. ä½¿ç”¨ `with_transform()` å‡½æ•¸å³æ™‚æ‡‰ç”¨æ•¸æ“šå¢å¼·ï¼š

    ```python
    dataset = dataset.with_transform(transforms)
    ```

5. æ ¹æ“šæ‚¨ä½¿ç”¨çš„æ©Ÿå™¨å­¸ç¿’æ¡†æ¶è¨­ç½®æ•¸æ“šé›†æ ¼å¼ã€‚

    ??? info "Pytorch"

        å°‡æ•¸æ“šé›†åŒ…è£åœ¨ `torch.utils.data.DataLoader` ä¸­ã€‚æ‚¨é‚„éœ€è¦å‰µå»ºä¸€å€‹æ•´ç†å‡½æ•¸ä¾†å°‡æ¨£æœ¬æ•´ç†æˆæ‰¹æ¬¡ï¼š

        ```python
        from torch.utils.data import DataLoader

        def collate_fn(examples):
            images = []
            labels = []
            for example in examples:
                images.append((example["pixel_values"]))
                labels.append(example["labels"])
                
            pixel_values = torch.stack(images)
            labels = torch.tensor(labels)
            return {"pixel_values": pixel_values, "labels": labels}

        dataloader = DataLoader(dataset, collate_fn=collate_fn, batch_size=4)
        ```

6. é–‹å§‹ä½¿ç”¨æ©Ÿå™¨å­¸ç¿’æ¡†æ¶é€²è¡Œè¨“ç·´ï¼æŸ¥çœ‹ ğŸ¤— Transformers [image classification guide](https://huggingface.co/docs/transformers/tasks/image_classification)ï¼Œäº†è§£å¦‚ä½•åœ¨åœ–åƒæ•¸æ“šé›†ä¸Šè¨“ç·´æ¨¡å‹çš„ç«¯åˆ°ç«¯ç¤ºä¾‹ã€‚

## NLP

æ–‡æœ¬éœ€è¦ç”±åˆ†è©å™¨åˆ†è©ç‚ºå–®ç¨çš„åˆ†è©ã€‚åœ¨å¿«é€Ÿå…¥é–€ä¸­ï¼Œæ‚¨å°‡åŠ è¼‰ [Microsoft Research Paraphrase Corpus (MRPC)](https://huggingface.co/datasets/glue/viewer/mrpc) è¨“ç·´æ•¸æ“šé›†ä¾†è¨“ç·´æ¨¡å‹ï¼Œä»¥ç¢ºå®šä¸€å°å¥å­æ˜¯å¦è¡¨ç¤ºç›¸åŒçš„æ„æ€ã€‚

1. é€šéå‘ `load_dataset()` å‡½æ•¸æä¾›æ•¸æ“šé›†åç¨±ã€æ•¸æ“šé›†é…ç½®ï¼ˆä¸¦éæ‰€æœ‰æ•¸æ“šé›†éƒ½æœ‰é…ç½®ï¼‰å’Œæ•¸æ“šé›†åˆ†å‰²ä¾†åŠ è¼‰ MRPC æ•¸æ“šé›†ï¼š

    ```python
    from datasets import load_dataset

    dataset = load_dataset("glue", "mrpc", split="train")
    ```

2. æ¥ä¸‹ä¾†ï¼Œå¾ ğŸ¤— Transformers å¥—ä»¶åŠ è¼‰é è¨“ç·´çš„ [BERT](https://huggingface.co/bert-base-uncased) æ¨¡å‹åŠå…¶ç›¸æ‡‰çš„åˆ†è©å™¨ã€‚åŠ è¼‰æ¨¡å‹å¾Œçœ‹åˆ°æœ‰é—œæŸäº›æ¬Šé‡æœªåˆå§‹åŒ–çš„è­¦å‘Šæ˜¯å®Œå…¨æ­£å¸¸çš„ã€‚é€™æ˜¯é æœŸçš„ï¼Œå› ç‚ºæ‚¨æ­£åœ¨åŠ è¼‰æ­¤æ¨¡å‹ check-point ä»¥ç”¨æ–¼å¦ä¸€ä»»å‹™çš„è¨“ç·´ã€‚

    ??? info "Pytorch"

        ```python
        from transformers import AutoModelForSequenceClassification, AutoTokenizer

        model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased")
        tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
        ```

    ??? info "Tensorflow"

        ```python
        from transformers import TFAutoModelForSequenceClassification, AutoTokenizer

        model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-uncased")
        tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")    
        ```

3. å‰µå»ºä¸€å€‹å‡½æ•¸ä¾†æ¨™è¨˜æ•¸æ“šé›†ï¼Œæ‚¨é‚„æ‡‰è©²å°‡æ–‡æœ¬æˆªæ–·ä¸¦å¡«å……ç‚ºæ•´é½Šçš„çŸ©å½¢å¼µé‡ã€‚åˆ†è©å™¨åœ¨æ•¸æ“šé›†ä¸­ç”Ÿæˆä¸‰å€‹æ–°åˆ—ï¼š`input_ids`ã€`token_type_ids` å’Œ `attention_mask`ã€‚é€™äº›æ˜¯æ¨¡å‹è¼¸å…¥ã€‚

    ä½¿ç”¨ `map()` å‡½æ•¸é€šéå°‡æ¨™è¨˜åŒ–å‡½æ•¸æ‡‰ç”¨æ–¼æ•¸æ“šé›†ä¸­çš„æ‰¹é‡ç¤ºä¾‹ä¾†åŠ å¿«è™•ç†é€Ÿåº¦ï¼š

    ```python
    def encode(examples):
        return tokenizer(examples["sentence1"], examples["sentence2"], truncation=True, padding="max_length")

    dataset = dataset.map(encode, batched=True)

    dataset[0]
    ```

    çµæœ:

    ```bash
    {'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
    'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .',
    'label': 1,
    'idx': 0,
    'input_ids': array([  101,  7277,  2180,  5303,  4806,  1117,  1711,   117,  2292, 1119,  1270,   107,  1103,  7737,   107,   117,  1104,  9938, 4267, 12223, 21811,  1117,  2554,   119,   102, 11336,  6732, 3384,  1106,  1140,  1112,  1178,   107,  1103,  7737,   107, 117,  7277,  2180,  5303,  4806,  1117,  1711,  1104,  9938, 4267, 12223, 21811,  1117,  2554,   119,   102]),
    'token_type_ids': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),
    'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}
    ```

4. å°‡æ¨™ç±¤åˆ—é‡å‘½åç‚º `labels`ï¼Œé€™æ˜¯ [BertForSequenceClassification](https://huggingface.co/docs/transformers/main/en/model_doc/bert#transformers.BertForSequenceClassification) ä¸­é æœŸçš„è¼¸å…¥åç¨±ï¼š

    ```python
    dataset = dataset.map(lambda examples: {"labels": examples["label"]}, batched=True)
    ```

5. æ ¹æ“šæ‚¨ä½¿ç”¨çš„æ©Ÿå™¨å­¸ç¿’æ¡†æ¶è¨­ç½®æ•¸æ“šé›†æ ¼å¼ã€‚

    ??? info "Pytorch"

        ä½¿ç”¨ `set_format()` å‡½æ•¸å°‡æ•¸æ“šé›†æ ¼å¼è¨­ç½®ç‚º `torch` ä¸¦æŒ‡å®šè¦è¨­ç½®æ ¼å¼çš„åˆ—ã€‚æ­¤å‡½æ•¸å³æ™‚æ‡‰ç”¨æ ¼å¼ã€‚è½‰æ›ç‚º PyTorch å¼µé‡å¾Œï¼Œå°‡æ•¸æ“šé›†åŒ…è£åœ¨ `torch.utils.data.DataLoader` ä¸­ï¼š

        ```python
        import torch

        dataset.set_format(type="torch", columns=["input_ids", "token_type_ids", "attention_mask", "labels"])
        dataloader = torch.utils.data.DataLoader(dataset, batch_size=32)    
        ```

    ??? info "Tensorflow"

        ä½¿ç”¨ `to_tf_dataset()` å‡½æ•¸è¨­ç½®æ•¸æ“šé›†æ ¼å¼ä»¥èˆ‡ TensorFlow å…¼å®¹ã€‚æ‚¨é‚„éœ€è¦å¾ ğŸ¤— Transformers å°å…¥æ•¸æ“šæ•´ç†å™¨ï¼Œä»¥å°‡ä¸åŒçš„åºåˆ—é•·åº¦çµ„åˆæˆä¸€æ‰¹ç›¸åŒé•·åº¦çš„ï¼š

        ```python
        import tensorflow as tf
        from transformers import DataCollatorWithPadding

        data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")

        tf_dataset = dataset.to_tf_dataset(
            columns=["input_ids", "token_type_ids", "attention_mask"],
            label_cols=["labels"],
            batch_size=2,
            collate_fn=data_collator,
            shuffle=True)
        ```

6. é–‹å§‹ä½¿ç”¨æ©Ÿå™¨å­¸ç¿’æ¡†æ¶é€²è¡Œè¨“ç·´ï¼æŸ¥çœ‹ ğŸ¤— Transformers [text classification guide](https://huggingface.co/docs/transformers/tasks/sequence_classification)ï¼Œäº†è§£å¦‚ä½•åœ¨æ–‡æœ¬æ•¸æ“šé›†ä¸Šè¨“ç·´æ¨¡å‹çš„ç«¯åˆ°ç«¯ç¤ºä¾‹ã€‚

## ä¸‹ä¸€æ­¥æ˜¯ä»€éº¼ï¼Ÿ

ğŸ¤— æ•¸æ“šé›†å¿«é€Ÿå…¥é–€åˆ°æ­¤çµæŸï¼æ‚¨å¯ä»¥ä½¿ç”¨å–®å€‹å‡½æ•¸åŠ è¼‰ä»»ä½•æ–‡æœ¬ã€éŸ³é »æˆ–åœ–åƒæ•¸æ“šé›†ï¼Œä¸¦ç‚ºæ¨¡å‹è¨“ç·´åšå¥½æº–å‚™ã€‚

å°æ–¼å¾ŒçºŒæ­¥é©Ÿï¼Œè«‹æŸ¥çœ‹æˆ‘å€‘çš„[æ“ä½œæŒ‡å—](https://huggingface.co/docs/datasets/how_to)ï¼Œäº†è§£å¦‚ä½•åŸ·è¡Œæ›´å…·é«”çš„æ“ä½œï¼Œä¾‹å¦‚åŠ è¼‰ä¸åŒçš„æ•¸æ“šé›†æ ¼å¼ã€å°é½Šæ¨™ç±¤å’Œæµå¼å‚³è¼¸å¤§å‹æ•¸æ“šé›†ã€‚å¦‚æœæ‚¨æœ‰èˆˆè¶£äº†è§£æœ‰é—œ ğŸ¤— æ•¸æ“šé›†æ ¸å¿ƒæ¦‚å¿µçš„æ›´å¤šä¿¡æ¯ï¼Œè«‹å–æ¯å’–å•¡ä¸¦é–±è®€æˆ‘å€‘çš„[æ¦‚å¿µæŒ‡å—](https://huggingface.co/docs/datasets/about_arrow)ï¼
