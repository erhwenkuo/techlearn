# å‹•æ…‹ä¿®æ”¹é‹ä½œä¸­ Chain çš„è¨­å®š

åŸæ–‡: [å‹•æ…‹ä¿®æ”¹é‹ä½œä¸­çš„ Chain è¨­å®š](https://myapollo.com.tw/blog/langchain-configure-chain-at-runtime/)

ä¸çŸ¥é“ä½ æœ‰æ²’æœ‰æƒ³é 1 å€‹ Chain è¦å¦‚ä½•åšåˆ°å‹•æ…‹è¼‰å…¥ä¸åŒä½¿ç”¨è€…çš„è³‡æ–™ï¼Ÿæˆ–è€…å¦‚ä½•åƒ ChatGPT é‚£æ¨£å¯ä»¥åˆ‡æ›æ¨¡å‹ï¼Ÿ

é€™å€‹åŠŸèƒ½åœ¨ LangChain ä¸­ç¨±ç‚º `configurable_fields` èˆ‡ `configurable_alternatives` ï¼Œå¯ä»¥è®“æˆ‘å€‘å‹•æ…‹ä¿®æ”¹ chain çš„è¨­å®šï¼Œæˆ–è€…ç½®æ› chain ä¸Šçš„æŸå€‹ Runnable (ä¾‹å¦‚ `prompt`, `language model` éƒ½æ˜¯ Runnable) ã€‚

èƒ½å‹•æ…‹è®Šæ›´ chain è¨­å®šçš„åŠŸèƒ½ç›¸ç•¶é‡è¦ï¼Œå­¸æœƒä½¿ç”¨å®ƒä¹Ÿæ˜¯å¿…è¦çš„åŠŸèª²ä¹‹ä¸€ï¼Œå¦å‰‡æˆ‘å€‘æ‰€é–‹ç™¼å‡ºä¾†çš„æ‡‰ç”¨æœƒå–ªå¤±ä¸å°‘å½ˆæ€§ï¼

## æœ¬æ–‡ç’°å¢ƒ

- Linux (Ubuntu)
- Python 3
- LangChain

```
pip install langchain langchain-openai python-dotenv
```

## è¨­å®š `configurable_fields` ä»¥å‹•æ…‹æ›´æ–°è¨­å®š

å­¸ç¿’å¦‚ä½•ä½¿ç”¨ configurable_fields ä¹‹å‰ï¼Œå…ˆçœ‹ 1 å€‹å•é¡Œï¼Œä»¥ä¸‹ç¯„ä¾‹è®“ openai èªè¨€æ¨¡å‹èªªå€‹ç¬‘è©±ï¼š

```python
from langchain_community.llms import Ollama
from langchain_core.prompts import ChatPromptTemplate


llm = Ollama(model='llama2', temperature=0)
prompt = ChatPromptTemplate.from_messages([
    ("user", "{input}"),
])
chain = prompt | llm
print(chain.invoke({'input': 'Tell me a joke'}))
```

ä¸Šè¿°ç¯„ä¾‹åŸ·è¡Œä¹‹å¾Œï¼Œæ¯æ¬¡éƒ½æœƒå‡ºç¾ç›¸åŒå…§å®¹ï¼ˆå…§å®¹å¦‚ä¸‹ï¼‰ï¼Œé€™æ˜¯å› ç‚ºæˆ‘å€‘è¼‰å…¥ llama2 æ™‚å°‡ temperature è¨­å®šç‚º 0 çš„ç·£æ•…ï¼Œæ‰€ä»¥èªè¨€æ¨¡å‹çš„å›æ‡‰ä¸æœƒæœ‰ä¸€äº›éš¨æ©Ÿæ€§ç”¢ç”Ÿï¼Œé€ æˆæ¯æ¬¡éƒ½å›æ‡‰ç›¸åŒå…§å®¹ï¼š

```bash
Sure, here's one:

Why don't scientists trust atoms?
Because they make up everything!

I hope you found that amusing! Do you want to hear another one?
```

å¦‚æœæˆ‘å€‘æƒ³å‹•æ…‹è¨­å®šä¸€äº›éš¨æ©Ÿæ€§çš„è©±ï¼Œé›–ç„¶å¯ä»¥æ¯æ¬¡ç”¢ç”Ÿå›æ‡‰éƒ½é‡æ–°è¼‰å…¥èªè¨€æ¨¡å‹ä¸¦é‡æ–°è¨­å®š `temperature` åƒæ•¸ï¼Œä½†æ˜¯ä¹Ÿæœƒå°è‡´å›æ‡‰æ™‚é–“æ‹‰é•·ï¼Œå› ç‚ºèªè¨€æ¨¡å‹çš„è¼‰å…¥æœƒéœ€è¦ä¸€é»é»æ™‚é–“ï¼Œä¹Ÿè¨±æ¯”è¼ƒå¥½çš„æ–¹å¼æ˜¯è®“ chain å…·æœ‰å‹•æ…‹ä¿®æ”¹è¨­å®šçš„èƒ½åŠ›ï¼›åˆæˆ–è€…æˆ‘å€‘æƒ³åšäº›å¯¦é©—æ¯”è¼ƒä¸åŒ prompt / åƒæ•¸ / æ¨¡å‹çš„å·®ç•°ï¼Œé€™æ™‚å€™ä¹Ÿéœ€è¦å‹•æ…‹ä¿®æ”¹è¨­å®šçš„èƒ½åŠ›ï¼Œè®“æˆ‘å€‘åœ¨å‘¼å« chain æ™‚ä½¿ç”¨ä¸åŒçš„åƒæ•¸ï¼Œä»¥å°çµæœé€²è¡Œæ¯”è¼ƒï¼›ç”šè‡³æ˜¯è®“ chain å…·æœ‰æœå‹™å¤šåä½¿ç”¨è€…çš„èƒ½åŠ›ï¼Œé‡å°ä½¿ç”¨è€…å¸³è™Ÿä¸åŒè¼‰å…¥ä¸åŒçš„èªè¨€æ¨¡å‹ã€å°è©±ç´€éŒ„ï¼Œå°±åƒ ChatGPT ä¸€æ¨£ï¼Œæ‰€æœ‰ä½¿ç”¨è€…ä¹‹é–“ä¸æœƒçœ‹åˆ°å½¼æ­¤çš„å°è©±ç´€éŒ„ä»¥ç¢ºä¿éš±ç§ï¼Œè€Œä¸”ä»˜è²»ä½¿ç”¨è€…å¤šäº†å¯ä»¥åˆ‡æ›è‡³æ›´å¼·å¤§çš„èªè¨€æ¨¡å‹çš„åŠŸèƒ½ã€‚

é‡å°é€™äº›éœ€æ±‚ï¼Œ LangChain ç‚ºæ¯å€‹ Runnable æä¾› 1 å€‹ç¨±ç‚º `configurable_fields` çš„æ–¹æ³•ï¼Œè®“æˆ‘å€‘å¯ä»¥å‹•æ…‹ä¿®æ”¹ chain ä¸Šçš„è¨­å®šå€¼ã€‚

ä¸‹åˆ—æ˜¯èƒ½å‹•æ…‹è¨­å®š `temperature` çš„ç¨‹å¼ç¢¼ç¯„ä¾‹ï¼š

```python
from langchain_community.llms import Ollama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import ConfigurableField

# è¨­å®šè¦å‹•æ…‹ä¿®æ”¹çš„è¨­å®šå€¼
llm = Ollama(model='llama2', temperature=0).configurable_fields(
    temperature=ConfigurableField(
        id="llm_temperature",
        name="LLM Temperature",
        description="The temperature of the LLM",
    )
)

# æ§‹å»ºä¸€å€‹ ChatPromptTempate ç‰©ä»¶
prompt = ChatPromptTemplate.from_messages([
    ("user", "{input}"),
])

# çµ„åˆæˆä¸€å€‹ chain
chain = prompt | llm

# åœ¨è§¸ç™¼ chain åŸ·è¡Œä¹‹å‰, è¨­å®šåƒæ•¸
print(
    chain.with_config(
        configurable={"llm_temperature": 0.9}
    ).invoke({'input': 'Tell me a joke'})
)
```

ä¸Šè¿°ç¯„ä¾‹åŸ·è¡Œçµæœå¦‚ä¸‹ï¼Œå¯ä»¥çœ‹åˆ°ç”¢ç”Ÿçš„å…§å®¹å·²ç¶“ç”¢ç”Ÿè®ŠåŒ–ï¼š

```bash
I'm just an AI, I don't have personal preferences or emotions, but I can certainly try to come up with a joke for you! Here is one:

Why don't scientists trust atoms? Because they make up everything! ğŸ˜‚

I hope that brought a smile to your face! Do you have any specific topics or subjects you would like me to generate jokes about?
```

ä»¥ä¸‹èªªæ˜ä¸Šè¿°ç¯„ä¾‹çš„é‡è¦éƒ¨åˆ†ã€‚

ç‚ºäº†èƒ½å¤ å‹•æ…‹è¨­å®š `Ollama()` çš„ `temperature` åƒæ•¸ï¼Œæ‰€ä»¥åœ¨ `Ollama(model='llama2', temperature=0)` ä¹‹å¾Œé¡å¤–å‘¼å«äº† `configurable_fields` æ–¹æ³•ï¼Œå°‡ `temperature` åƒæ•¸è¨­å®šç‚º `ConfigurableField`ï¼Œä¸¦å°‡å®ƒçš„è¨­å®šç¨±ç‚º `llm_temperature` ï¼Œæˆ‘å€‘ä¹‹å¾Œå°±å¯ä»¥åœ¨ä½¿ç”¨ chain æ™‚ï¼Œå¸¶å…¥ `llm_temperature` çš„è¨­å®šå€¼ï¼Œ LangChain å°±æœƒçŸ¥é“è¦ä¿®æ”¹ `temperature` åƒæ•¸ï¼š

```python
llm = Ollama(model='llama2', temperature=0).configurable_fields(
    temperature=ConfigurableField(
        id="llm_temperature",
        name="LLM Temperature",
        description="The temperature of the LLM",
    )
)
```

ä½¿ç”¨ chain æ™‚ï¼Œå¯ä»¥é¡å¤–å‘¼å« with_config() æ–¹æ³•è¨­å®š temperature :

```python
chain.with_config(
    configurable={"llm_temperature": 0.9}
).invoke({'input': 'Tell me a joke'})
```

æ¥ä¸‹ä¾†ï¼Œå†çœ‹ 1 å€‹åŒæ™‚å¯ä»¥è¨­å®šèªè¨€æ¨¡å‹èˆ‡ `temperature` çš„ç¯„ä¾‹ï¼š

```python
from langchain_community.llms import Ollama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import ConfigurableField


llm = Ollama(model='llama2', temperature=0).configurable_fields(
    temperature=ConfigurableField(
        id="temperature",
        name="LLM Temperature",
        description="The temperature of the LLM",
    ),
    model=ConfigurableField(
        id="model",
        name="The Model",
        description="The language model",
    ),
)

prompt = ChatPromptTemplate.from_messages([
    ("user", "{input}"),
])
chain = prompt | llm

print(chain.with_config(
    configurable={
        "model": "codellama",
        "temperature": 0.9
    }
).invoke({'input': 'Tell me a joke'}))
```

ä¸Šè¿°ç¯„ä¾‹é€é `configurable_fields()` æ–¹æ³•è¨­å®š 2 å€‹å¯å‹•æ…‹èª¿æ•´çš„åƒæ•¸ï¼Œåˆ†åˆ¥ç‚º `temperature` èˆ‡ `model`ï¼Œä¸¦ä¸”åœ¨å‘¼å« chain æ™‚è¨­å®š 2 å€‹åƒæ•¸ï¼Œ `with_config(configurable={"model": "codellama", "temperature": 0.9})`ã€‚

## ç”¨ `configurable_alternatives` å‹•æ…‹ç½®æ› Runnable

å…ˆå‰çš„ç¯„ä¾‹éƒ½æ˜¯ä½¿ç”¨ llama2 çš„èªè¨€æ¨¡å‹ï¼Œé€™äº›èªè¨€æ¨¡å‹æ˜¯ä½¿ç”¨ [Ollama](https://ollama.com/) è¼‰å…¥ï¼Œæ‰€ä»¥å¯ä»¥ä½¿ç”¨ `configurable_fields()` å‹•æ…‹è¨­å®šèªè¨€æ¨¡å‹ï¼Œä¸é OpenAI çš„èªè¨€æ¨¡å‹å°±ä¸èƒ½ä½¿ç”¨ Ollama è¼‰å…¥ï¼Œé€™ä½¿å¾—æˆ‘å€‘ç„¡æ³•ä½¿ç”¨ `configurable_fields()` æ–¹æ³•å‹•æ…‹åˆ‡æ›èªè¨€æ¨¡å‹ã€‚

LangChain é‚„æœ‰ 1 å€‹ç¨±ç‚º `configurable_alternatives()` çš„æ–¹æ³•ï¼Œå¯ä»¥å‹•æ…‹ç½®æ› Runnable, ä¹Ÿå°±æ˜¯èªªæˆ‘å€‘å¯ä»¥é€é `configurable_alternatives()` æ–¹æ³•ï¼Œå‹•æ…‹æŠŠ `Ollama(model='llama2')` æ›æˆ `ChatOpenAI(model="gpt-3.5-turbo", temperature=0)`, å…¶ç¯„ä¾‹å¦‚ä¸‹ï¼š

```python
from langchain_community.llms import Ollama
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import ConfigurableField

llm = Ollama(model='llama2').configurable_alternatives(
    ConfigurableField(id="llm"),
    default_key='llama2',
    gpt35=ChatOpenAI(model="gpt-3.5-turbo", temperature=0),
)

prompt = ChatPromptTemplate.from_messages([
    ("user", "{input}"),
])

chain = prompt | llm

print(chain.with_config(configurable={"llm": "gpt35"}).invoke({'input': 'Tell me a joke'}))
```

ä¸Šè¿°ç¯„ä¾‹é€é `configurable_alternatives()` è¨­å®š 1 å€‹ç¨±ç‚º `llm` çš„åƒæ•¸ï¼Œè©²åƒæ•¸æä¾› `llama2` èˆ‡ `gpt35` 2 å€‹é¸é …çš„è¨­å®šå€¼ï¼Œå…¶ä¸­ `llama2` æ˜¯é è¨­å€¼ï¼Œè©²é è¨­å€¼ä»£è¡¨ `Ollama(model='llama2')` ï¼Œè€Œ `gpt35` ä»£è¡¨ `ChatOpenAI(model="gpt-3.5-turbo", temperature=0) `ã€‚

æ‰€ä»¥æˆ‘å€‘å¯ä»¥åœ¨å‘¼å« chain çš„æ™‚å€™ï¼ŒåŒæ¨£é€é `with_config()` æ–¹æ³•å°‡ `llm` åƒæ•¸è¨­å®šç‚º `gpt35` æˆ–è€… `llama2` ï¼Œä¹Ÿå°±æ˜¯ä»¥ä¸‹é‡é»éƒ¨åˆ†ï¼š

```python
(ç•¥).with_config(configurable={"llm": "gpt35"})
```

å¦‚æ­¤ä¸€ä¾†æˆ‘å€‘å°±èƒ½å¤ å‹•æ…‹åˆ‡æ›ä»»ä½•èªè¨€æ¨¡å‹ï¼Œæˆ–è€… Runnable ã€‚

æœ€å¾Œå†çœ‹ 1 å€‹ç”¨ `configurable_alternatives()` å‹•æ…‹ç½®æ› Prompt çš„ç¯„ä¾‹ï¼š

```python
prompt = PromptTemplate.from_template(
    "Tell me a joke about {topic}"
).configurable_alternatives(
    ConfigurableField(id="prompt"),
    default_key="joke",
    poem=PromptTemplate.from_template("Write a short poem about {topic}"),
)

chain.with_config(configurable={"prompt": "poem"}).invoke({"topic": "Earth"})
```

## å‹•æ…‹è¼‰å…¥ä¸åŒä½¿ç”¨è€…çš„å°è©±ç´€éŒ„

æˆ‘å€‘åœ¨ä½¿ç”¨ ChatGPT æ™‚ï¼Œ ChatGPT æœƒå¹«æˆ‘å€‘ä¿ç•™ä¸åŒçš„å°è©±ç´€éŒ„ï¼Œè€Œä¸”ä¸æœƒçœ‹åˆ°ä¸å±¬æ–¼æˆ‘å€‘çš„å°è©±ç´€éŒ„ï¼Œé€™ä¹Ÿæ˜¯åšä»»ä½•æ‡‰ç”¨é‡è¦çš„åŠŸèƒ½â€”é‡å°ä¸åŒä½¿ç”¨è€…è¼‰å…¥å°ˆå±¬çš„è¨­å®šã€è³‡æ–™ã€‚

å­¸æœƒä½¿ç”¨ `configurable_fields()` èˆ‡ `configurable_alternatives()` ä¹‹å¾Œï¼Œæˆ‘å€‘å°±å¯ä»¥å‹•æ…‹è¼‰å…¥ä½¿ç”¨è€…çš„ç›¸é—œè¨­å®šèˆ‡è³‡æ–™ã€‚

èˆ‰å°è©±ç´€éŒ„(chat history)ç‚ºä¾‹ï¼Œ LangChain ç¨±æ­¤åŠŸèƒ½ç‚º memory, æˆ‘å€‘å¯ä»¥å°‡å°è©±ç´€éŒ„å­˜åœ¨å„ç¨® memory ä¸­ï¼Œä¾‹å¦‚æª”æ¡ˆã€è³‡æ–™åº«ç­‰ç­‰ï¼Œ LangChain æä¾›å¤šç¨®æ•´åˆæ–¹æ¡ˆå¯ä»¥é¸æ“‡ï¼Œè©³ç´°å¯ä»¥é–±è®€æ­¤[æ–‡ä»¶](https://integrations.langchain.com/memory)ã€‚

æœ¬æ–‡åƒ…å±•ç¤ºä»¥ Python dictionary å„²å­˜å°è©±ç´€éŒ„ä½œç‚ºæ•™å­¸ç¯„ä¾‹ã€‚

LangChain å…¶å¯¦æœ‰æä¾› 1 å€‹ Runnable ç¨±ç‚º `RunnableWithMessageHistory`, é€™å€‹é¡åˆ¥å…¶å¯¦å°±æ˜¯ä½¿ç”¨ `configurable_fields()` æ–¹æ³•ï¼Œé è¨­è®“æˆ‘å€‘å¯ä»¥å‹•æ…‹è¨­å®š `session_id` è—‰æ­¤è¼‰å…¥ä¸åŒå°è©±ç´€éŒ„ã€‚

ä»¥ä¸‹æ˜¯ä½¿ç”¨ `RunnableWithMessageHistory` çš„ç¯„ä¾‹ç¨‹å¼ç¢¼ï¼š

```python
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.prompts import MessagesPlaceholder
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_community.llms import Ollama


chat001 = ChatMessageHistory()

chat001.add_user_message('My name is Amo.')

store = {
    'chat001': chat001,
}


def get_chat_history(session_id: str) -> BaseChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]


llm = Ollama(model='llama2')

prompt = ChatPromptTemplate.from_messages([
    ('system', 'You are a good assistant.'),
    MessagesPlaceholder(variable_name='chat_history'),
    ('user', '{input}'),
])

chain = prompt | llm

with_message_history = RunnableWithMessageHistory(
    chain,
    get_chat_history,
    input_messages_key="input",
    history_messages_key="chat_history",
)

session_id = 'chat001'

input_text = input('>>> ')

while input_text.lower() != 'bye':
    if input_text:
        response = with_message_history.with_config(
            configurable={'session_id': session_id}
        ).invoke({'input': input_text})
        print(response)
    input_text = input('>>> ')
```

ä¸Šè¿°ç¯„ä¾‹çš„é‡é»éƒ¨åˆ†åœ¨æ–¼ä½¿ç”¨ `RunnableWithMessageHistory()` å° chain é€²è¡Œè¨­å®šï¼Œè¨­å®š 1 å€‹æ–¹æ³•æ¥å— `session_id` ï¼Œä¸¦å›å‚³å°è©±ç´€éŒ„ï¼Œä¹Ÿå°±æ˜¯å‡½å¼ `get_chat_history` , ä¸¦ä¸”è¨­å®šé€™äº›å°è©±ç´€éŒ„è¦æ”¾é€² prompt çš„ `chat_history message` placeholder ä¸­ï¼š

```python
with_message_history = RunnableWithMessageHistory(
    chain,
    get_chat_history,
    input_messages_key="input",
    history_messages_key="chat_history",
)
```

å¦‚æ­¤ä¸€ä¾†ï¼Œæˆ‘å€‘å°±å¯ä»¥è¨­å®šè¦è®€å–å“ªå€‹ `session_id` çš„å°è©±ç´€éŒ„ï¼Œä»¥æ–°å¢/å»¶çºŒå°è©±ã€‚

åªæ˜¯é è¨­çš„ `get_chat_history()` é‚„æ˜¯ç„¡æ³•é‡å°ä¸åŒä½¿ç”¨è€…é€²è¡Œè¨­å®šï¼Œå¹¸å¥½ `RunnableWithMessageHistory` é‚„æä¾› `history_factory_config` åƒæ•¸ï¼Œå¯ä»¥å®¢è£½åŒ– `get_chat_history()` æ‰€æ¥å—çš„åƒæ•¸ï¼Œä¾‹å¦‚ä¸‹åˆ—ç¯„ä¾‹ä»¥ `history_factory_config` åƒæ•¸è¨­å®š `user_id` èˆ‡ `conversation_id` å¿…é ˆå‚³å…¥çµ¦ `get_chat_history()` æ–¹æ³•ï¼Œä»¥è—‰æ­¤å–å¾—ä¸åŒä½¿ç”¨è€…çš„ç‰¹å®šå°è©±ç´€éŒ„ï¼š

```python
from langchain_core.messages import HumanMessage
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.prompts import MessagesPlaceholder
from langchain_core.runnables import ConfigurableFieldSpec
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_community.llms import Ollama
from langchain_community.embeddings import OllamaEmbeddings


chat001 = ChatMessageHistory()
chat001.add_user_message('My name is Amo.')

chat002 = ChatMessageHistory()
chat002.add_user_message('My name is Jeff.')

store = {
    ('amo', 'chat001'): chat001,
    ('jeff', 'chat002'): chat002,
}


def get_chat_history(user_id: str, conversation_id: str) -> BaseChatMessageHistory:
    key = (user_id, conversation_id, )
    if key not in store:
        store[key] = ChatMessageHistory()
    return store[key]


llm = Ollama(model='llama2')

prompt = ChatPromptTemplate.from_messages([
    ('system', 'You are a good assistant.'),
    MessagesPlaceholder(variable_name='chat_history'),
    ('user', '{input}'),
])

chain = prompt | llm

with_message_history = RunnableWithMessageHistory(
    chain,
    get_chat_history,
    input_messages_key="input",
    history_messages_key="chat_history",
    history_factory_config=[
        ConfigurableFieldSpec(
            id="user_id",
            annotation=str,
            name="User Id",
            description="Unique identifier for the user.",
            default="",
            is_shared=True,
        ),
        ConfigurableFieldSpec(
            id="conversation_id",
            annotation=str,
            name="Conversation Id",
            description="Unique identifier for the conversation.",
            default="",
            is_shared=True,
        ),
    ],
)

user_id = 'jeff'
conversation_id = 'chat002'
input_text = input('>>> ')
while input_text.lower() != 'bye':
    if input_text:
        response = with_message_history.with_config(
            configurable={
                'user_id': user_id,
                'conversation_id': conversation_id
            }
        ).invoke({'input': input_text})
        print(response)
    input_text = input('>>> ')
```

ä¸Šè¿°ç¯„ä¾‹çš„é‡é»åœ¨æ–¼æˆ‘å€‘åœ¨ `history_factory_config` åƒæ•¸ä¸­ä»¥ `ConfigurableFieldSpec()` æŒ‡å®š 2 å€‹è¨­å®šï¼Œåˆ†åˆ¥ç‚º `user_id` èˆ‡ `conversation_id` ï¼Œé€™ 2 å€‹è¨­å®šéƒ½æ˜¯å­—ä¸²å‹æ…‹ï¼š

```python
    history_factory_config=[
        ConfigurableFieldSpec(
            id="user_id",
            annotation=str,
            name="User Id",
            description="Unique identifier for the user.",
            default="",
            is_shared=True,
        ),
        ConfigurableFieldSpec(
            id="conversation_id",
            annotation=str,
            name="Conversation Id",
            description="Unique identifier for the conversation.",
            default="",
            is_shared=True,
        ),
    ],
```

å¦‚æ­¤å°±å¯ä»¥åœ¨å‘¼å« chain æ™‚ï¼Œå¸¶å…¥ `user_id` èˆ‡ `conversation_id` 2 å€‹è¨­å®šï¼Œåšåˆ°å‹•æ…‹è¼‰å…¥ä½¿ç”¨è€…è³‡æ–™çš„åŠŸèƒ½ï¼ˆç•¶ç„¶ `get_chat_history` å‡½å¼ä¹Ÿéœ€è¦åšç›¸å°æ‡‰çš„é‚è¼¯ä¿®æ”¹ï¼‰ï¼š

```python
with_message_history.with_config(
    configurable={
        'user_id': user_id,
        'conversation_id': conversation_id
    }
).invoke({'input': input_text})
```

è‡³æ­¤ï¼Œä½ å·²ç¶“ç†è§£å¦‚ä½•åšåˆ° 1 å€‹ chain æœå‹™å¤šå€‹ä½¿ç”¨è€…çš„åŸºæœ¬æ¶æ§‹ã€‚

## ç¸½çµ

å‹•æ…‹ä¿®æ”¹ chain çš„è¨­å®šæ˜¯ LangChain é‡è¦çš„ä¸€é …æŠ€è¡“ï¼Œå­¸æœƒæ­¤æŠ€è¡“æ‰èƒ½å¤ åšåˆ°å‹•æ…‹è¼‰å…¥ä½¿ç”¨è€…è¨­å®šã€è³‡æ–™ç­‰åŠŸèƒ½ã€‚