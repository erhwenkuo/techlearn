# ç”¨ LangServe æ¶å€‹ API ä¼ºæœå™¨

åŸæ–‡: [LangChain æ€éº¼ç©ï¼Ÿç”¨ LangServe æ¶å€‹ API ä¼ºæœå™¨å§ï¼](https://myapollo.com.tw/blog/langchain-langserve/)

æ‰“é€ èªè¨€æ¨¡å‹ç›¸é—œæ‡‰ç”¨ï¼Œç¸½ä¸èƒ½è¦æ±‚ä½¿ç”¨è€…éƒ½è¦é€éç¶²è·¯ä¸‹è¼‰/æ›´æ–°èªè¨€æ¨¡å‹ï¼Œç•¢ç«Ÿæ¯å€‹ä½¿ç”¨è€…çš„ç¡¬é«”æ¢ä»¶ä¸ä¸€ï¼ŒåŒ…å« CPU / GPU / è¨˜æ†¶é«” / ç¡¬ç¢Ÿå®¹é‡éƒ½å¯èƒ½éœ€è¦æ»¿è¶³ä¸€å®šæ¢ä»¶ï¼Œæ‰èƒ½å¤ åŸ·è¡Œèªè¨€æ¨¡å‹ï¼Œè€Œä¸”èªè¨€æ¨¡å‹ä½œç‚ºç«¶çˆ­åŠ›æ ¸å¿ƒï¼Œç„¡æ³•è¼•æ˜“é–‹æ”¾ä»–äººä¸‹è¼‰ä½¿ç”¨ï¼Œä¹Ÿæ˜¯ä¸€ç¨®å•†æ¥­è€ƒé‡ã€‚

ç¶œè§€è€Œè¨€ï¼Œå°‡èªè¨€æ¨¡å‹çš„åŠŸèƒ½é€é API æ–¹å¼é–‹æ”¾ï¼Œæ˜¯ 1 å€‹ç›¸ç•¶åˆç†çš„é¸é …ï¼Œä¸åƒ…ä½¿ç”¨è€…ä¸éœ€è¦æ»¿è¶³ç¡¬é«”æ¢ä»¶ï¼Œåˆå¯ä»¥ä¿è­·é–‹ç™¼è€…çš„å•†æ¥­ç«¶çˆ­åŠ›ã€‚

LangChain è‡ªç„¶ä¹Ÿæœ‰æä¾›å°‡èªè¨€æ¨¡å‹è½‰å› API æœå‹™çš„èƒ½åŠ›ï¼Œè©²åŠŸèƒ½/å¥—ä»¶ç¨±ç‚º LangServe ã€‚

æœ¬æ–‡å°‡æ•™å­¸å¦‚ä½•ä½¿ç”¨ [LangServe](https://python.langchain.com/docs/langserve/) å°‡èªè¨€æ¨¡å‹è½‰ç‚º API æœå‹™ï¼

å¦‚æœä¸æ¸…æ¥šçµ¦å¦‚ä½•é–‹å§‹è¨­ç½®ç›¸é—œç’°å¢ƒ, è«‹å…ˆç·´ç¿’ [ä½¿ç”¨ LangServe ç‚º LangChain æ‡‰ç”¨ç¨‹å¼å»ºç«‹ REST API](../langserve/build-rest-apis.md)ã€‚

## æœ¬æ–‡ç’°å¢ƒ

- Linux (Ubuntu)
- Python 3
- LangChain
- LangServe
- Ollama
- Faiss cpu
- pydantic 1.10.15

```bash
pip install langchain faiss-cpu "langserve[all]" pydantic==1.10.15
```

æœ¬æ–‡éœ€è¦ Ollama æŒ‡ä»¤èˆ‡ Meta å…¬å¸æä¾›çš„ `llama2` æ¨¡å‹(model)ï¼Œ`ollama` æŒ‡ä»¤è«‹è‡³ [Ollama å®˜æ–¹é é¢](https://ollama.ai/download)ä¸‹è¼‰å®‰è£ï¼Œå®‰è£å®Œæˆä¹‹å¾Œå³å¯åŸ·è¡ŒæŒ‡ä»¤å®‰è£ `llama2` æ¨¡å‹ï¼Œå…¶å®‰è£æŒ‡ä»¤å¦‚ä¸‹ï¼š

```bash
ollama pull llama2
```

!!! info
    p.s. åŸ·è¡Œä¸Šè¿°æ¨¡å‹éœ€è¦è‡³å°‘ 8GB è¨˜æ†¶é«”ï¼Œ 3.8G ç¡¬ç¢Ÿç©ºé–“

ç›®å‰ LangServe è‡ªå‹•ç”¢ç”Ÿ OpenAPI æ–‡ä»¶çš„åŠŸèƒ½éœ€è¦ä½¿ç”¨ `pydantic v1`, å› æ­¤æœ¬æ–‡ä½¿ç”¨ pydantic 1.10.15 ç‰ˆæœ¬ã€‚

## LangServe

LangServe æ˜¯ LangChain æ‰€æä¾›çš„ 1 å€‹å¥—ä»¶ï¼Œæä¾›èƒ½å°‡ Chain æˆ–è€… Runnable è®Šæˆ REST API çš„åŠŸèƒ½ï¼Œå®ƒæ‰€æä¾›çš„ REST API åŠŸèƒ½å…¶å¯¦æ˜¯ä½¿ç”¨ `FastAPI` èˆ‡ `pydantic` å¯¦ä½œï¼Œæ‰€ä»¥éœ€è¦æ‡‚å¾—å¦‚ä½•ä½¿ç”¨ `FastAPI` èˆ‡ `pydantic`, ä¸é LangChain ä¹Ÿæä¾›ä¸å°‘ç¯„ä¾‹å¯ä»¥åƒè€ƒï¼Œå¦‚æœ‰éœ€è¦å¯ä»¥æ‰¾çœ‹çœ‹æœ‰æ²’æœ‰ç”¨é€”ç›¸ä¼¼çš„ç¯„ä¾‹ï¼Œå†å¾è©²ç¯„ä¾‹é–‹å§‹è‘—æ‰‹ä¿®æ”¹ä¹Ÿè¡Œã€‚

ä»¥ä¸‹å°‡æ­ç¤ºå¦‚ä½•å°‡ LangChain æ‡‰ç”¨é€é LangServe è½‰ç‚º REST API æœå‹™ã€‚

## LangServe Server - add_routes()

LangServe ä½œç‚º LangChain æ¡†æ¶çš„ä¸€éƒ¨åˆ†ï¼Œå…¶å¯¦å®ƒä¹Ÿå°‡æ˜“ç”¨æ€§åšå¾—å¾ˆå¥½ï¼Œæœ€ç°¡å–®çš„æƒ…æ³ä¸‹åªè¦ä½¿ç”¨ `add_routes()` å‡½å¼ï¼Œå°±èƒ½è¼•é¬†å°‡ Chain æˆ– Runnable è½‰ç‚º REST API ã€‚

`add_routes()` çš„åŸºæœ¬ç”¨æ³•å¦‚ä¸‹ï¼š

```python
add_routes(
    app,     # FastAPI APP instance
    chain,   # chain or runnable
    path="<path>",  # any path you want to bind
)
```

ä»¥ä¸‹æ˜¯ 1 å€‹æœ€ç°¡å–®çš„ LangServe REST API æœå‹™ï¼Œè©²æœå‹™ä½¿ç”¨ LLaMa æ¨¡å‹æä¾›æœå‹™ï¼Œä¸¦å°‡ chain ç¶å®šåœ¨ `/llama2` ç¶²å€åº•ä¸‹ï¼Œ LangServe æœƒè² è²¬ç”¢ç”Ÿ `/llama2/<endpoint>` å„ç¨® APIï¼š

```python
from fastapi import FastAPI

from langchain_core.prompts import ChatPromptTemplate
from langchain_community.llms import Ollama

from langserve import add_routes


llm = Ollama(model='llama2')

prompt = ChatPromptTemplate.from_messages([
    ('system', 'You are a powerful assistant.'),
    ('user', '{input}'),
])


app = FastAPI(
    title="LangChain Server",
    version="1.0",
    description="A simple api server using Langchain's Runnable interfaces",
)

add_routes(
    app,
    prompt | llm,
    path="/llama2",
)


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="localhost", port=9000)
```

åŸ·è¡Œä¸Šè¿°ç¯„ä¾‹æˆåŠŸçš„è©±ï¼Œæœƒå‡ºç¾ä»¥ä¸‹ç•«é¢ï¼š

![](./assets/langserve-sever.png)

è©²ç•«é¢å‘Šè¨´æˆ‘å€‘ï¼Œ REST API ä¼ºæœå™¨æ­£åœ¨é‹ä½œï¼Œå¯ä»¥å­˜å– `http://localhost:9000/docs` å–å¾— API èªªæ˜æ–‡ä»¶ï¼Œé€™ä»½æ–‡ä»¶æ˜¯ç”± FastAPI æ•´åˆ OpenAPI æ ¼å¼æ‰€ç”¢ç”Ÿçš„ï¼Œæœ‰äº†é€™ä»½æ–‡ä»¶ï¼Œä»»ä½•äººéƒ½çŸ¥é“è¦å¦‚ä½•ç™¼é€è¦æ±‚(request)èˆ‡ REST API ä¼ºæœå™¨äº’å‹•ï¼Œ API èªªæ˜æ–‡ä»¶å¦‚ä¸‹æ‰€ç¤ºï¼š

![](./assets/langserve-docs.png)

å¾ç•«é¢ä¸­å¯ä»¥çœ‹åˆ° `add_routes()` å…¶å¯¦åšäº†ç›¸ç•¶å¤šäº‹ï¼ŒæœƒæŠŠå„å¼å„æ¨£çš„ API è‡ªå‹•åŠ åˆ° FastAPI çš„ REST API ä¼ºæœå™¨è£¡ï¼Œå…¶ä¸­åŒ…æ‹¬å¾ˆçœ¼ç†Ÿçš„ API å‘½å ï¼Œä¾‹å¦‚ï¼š

- POST `/<path>/invoke`
- POST `/<path>/batch`
- POST `/<path>/stream`

æ²’éŒ¯ï¼Œå…¶å¯¦é€™äº›å‘½åéƒ½ä¾†è‡ª `Runnable`, é€éçµ±ä¸€çš„ `Runnable` å”å®šï¼Œå°±é€£ REST API ä¹Ÿèƒ½é€éå¯¦ä½œ `Runnable` å”å®šè¼•é¬†æ•´åˆ `chain` æˆ–è€… `Runnable`! è€Œä¸”å°é–‹ç™¼è€…è€Œè¨€ï¼Œä¹Ÿèƒ½è¼•æ˜“ç†è§£ REST API çš„åŸºæœ¬æ¡†æ¶ã€‚

ä¸Šè¿°é™¤äº† `/docs` ä¹‹å¤–ï¼Œ LangServe ä¹Ÿæä¾› playground è®“æˆ‘å€‘å¯ä»¥é€éç¶²é ä»‹é¢èˆ‡æ‡‰ç”¨é€²è¡Œäº’å‹•ï¼Œå…¶ç¶²å€åœ¨ `http://localhost:9000/llama2/playground/` ï¼Œå…¶ç•«é¢å¦‚ä¸‹ï¼š

![](./assets/langserve-playground.png)

å°±é€£ `Configurable` ä¹Ÿéƒ½å¯ä»¥åœ¨ä»‹é¢ä¸Šè‡ªå‹•é¡¯ç¤ºï¼Œä¾‹å¦‚ä¸‹åˆ—ç¨‹å¼ç¢¼ï¼š

```python
from fastapi import FastAPI

from langchain_core.prompts import ChatPromptTemplate
from langchain_community.llms import Ollama
from langchain_core.runnables import ConfigurableField

from langserve import add_routes


llm = Ollama(model='llama2').configurable_fields(
    temperature=ConfigurableField(
        id="temperature",
        name="LLM Temperature",
        description="The temperature of the LLM",
    ),
)

prompt = ChatPromptTemplate.from_messages([
    ('system', 'You are a powerful assistant.'),
    ('user', '{input}'),
])

app = FastAPI(
    title="LangChain Server",
    version="1.0",
    description="A simple api server using Langchain's Runnable interfaces",
)

chain = prompt | llm

add_routes(
    app,
    chain,
    path="/llama2",
)


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="localhost", port=9000)

```

ä¸Šè¿°ç¯„ä¾‹åŸ·è¡ŒæˆåŠŸä¹‹å¾Œï¼Œå¯ä»¥åœ¨ Playground çœ‹åˆ°ç›¸å°æ‡‰çš„ configurable, è®“æˆ‘å€‘å¯ä»¥å‹•æ…‹è¨­å®šèªè¨€æ¨¡å‹çš„ temperature:

![](./assets/langserve-configurable.png)

LangServe å°±æ˜¯å¦‚æ­¤æ˜“ç”¨ï¼

## LangServe Client - RemoteRunnable

LangServe é™¤äº† REST API server ä¹‹å¤–ï¼Œä¹Ÿæä¾› client çš„å¯¦ä½œï¼Œæˆ‘å€‘å¯ä»¥é€é `RemoteRunnable` è¼•é¬†æŠŠ LangServe çš„ REST API sever ç›´æ¥è½‰æˆ Runnable, æ•´é«”ä½¿ç”¨ä¸Šå°±è·Ÿä»¥å‰çš„ç¯„ä¾‹ä¸€æ¨£ï¼š

```python
from langchain.prompts import ChatPromptTemplate
from langserve import RemoteRunnable

llama2 = RemoteRunnable("http://localhost:9000/llama2/")

response = llama2.invoke({"input": "Hi there"})

print(response)
```

æ˜¯å¦ç›¸ç•¶æ–¹ä¾¿ç›´è¦ºï¼Ÿæ›´è©³ç´°ç”¨æ³•å¯ä»¥åƒè€ƒ [RemoteRunnable](https://github.com/langchain-ai/langserve/blob/b6ec1e86bd31950dba7118ab0f23f987c4906765/langserve/client.py#L258) çš„ç¨‹å¼ç¢¼ã€‚

## LangServe å¦‚ä½•åšä½¿ç”¨è€…èªè­‰ï¼Ÿ

é è¨­æƒ…æ³ä¸‹ LangServe æ˜¯æ²’æœ‰ä»»ä½•ä½¿ç”¨è€…èªè­‰æ©Ÿåˆ¶çš„ï¼Œä½†æ˜¯å®ƒä¾ç„¶æä¾›ä¿®æ”¹çš„å½ˆæ€§ï¼Œè®“æˆ‘å€‘å¯ä»¥è¼•æ˜“çš„åŠ å…¥èªè­‰æ©Ÿåˆ¶æˆ–å…¶ä»–æµç¨‹ï¼Œé€™äº›å½ˆæ€§å…¶å¯¦å¤šåŠèˆ‡ LangChain æ€éº¼ç©ï¼Ÿ å‹•æ…‹ä¿®æ”¹é‹ä½œä¸­çš„ [Chain è¨­å®š / configure chain internals at runtime æ–‡ç« ](configure-chain-at-runtime.md)å…§æåˆ°çš„åŠŸèƒ½ç›¸é—œã€‚

!!! info
    p.s. åƒè¬åˆ¥æŠŠæ²’åšä»»ä½•èªè­‰æ©Ÿåˆ¶çš„ LangServe REST API server æ¶è¨­åœ¨é–‹æ”¾å…¬çœ¾å­˜å–çš„ç¶²è·¯ä¸Š

ä»¥ä¸‹æ•™å­¸æ¨¡æ“¬ä»¥ API key ä½œç‚ºèªè­‰æ©Ÿåˆ¶ï¼Œå°‡èªè­‰é˜²è­·æ©Ÿåˆ¶åŠ å…¥ LangServe çš„ API ã€‚

é—œæ–¼å¦‚ä½•åŠ ä¸Šèªè­‰æ©Ÿåˆ¶ï¼Œå…¶å¯¦ä¹Ÿå¯ä»¥åƒè€ƒ LangServe æ‰€æä¾›çš„ç¯„ä¾‹ã€‚æœ¬æ–‡ä½œç‚ºæ•™å­¸æ–‡ç« ï¼Œå› æ­¤åœ¨ç¨‹å¼ç¢¼ç¯„ä¾‹åŠ›æ±‚ç°¡å–®ã€å®¹æ˜“ç†è§£ï¼Œæ‰€ä»¥éƒ¨åˆ†å…§å®¹ä»¥ç°¡åŒ–æ–¹å¼å‘ˆç¾ï¼Œå¯¦éš›ä»æ‡‰ä»¥å„è‡ªéœ€æ±‚é€²è¡Œä¿®æ”¹ã€‚

## å¾ç°¡å–®çš„ FastAPI header èªè­‰é–‹å§‹

é¦–å…ˆï¼Œä»¥ä¸‹æ˜¯ 1 å€‹ç°¡å–®çš„ FastAPI server, è©² server åƒ…æœ‰ 1 å€‹ API ã€‚

è©² API æœƒå¾ HTTP headers ä¸­å–çš„ Authorization æ¨™é ­çš„å€¼ï¼Œä¸¦ä¸”æª¢æŸ¥å…¶å€¼æ˜¯å¦ç‚ºåˆæ³•çš„ API é‡‘é‘°ä½œç‚ºèªè­‰æ©Ÿåˆ¶:

```python
from fastapi import FastAPI, Header, HTTPException
from typing import Optional


app = FastAPI(
    title="LangChain Server",
    version="1.0",
    description="A simple api server using Langchain's Runnable interfaces",
)


def get_user_from_api_key(api_key: str) -> Optional[dict]:
    if api_key == "valid_api_key":
        return {"user_id": "useridx", "user_name": "John"}
    return None


@app.get("/user")
async def get_user(authorization: Optional[str] = Header(None)):
    if authorization is None:
        raise HTTPException(status_code=401, detail="Authorization header missing")

    # assuming the token is provided as a Bearer token
    api_key = authorization.split(" ")[1] if len(authorization.split(" ")) == 2 else None
    if api_key is None:
        raise HTTPException(status_code=401, detail="Invalid Authorization header format")

    user_data = get_user_from_api_key(api_key)
    if user_data is None:
        raise HTTPException(status_code=403, detail="Invalid API Key")

    return {"user_name": user_data["user_name"]}


if __name__ == '__main__':
    import uvicorn

    uvicorn.run(app, host="localhost", port=9000)
```

ä¸Šè¿° API server å¯ä»¥ç”¨ä»¥ä¸‹ `curl` æŒ‡ä»¤é€²è¡Œæ¸¬è©¦ï¼Œé‹ä½œæ­£å¸¸çš„è©±ï¼Œå°‡æœƒå›æ‡‰ user_name èˆ‡ user_id :

```bash
$ curl -H "Authorization: Bearer valid_api_key" http://127.0.0.1:9000/user

{"user_id": "useridx", "user_name":"John"}
```

æ¥ä¸‹ä¾†ï¼Œæˆ‘å€‘å¾ä¸Šè¿°ç¯„ä¾‹é–‹å§‹ï¼Œç‚º LangServe åŠ å…¥ API èªè­‰ã€‚

é¦–å…ˆï¼Œæˆ‘å€‘å·²ç¶“çŸ¥é“å¦‚ä½•æª¢æŸ¥ API key ä¸¦å–å¾—ç›¸å°æ‡‰çš„ä½¿ç”¨è€…è³‡æ–™äº†ï¼Œæ¥ä¸‹ä¾†è©¦åœ–æŠŠé€™å€‹éç¨‹åŠ é€² LangServe, é‡é»åœ¨æ–¼è¦è®“ LangServe ä¹Ÿèƒ½å¤ æª¢æŸ¥ `Authorization` æ¨™é ­ä»¥åŠå®ƒçš„å€¼ï¼Œå¦‚æœå®ƒæœ‰ä»»ä½•å•é¡Œå°±è©²åƒå‰è¿°ç¯„ä¾‹ä¸€æ¨£æ‹‹å‡º HTTPException ä¾‹å¤–éŒ¯èª¤ã€‚

è€Œ LangServe çš„ `add_routes()` å‡½å¼æœ‰æä¾› 1 å€‹åƒæ•¸ç¨±ç‚º **dependencies**, å¯ä»¥è®“æˆ‘å€‘ç”¨ FastAPI çš„ dependency injection åŠŸèƒ½ï¼ŒæŠŠèªè­‰çš„é‚è¼¯åŠ åˆ° API ä¹‹ä¸­ï¼Œæˆ‘å€‘è¦åšçš„å°±æ˜¯åƒè€ƒ FastAPI çš„ç¯„ä¾‹ï¼Œå°‡å‰è¿°ç¯„ä¾‹çš„èªè­‰æµç¨‹åŠ åˆ° `add_routes()` çš„ `dependencies` åƒæ•¸ä¸­ï¼Œä¾‹å¦‚ï¼š

```python
from fastapi import Depends

add_routes(
    app,
    chain,
    path="/llama2",
    dependencies=[
        Depends(verify_api_key),
    ],
)
```

ä»¥ä¸‹æ˜¯ LangServe æ”¹å®Œä¹‹å¾Œçš„çµæœï¼š

```python
from fastapi import Depends, FastAPI, Header, HTTPException
from typing import Optional


from langchain_core.prompts import ChatPromptTemplate
from langchain_community.llms import Ollama
from langchain_core.runnables import ConfigurableField

from langserve import add_routes


llm = Ollama(model='llama2').configurable_fields(
    temperature=ConfigurableField(
        id="temperature",
        name="LLM Temperature",
        description="The temperature of the LLM",
    ),
)

prompt = ChatPromptTemplate.from_messages([
    ('system', 'You are a powerful assistant.'),
    ('user', '{input}'),
])

app = FastAPI(
    title="LangChain Server",
    version="1.0",
    description="A simple api server using Langchain's Runnable interfaces",
)

chain = prompt | llm

# æ§‹å»ºä¸€å€‹ function ä¾†æª¢æŸ¥ api_key
async def verify_api_key(authorization: Optional[str] = Header(None)):
    if authorization is None:
        raise HTTPException(status_code=401, detail="Authorization header missing")

    # assuming the token is provided as a Bearer token
    api_key = authorization.split(" ")[1] if len(authorization.split(" ")) == 2 else None
    if api_key is None:
        raise HTTPException(status_code=401, detail="Invalid Authorization header format")

    if api_key != "valid_api_key":
        raise HTTPException(status_code=403, detail="Invalid API Key")

    return {"user_name": "John"}


# ä½¿ç”¨ dependencies ä¾†é©—è­‰
add_routes(
    app,
    chain,
    path="/llama2",
    dependencies=[Depends(verify_api_key)],
)


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="localhost", port=9000)
```

ä¸Šè¿°ç¯„ä¾‹åŸ·è¡Œä¹‹å¾Œï¼Œåªè¦è©¦è‘—å­˜å– `http://localhost:9000/llama2/playground/` å°±æœƒå‡ºç¾ä»¥ä¸‹å›æ‡‰ï¼š

```bash
{"detail":"Authorization header missing"}
```

æ˜¯çš„ï¼Œå®ƒå‘Šè¨´æˆ‘å€‘éœ€è¦ `Authorization` æ¨™é ­æ‰è¡Œï¼Œå¦‚æœæƒ³æ¸¬è©¦é€šéé©—è­‰çš„è©±ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹ `curl` æŒ‡ä»¤é€²è¡Œæ¸¬è©¦ï¼š

```bash
curl -H 'Authorization: Bearer valid_api_key' \
     -d '{"input": {"input": "hi"}}' \
     http://localhost:9000/llama2/invoke
```

ä»¥ä¸Šæ˜¯å¦‚ä½•åŠ å…¥èªè­‰çš„å¤§æ¦‚æ­¥é©Ÿã€‚

å¦‚æœè¦åšåˆ°æ›´ç²¾ç´°çš„æ§åˆ¶ï¼Œå¯ä»¥é€²ä¸€æ­¥ä¿®æ”¹ `dependencies` çš„éƒ¨åˆ†ï¼Œä¾‹å¦‚åŠ å…¥å“ªäº›è·¯å¾‘éœ€è¦èªè­‰ã€å“ªäº›æ–¹æ³•éœ€è¦èªè­‰ç­‰ç­‰ã€‚

## å¦‚ä½•å–å¾—ä½¿ç”¨è€… Id

æˆ‘å€‘å·²ç¶“çŸ¥é“åŠ å…¥èªè­‰æ©Ÿåˆ¶çš„æµç¨‹ã€‚

ä¸éå°æ–¼è¨±å¤šæ‡‰ç”¨ä¾†èªªï¼Œ chain éœ€è¦èƒ½å¤ è™•ç†ä¸åŒä½¿ç”¨è€…çš„è¨­å®šã€è³‡æ–™ï¼Œä¾‹å¦‚å‰ä¸€ç¯‡æ•™å­¸æ–‡æ‰€æåˆ°çš„ï¼Œé€é `user_id` è¼‰å…¥ä¸åŒä½¿ç”¨è€…çš„å°è©±ç´€éŒ„ï¼Œå¾è€Œä½¿å¾— chain å…·æœ‰è¨˜æ†¶ä¸Šä¸‹æ–‡çš„èƒ½åŠ›ã€‚

ä»¥ä¸‹æ˜¯èƒ½å¤ é€é API key å–å¾— user_id é€²è€Œå–å¾—å°è©±ç´€éŒ„(chat history)çš„ LangServe ç¨‹å¼ç¢¼ç¯„ä¾‹ï¼Œçœ¼å°–çš„äººæ‡‰è©²å¯ä»¥ç™¼ç¾åŠæ•¸ä»¥ä¸Šçš„å…§å®¹èˆ‡å…ˆå‰æéçš„ç¯„ä¾‹ï¼ˆåŒ…å«å‰ä¸€ç¯‡æ•™å­¸æ–‡ç« ï¼‰éƒ½ç›¸åŒï¼Œä½†å¤šäº† `per_request_config_modifier()` å‡½å¼èˆ‡ `add_routes()` è£¡çš„ `per_req_config_modifier` åƒæ•¸ï¼Œç¨å¾Œèªªæ˜ä»¥ä¸‹ç¯„ä¾‹ä¸­çš„é‡é»éƒ¨åˆ†ï¼š

```python
from typing import Any, Dict, Optional

from fastapi import Depends, FastAPI, Header, HTTPException, Request
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_community.llms import Ollama
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import ConfigurableFieldSpec
from langchain_core.runnables.history import RunnableWithMessageHistory
from langserve import add_routes

chat001 = ChatMessageHistory()
chat001.add_user_message('My name is Amo.')

store = {
    'amo': chat001,
}

def get_chat_history(user_id: str) -> BaseChatMessageHistory:
    if user_id not in store:
        store[user_id] = ChatMessageHistory()
    return store[user_id]


llm = Ollama(model='llama2')

prompt = ChatPromptTemplate.from_messages([
    ('system', 'You are a good assistant.'),
    MessagesPlaceholder(variable_name='chat_history'),
    ('user', '{input}'),
])

chain = prompt | llm

with_message_history = RunnableWithMessageHistory(
    chain,
    get_chat_history,
    input_messages_key="input",
    history_messages_key="chat_history",
    history_factory_config=[
        ConfigurableFieldSpec(
            id="user_id",
            annotation=str,
            name="User Id",
            description="Unique identifier for the user.",
            default="",
            is_shared=True,
        ),
    ],
)

app = FastAPI(
    title="LangChain Server",
    version="1.0",
    description="A simple api server using Langchain's Runnable interfaces",
)

def per_request_config_modifier(
    config: Dict[str, Any], request: Request
) -> Dict[str, Any]:
    """Update the config"""
    config = config.copy()
    configurable = config.get("configurable", {})
    user_id = getattr(request.state, 'user_id', None)

    if user_id is None:
        raise HTTPException(
            status_code=400,
            detail="No user id found. Please set a state named 'user_id'.",
        )

    configurable["user_id"] = user_id
    config["configurable"] = configurable

    return config


async def verify_api_key(request: Request, authorization: Optional[str] = Header(None)):
    if authorization is None:
        raise HTTPException(status_code=401, detail="Authorization header missing")

    # assuming the token is provided as a Bearer token
    api_key = authorization.split(" ")[1] if len(authorization.split(" ")) == 2 else None

    if api_key is None:
        raise HTTPException(status_code=401, detail="Invalid Authorization header format")

    if api_key != "valid_api_key":
        raise HTTPException(status_code=403, detail="Invalid API Key")

    request.state.user_id = 'amo'  # You can modify the logic here


add_routes(
    app,
    with_message_history,
    per_req_config_modifier=per_request_config_modifier,
    path="/llama2",
    dependencies=[Depends(verify_api_key)],
)


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="localhost", port=9000)
```

ä¸Šè¿°ç¯„ä¾‹å¯ä»¥ç”¨ä»¥ä¸‹æŒ‡ä»¤æ¸¬è©¦ï¼Œæˆ‘å€‘è©¦åœ–é€é API è©¢å•èªè¨€æ¨¡å‹é—œæ–¼åå­—çš„å•é¡Œï¼Œæ­£å¸¸çš„è©±ï¼Œå®ƒå¯ä»¥å¾å°è©±ç´€éŒ„ä¸­æ‰¾å‡ºåå­—ä¸¦å›ç­”æˆ‘å€‘ï¼š

```bash
$ curl -H 'Authorization: Bearer valid_api_key' \
    -d '{"input": {"input": "what is my name?"}}' \
    http://localhost:9000/llama2/invoke
```

å¾å›æ‡‰ä¸­å¯ä»¥çœ‹åˆ°èªè¨€æ¨¡å‹å–å¾—å°è©±ç´€éŒ„ï¼Œä¸¦æ­£ç¢ºå›ç­”æˆ‘å€‘çš„å•é¡Œï¼š

```bash
{"output":"Of course, Amo! Your name is Amo. ğŸ˜Š","callback_events":[],"metadata":{"run_id":"059eaa9e-604f-457f-bd4b-f195a6d90fc7"}}
```

æ¥è‘—è§£é‡‹ç¨‹å¼ç¢¼ä¸­çš„é‡é»éƒ¨åˆ†ã€‚

é¦–å…ˆï¼Œ verify_api_key() å‡½å¼ä¸­å¤šäº† 1 è¡Œï¼š

```python
request.state.user_id = 'amo'
```

é€™ 1 è¡Œå¯ä»¥æŠŠ `user_id` å¯«å…¥ FastAPI çš„ Request çš„ `state` ä¸­ï¼Œå¦‚æ­¤ä¸€ä¾†å¾ŒçºŒå­˜å– Request çš„ç‰©ä»¶éƒ½èƒ½å¤ è®€å–åˆ° `user_id` çš„å€¼ï¼Œå¦‚æœæ˜¯åœ¨æ›´å¯¦éš›çš„æƒ…æ³ï¼Œæ­¤è™•æœƒè®€å–è³‡æ–™åº«å–å¾— `user_id`ã€‚

æ¥è‘—ï¼Œæˆ‘å€‘è«‡è«‡ `add_routes()` çš„ `per_req_config_modifier` åƒæ•¸ã€‚

`per_req_config_modifier` åƒæ•¸å¯ä»¥è®“æˆ‘å€‘è¨­å®š 1 å€‹å‡½å¼(function), æ¯ 1 æ¬¡æœ‰ request é€²åˆ° chain ä¹‹å‰ï¼Œéƒ½å¯ä»¥åŸ·è¡Œæˆ‘å€‘è¨­å®šçš„å‡½å¼ï¼Œå° chain çš„è¨­å®šé€²è¡Œä¿®æ”¹ï¼Œä¾‹å¦‚å–å¾—å°è©±ç´€éŒ„(chat history)éœ€è¦çš„ `user_id` å°±ç›¸ç•¶é©åˆä½¿ç”¨ã€‚

`per_req_config_modifier` åƒæ•¸çš„å‡½å¼æ¥å— 2 å€‹åƒæ•¸ï¼Œå›å‚³ 1 å€‹ dictionary ï¼š

```python
def per_request_config_modifier(
    config: Dict[str, Any], request: Request
) -> Dict[str, Any]:
    ...
```

å‚³å…¥ `config` å³æ˜¯å‘¼å« `<Runnable>.with_config(config)` ä¹‹å‰çš„ config å€¼ï¼Œè€Œ `request` å‰‡æ˜¯ FastAPI çš„ Request, å› æ­¤æˆ‘å€‘å¯ä»¥å¾ `request.state` ä¸­å–å¾— `user_id`, ä¸¦æŠŠè©² `user_id` å¯«å…¥ `config` ä¸­ï¼Œå¦‚æ­¤ä¸€ä¾†ï¼Œ chain å°±èƒ½å¤ å¾—åˆ° `user_id` ä¸¦è¼‰å…¥ç›¸å°æ‡‰çš„å°è©±ç´€éŒ„ï¼š

```python
def per_request_config_modifier(
    config: Dict[str, Any], request: Request
) -> Dict[str, Any]:
    """Update the config"""
    config = config.copy()
    configurable = config.get("configurable", {})
    user_id = getattr(request.state, 'user_id', None)
    if user_id is None:
        raise HTTPException(
            status_code=400,
            detail="No user id found. Please set a state named 'user_id'.",
        )
    configurable["user_id"] = user_id
    config["configurable"] = configurable
    return config
```

ä¸Šè¿°å‡½å¼åšçš„äº‹æƒ…ï¼Œç°¡åŒ–ä¹‹å¾Œå¦‚ä¸‹ï¼š

```python
config = {'configurable': {}}
config['configurable']['user_id'] = request.state.user_id
```

æ›´ç°¡å–®ä¸€é»çš„è§£é‡‹æ˜¯ï¼Œ`per_request_config_modifier` å¯ä»¥è®“æˆ‘å€‘åœ¨ `<Runnable>.with_config(config)` åŸ·è¡Œä¹‹å‰ï¼Œå…ˆæ””æˆªäº† `config` çš„å€¼ä¸¦ä¸”ä¿®æ”¹å®ƒã€‚

é€™å°±æ˜¯å¦‚ä½•è®€å–/è¨­å®š `user_id` çš„æ–¹æ³•ï¼Œä¸éæ­¤ç¯„ä¾‹ä¸¦æ²’æœ‰æåˆ°å¦‚ä½•å„²å­˜å°è©±ç´€éŒ„ï¼Œä¸€èˆ¬ä¾†èªªé‚„æœƒå°‡å°è©±ç´€éŒ„å„²å­˜åˆ°å¾Œç«¯è³‡æ–™åº«ä¸­ï¼Œå¦‚æ­¤ä¸€ä¾†èªè¨€æ¨¡å‹æ‰æœƒæŒçºŒæœ‰æ–°çš„å°è©±ç´€éŒ„å¯ä»¥è¼‰å…¥ï¼Œ LangServe å…¶å¯¦ä¹Ÿå·²ç¶“æ•´åˆå¤šå€‹è§£æ±ºæ–¹æ¡ˆï¼Œä¾‹å¦‚å°‡å°è©±ç´€éŒ„å­˜åœ¨æª”æ¡ˆç³»çµ±ï¼Œå¯ä»¥åƒè€ƒæ­¤ [GitHub é€£çµ](https://github.com/langchain-ai/langserve/blob/main/examples/chat_with_persistence_and_user/server.py)ã€‚

## å®¢è£½ LangServe API

é™¤äº† `add_routes()` æ–¹æ³•ä¹‹å¤–ï¼Œ LangServe ä¹Ÿæä¾›å®¢è£½ API çš„å½ˆæ€§ï¼Œè©²æ–¹æ³•ä¸»è¦ä½¿ç”¨ LangServe çš„ `APIHandler` ï¼Œä½¿ç”¨æ–¹æ³•è·Ÿ `add_routes()` ååˆ†é›·åŒï¼Œåœ¨æ­¤ä¸å¤šåŠ è´…è¿°ã€‚

ç›´æ¥çœ‹æˆ‘å€‘å°‡ä¸Šä¸€å€‹ç¯„ä¾‹æ”¹æˆä½¿ç”¨ `APIHandler` çš„çµæœï¼š

```python
from typing import Any, Dict, Optional

from fastapi import Depends, FastAPI, Header, HTTPException, Request, Response
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_community.llms import Ollama
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import ConfigurableFieldSpec
from langchain_core.runnables.history import RunnableWithMessageHistory
from langserve import APIHandler

chat001 = ChatMessageHistory()
chat001.add_user_message('My name is Amo.')

store = {
    'amo': chat001,
}

def get_chat_history(user_id: str) -> BaseChatMessageHistory:
    if user_id not in store:
        store[user_id] = ChatMessageHistory()
    return store[user_id]


llm = Ollama(model='llama2')

prompt = ChatPromptTemplate.from_messages([
    ('system', 'You are a good assistant.'),
    MessagesPlaceholder(variable_name='chat_history'),
    ('user', '{input}'),
])

chain = prompt | llm

with_message_history = RunnableWithMessageHistory(
    chain,
    get_chat_history,
    input_messages_key="input",
    history_messages_key="chat_history",
    history_factory_config=[
        ConfigurableFieldSpec(
            id="user_id",
            annotation=str,
            name="User Id",
            description="Unique identifier for the user.",
            default="",
            is_shared=True,
        ),
    ],
)


app = FastAPI(
    title="LangChain Server",
    version="1.0",
    description="A simple api server using Langchain's Runnable interfaces",
)

def per_request_config_modifier(
    config: Dict[str, Any], request: Request
) -> Dict[str, Any]:
    """Update the config"""
    config = config.copy()
    configurable = config.get("configurable", {})
    user_id = getattr(request.state, 'user_id', None)
    if user_id is None:
        raise HTTPException(
            status_code=400,
            detail="No user id found. Please set a state named 'user_id'.",
        )
    configurable["user_id"] = user_id
    config["configurable"] = configurable
    return config


async def verify_api_key(request: Request, authorization: Optional[str] = Header(None)):
    if authorization is None:
        raise HTTPException(status_code=401, detail="Authorization header missing")

    # assuming the token is provided as a Bearer token
    api_key = authorization.split(" ")[1] if len(authorization.split(" ")) == 2 else None
    if api_key is None:
        raise HTTPException(status_code=401, detail="Invalid Authorization header format")

    if api_key != "valid_api_key":
        raise HTTPException(status_code=403, detail="Invalid API Key")

    request.state.user_id = 'amo'  # You can modify the logic here


api_handler = APIHandler(
    with_message_history,
    per_req_config_modifier=per_request_config_modifier,
    path="/my_runnable",
)


@app.post("/my_runnable/invoke", dependencies=[Depends(verify_api_key)])
async def invoke(request: Request) -> Response:
    return await api_handler.invoke(request)


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="localhost", port=9000)
```

æœ€é‡è¦çš„éƒ¨åˆ†åœ¨æ–¼ï¼š

```python
api_handler = APIHandler(
    with_message_history,
    per_req_config_modifier=per_request_config_modifier,
    path="/my_runnable",
)

@app.post("/my_runnable/invoke", dependencies=[Depends(verify_api_key)])
async def invoke(request: Request) -> Response:
    return await api_handler.invoke(request)
```

æˆ‘å€‘å°‡ `add_routes()` æ›æˆä½¿ç”¨ `APIHandler()`, `APIHandler()` åŒæ¨£æ¥å— `Runnable` èˆ‡ `per_req_config_modifier`, `path` ç­‰åƒæ•¸ï¼Œé€™äº›åƒæ•¸èˆ‡ `add_routes()` çš„åŒååƒæ•¸ä½œç”¨ç›¸åŒï¼š

```python
api_handler = APIHandler(
    with_message_history,
    per_req_config_modifier=per_request_config_modifier,
    path="/my_runnable",
)
```

ç›¸è¼ƒæ–¼ `add_routes()` ç›´æ¥å°‡ API handlers bind åœ¨ FastAPI çš„ app instance ä¸Šï¼Œ `APIHander()` åƒ…å›å‚³ API handler, è©² handler å…·æœ‰ `invoke()`, `batch()` ç­‰æ–¹æ³•å¯ä»¥ä½¿ç”¨ï¼Œæˆ‘å€‘è‡ªè¡Œå¯ä»¥æ±ºå®šå¦‚ä½•ä½¿ç”¨ã€‚

æœ€å¾Œï¼Œæˆ‘å€‘åœ¨ FastAPI çš„ `app` instance ä¸Šæ–°å¢ 1 å€‹ endpoint `/my_runnable/invoke` ï¼Œè©² API éœ€è¦ä½¿ç”¨ **Authorization** æ¨™é ­é€²è¡Œèªè­‰ï¼Œå¦‚èªè­‰é€šéå°±æœƒæŠŠ FastAPI request æ”¾å…¥ `api_handler.invoke(request)` åŸ·è¡Œï¼Œå›å‚³å…¶å›æ‡‰çµæœã€‚

ä»¥ä¸Šå°±æ˜¯ä½¿ç”¨ `APIHandler()` å®¢è£½ LangServe API çš„æ–¹å¼ä»‹ç´¹ã€‚

## ç¸½çµ

LangServe å…¶å¯¦æ˜¯ç›¸ç•¶å¥½ç”¨çš„å¥—ä»¶ï¼Œä¸éå®˜æ–¹æ–‡ä»¶å…¶å¯¦å¤šåŠä»¥æä¾›ç¯„ä¾‹ç‚ºä¸»ï¼Œå¦‚æœæ˜¯æ–°æ‰‹è‚¯å®šæœƒé›£ä»¥è¿…é€Ÿç†è§£ï¼Œç©¶å…¶åŸå› æ˜¯æˆ‘å€‘éœ€è¦å° LCEL èˆ‡ FastAPI æœ‰ç›¸ç•¶ç¨‹åº¦çš„äº†è§£æ‰è¡Œï¼Œä¸€æ—¦å° LCEL èˆ‡ FastAPI ä¸Šæ‰‹ä¹‹å¾Œï¼Œå°±ä¹Ÿèƒ½å¤ ç†è§£ LangServe åˆ°åº•è¦å¦‚ä½•ä½¿ç”¨äº†ï¼