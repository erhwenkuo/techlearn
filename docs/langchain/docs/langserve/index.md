# ğŸ¦œï¸ğŸ“ LangServe

## æ¦‚è¿°

LangServe å”åŠ©é–‹ç™¼äººå“¡å°‡ LangChain çš„ [runnables èˆ‡ chains](https://python.langchain.com/docs/expression_language/) éƒ¨ç½²ç‚º REST APIã€‚

LangServe æ•´åˆäº† [FastAPI](https://fastapi.tiangolo.com/)ï¼Œä¸¦ä½¿ç”¨ [pydantic](https://docs.pydantic.dev/latest/) é€²è¡Œè³‡æ–™é©—è­‰ã€‚

æ­¤å¤–ï¼Œå®ƒé‚„æä¾›äº†ä¸€å€‹å¯ç”¨æ–¼å‘¼å«éƒ¨ç½²åœ¨ä¼ºæœå™¨ä¸Šçš„å¯é‹è¡Œç¨‹å¼çš„ç”¨æˆ¶ç«¯ã€‚ [LangChain.js](https://js.langchain.com/docs/ecosystem/langserve) ä¸­æä¾›äº† JavaScript ç”¨æˆ¶ç«¯ã€‚

## åŠŸèƒ½

- è¼¸å…¥å’Œè¼¸å‡º schema æœƒè‡ªå‹•å¾ LangChain ç‰©ä»¶æ¨æ–·å‡ºä¾†ï¼Œä¸¦åœ¨æ¯å€‹ API å‘¼å«ä¸Šå¼·åˆ¶åŸ·è¡Œï¼Œä¸¦æä¾›è±å¯Œçš„éŒ¯èª¤è¨Šæ¯
- åŒ…å« JSONSchema å’Œ Swagger çš„ API æ–‡ä»¶é é¢
- é«˜æ•ˆçš„ `/invoke/`ã€`/batch/` å’Œ `/stream/` ç«¯é»ï¼Œæ”¯æ´å–®ä¸€ä¼ºæœå™¨ä¸Šçš„è¨±å¤šä¸¦ç™¼è«‹æ±‚
- `/stream_log/` ç«¯é»ï¼Œç”¨æ–¼ä¸²æµå‚³è¼¸éˆ/ä»£ç†ä¸­çš„æ‰€æœ‰ï¼ˆæˆ–éƒ¨åˆ†ï¼‰ä¸­é–“æ­¥é©Ÿ
- è‡ª 0.0.40 èµ·æ–°å¢ï¼Œæ”¯æ´ `astream_events`ï¼Œä»¥ä¾¿æ›´è¼•é¬†åœ°é€²è¡Œä¸²æµå‚³è¼¸ï¼Œè€Œç„¡éœ€è§£æ `stream_log` çš„è¼¸å‡ºã€‚
- Playground é é¢ä½æ–¼ `/playground/` ï¼Œå…·æœ‰ä¸²æµè¼¸å‡ºå’Œä¸­é–“æ­¥é©Ÿ
- å…§å»ºï¼ˆå¯é¸ï¼‰è¿½è¹¤ LangSmithï¼Œåªéœ€æ–°å¢æ‚¨çš„ API é‡‘é‘°
- ä½¿ç”¨ç¶“éå¯¦æˆ°è€ƒé©—çš„é–‹æº Python åº«ï¼ˆä¾‹å¦‚ FastAPIã€Pydanticã€uvloop å’Œ asyncioï¼‰æ§‹å»ºçš„ã€‚
- å¯ä½¿ç”¨[LangChain.js](https://js.langchain.com/docs/ecosystem/langserve)å®¢æˆ¶ç«¯ SDK å‘¼å« LangServe ä¼ºæœå™¨ï¼Œå°±åƒå®ƒæ˜¯åœ¨æœ¬åœ°é‹è¡Œçš„ Runnable ä¸€æ¨£ï¼ˆæˆ–ç›´æ¥å‘¼å« HTTP APIï¼‰

## é™åˆ¶

- å°æ–¼æºè‡ªä¼ºæœå™¨çš„äº‹ä»¶å°šä¸æ”¯æ´å®¢æˆ¶ç«¯å›èª¿
- ä½¿ç”¨ Pydantic V2 æ™‚ä¸æœƒç”¢ç”Ÿ OpenAPI æ–‡ä»¶ã€‚ Fast API ä¸æ”¯æ´æ··åˆ pydantic v1 å’Œ v2 å‘½åç©ºé–“ã€‚æœ‰é—œæ›´å¤šè©³ç´°ä¿¡æ¯ï¼Œè«‹åƒé–±ä¸‹é¢çš„éƒ¨åˆ†ã€‚

## å®‰è£

å°æ–¼å®¢æˆ¶ç«¯å’Œä¼ºæœå™¨:

```bash
pip install "langserve[all]"
```

æˆ– `pip install "langserve[client]"` å°æ–¼å®¢æˆ¶ç«¯ç¨‹å¼ç¢¼ï¼Œ`pip install "langserve[server]"` å°æ–¼ä¼ºæœå™¨ç¨‹å¼ç¢¼ã€‚

## LangChain CLI ğŸ› ï¸

ä½¿ç”¨ LangChain CLI å¿«é€Ÿ bootstrap `LangServe` projectã€‚

è¦ä½¿ç”¨ langchain CLIï¼Œè«‹ç¢ºä¿æ‚¨å®‰è£äº†æœ€æ–°ç‰ˆæœ¬çš„ `langchain-cli`ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ `pip install -U langchain-cli` å®‰è£å®ƒã€‚

## è¨­å®šå°ˆæ¡ˆ

æ³¨æ„ï¼šLangServe ä½¿ç”¨ [poetry](https://python-poetry.org/) é€²è¡Œä¾è³´ç®¡ç†ã€‚è«‹é—œæ³¨ [poetry](https://python-poetry.org/) æ–‡æª”ä»¥äº†è§£æ›´å¤šè³‡è¨Šã€‚

**1. Create new app using langchain cli command**

```bash
langchain app new my-app
```

**2. Define the runnable in add_routes. Go to `server.py` and edit**

```python
add_routes(app. NotImplemented)
```

**3. Use poetry to add 3rd party packages (e.g., `langchain-openai` etc).**

```bash
poetry add [package-name] // e.g `poetry add langchain-openai`
```

**4. Set up relevant env variables.**

```bash
export OPENAI_API_KEY="sk-..."
```

**5. Serve your app**

```bash
poetry run langchain serve --port=8100
```

## Examples

ä½¿ç”¨ [LangChain Templates](https://github.com/langchain-ai/langchain/blob/master/templates/README.md) å¿«é€Ÿå•Ÿå‹•æ‚¨çš„ LangServe å¯¦ä¾‹ã€‚

å¦‚éœ€æ›´å¤šç¯„ä¾‹ï¼Œè«‹åƒé–±ç¯„æœ¬ç´¢å¼•æˆ–ç¯„ä¾‹ç›®éŒ„ã€‚

|æè¿°	|é€£çµ|
|---------------|-----|
|**LLMs** Minimal example that reserves OpenAI and Anthropic chat models. Uses async, supports batching and streaming.|[server](https://github.com/langchain-ai/langserve/tree/main/examples/llm/server.py), [client](https://github.com/langchain-ai/langserve/blob/main/examples/llm/client.ipynb)|
|**Retriever** Simple server that exposes a retriever as a runnable.|[server](https://github.com/langchain-ai/langserve/tree/main/examples/retrieval/server.py), [client](https://github.com/langchain-ai/langserve/tree/main/examples/retrieval/client.ipynb)|
|**Conversational Retriever** A Conversational Retriever exposed via LangServe|[server](https://github.com/langchain-ai/langserve/tree/main/examples/conversational_retrieval_chain/server.py), [client](https://github.com/langchain-ai/langserve/tree/main/examples/conversational_retrieval_chain/client.ipynb)|
|**Agent** without **conversation history** based on [OpenAI tools](https://python.langchain.com/docs/modules/agents/agent_types/openai_functions_agent)|[server](https://github.com/langchain-ai/langserve/tree/main/examples/agent/server.py), [client](https://github.com/langchain-ai/langserve/tree/main/examples/agent/client.ipynb)|
|**Agent** with **conversation history** based on [OpenAI tools](https://python.langchain.com/docs/modules/agents/agent_types/openai_functions_agent)|[server](https://github.com/langchain-ai/langserve/blob/main/examples/agent_with_history/server.py), [client](https://github.com/langchain-ai/langserve/blob/main/examples/agent_with_history/client.ipynb)|
|[RunnableWithMessageHistory](https://python.langchain.com/docs/expression_language/how_to/message_history) to implement chat persisted on backend, keyed off a `session_id` supplied by client.|[server](https://github.com/langchain-ai/langserve/tree/main/examples/chat_with_persistence/server.py), [client](https://github.com/langchain-ai/langserve/tree/main/examples/chat_with_persistence/client.ipynb)|
|[RunnableWithMessageHistory](https://python.langchain.com/docs/expression_language/how_to/message_history) to implement chat persisted on backend, keyed off a `conversation_id` supplied by client, and `user_id` (see Auth for implementing `user_id` properly).|[server](https://github.com/langchain-ai/langserve/tree/main/examples/chat_with_persistence_and_user/server.py), [client](https://github.com/langchain-ai/langserve/tree/main/examples/chat_with_persistence_and_user/client.ipynb)|
|[Configurable Runnable](https://python.langchain.com/docs/expression_language/how_to/configure) to create a retriever that supports run time configuration of the index name.|[server](https://github.com/langchain-ai/langserve/tree/main/examples/configurable_retrieval/server.py), [client](https://github.com/langchain-ai/langserve/tree/main/examples/configurable_retrieval/client.ipynb)|
|[Configurable Runnable](https://python.langchain.com/docs/expression_language/how_to/configure) that shows configurable fields and configurable alternatives.|[server](https://github.com/langchain-ai/langserve/tree/main/examples/configurable_chain/server.py), [client](https://github.com/langchain-ai/langserve/tree/main/examples/configurable_chain/client.ipynb)|
|**APIHandler** Shows how to use `APIHandler` instead of `add_routes`. This provides more flexibility for developers to define endpoints. Works well with all FastAPI patterns, but takes a bit more effort.|[server](https://github.com/langchain-ai/langserve/tree/main/examples/api_handler_examples/server.py)|
|**LCEL Example** Example that uses LCEL to manipulate a dictionary input.|[server](https://github.com/langchain-ai/langserve/tree/main/examples/passthrough_dict/server.py), [client](https://github.com/langchain-ai/langserve/tree/main/examples/passthrough_dict/client.ipynb)|
|**Auth** with `add_routes`: Simple authentication that can be applied across all endpoints associated with app. (Not useful on its own for implementing per user logic.)|[server](https://github.com/langchain-ai/langserve/tree/main/examples/auth/global_deps/server.py)|
|**Auth** with `add_routes`: Simple authentication mechanism based on path dependencies. (No useful on its own for implementing per user logic.)|[server](https://github.com/langchain-ai/langserve/tree/main/examples/auth/path_dependencies/server.py)|
|**Auth** with `add_routes`: Implement per user logic and auth for endpoints that use per request config modifier. (**Note**: At the moment, does not integrate with OpenAPI docs.)|[server](https://github.com/langchain-ai/langserve/tree/main/examples/auth/per_req_config_modifier/server.py), [client](https://github.com/langchain-ai/langserve/tree/main/examples/auth/per_req_config_modifier/client.ipynb)|
|**Auth** with `APIHandler`: Implement per user logic and auth that shows how to search only within user owned documents.|[server](https://github.com/langchain-ai/langserve/tree/main/examples/auth/api_handler/server.py), [client](https://github.com/langchain-ai/langserve/tree/main/examples/auth/api_handler/client.ipynb)|
|**Widgets** Different widgets that can be used with playground (file upload and chat)|[server](https://github.com/langchain-ai/langserve/tree/main/examples/widgets/chat/tuples/server.py)|
|**Widgets** File upload widget used for LangServe playground.|[server](https://github.com/langchain-ai/langserve/tree/main/examples/file_processing/server.py), [client](https://github.com/langchain-ai/langserve/tree/main/examples/file_processing/client.ipynb)|

## Sample Application

### Server

é€™æ˜¯ä¸€å€‹éƒ¨ç½² OpenAI èŠå¤©æ¨¡å‹å’Œ Anthropic æ¨¡å‹è¬›è¿°æŸå€‹ä¸»é¡Œçš„ç¬‘è©±çš„ chain çš„ä¼ºæœå™¨ã€‚

```python
#!/usr/bin/env python
from fastapi import FastAPI
from langchain.prompts import ChatPromptTemplate
from langchain.chat_models import ChatAnthropic, ChatOpenAI
from langserve import add_routes

app = FastAPI(
    title="LangChain Server",
    version="1.0",
    description="A simple api server using Langchain's Runnable interfaces",
)

add_routes(
    app,
    ChatOpenAI(),
    path="/openai",
)

add_routes(
    app,
    ChatAnthropic(),
    path="/anthropic",
)

model = ChatAnthropic()

prompt = ChatPromptTemplate.from_template("tell me a joke about {topic}")

add_routes(
    app,
    prompt | model,
    path="/joke",
)

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="localhost", port=8000)
```

å¦‚æœæ‚¨æ‰“ç®—å¾ç€è¦½å™¨å‘¼å«ç«¯é»ï¼Œæ‚¨é‚„éœ€è¦è¨­å®š **CORS æ¨™é ­**ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ FastAPI çš„å…§å»ºä¸­ä»‹è»Ÿé«”ä¾†å¯¦ç¾ï¼š

```python
from fastapi.middleware.cors import CORSMiddleware

# Set all CORS enabled origins
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["*"],
)
```

## API æ–‡ä»¶

å¦‚æœæ‚¨å·²ç¶“éƒ¨ç½²äº†ä¸Šé¢çš„ä¼ºæœå™¨ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹ç”Ÿæˆçš„ OpenAPI æ–‡ä»¶ï¼š

!!! info
    âš ï¸ å¦‚æœä½¿ç”¨ `pydantic v2`ï¼Œlangserve å°‡ä¸æœƒç‚º invokeã€batchã€streamã€stream_log ç”¢ç”Ÿ api æ–‡ä»¶ã€‚æœ‰é—œæ›´å¤šè©³ç´°ä¿¡æ¯ï¼Œè«‹åƒé–±ä¸‹é¢çš„ [Pydantic](https://python.langchain.com/docs/langserve#pydantic) éƒ¨åˆ†ã€‚

```bash
curl localhost:8000/docs
```

ç¢ºä¿æ·»åŠ  `/docs` å¾Œç¶´ã€‚

!!! info
    âš ï¸ ç´¢å¼•é é¢ `/` ä¸æ˜¯è¨­è¨ˆå®šç¾©çš„ï¼Œå› æ­¤ `curl localhost:8000` æˆ–é€ è¨ª URL å°‡å‚³å› 404ã€‚å¦‚æœæ‚¨æƒ³è¦ `/` è™•çš„å…§å®¹ï¼Œè«‹å®šç¾©ä¸€å€‹ç«¯é» `@app.get("/")`ã€‚

### Client

**Python SDK**

```python
from langchain.schema import SystemMessage, HumanMessage
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnableMap
from langserve import RemoteRunnable

openai = RemoteRunnable("http://localhost:8000/openai/")
anthropic = RemoteRunnable("http://localhost:8000/anthropic/")
joke_chain = RemoteRunnable("http://localhost:8000/joke/")

joke_chain.invoke({"topic": "parrots"})

# or async
await joke_chain.ainvoke({"topic": "parrots"})

prompt = [
    SystemMessage(content='Act like either a cat or a parrot.'),
    HumanMessage(content='Hello!')
]

# Supports astream
async for msg in anthropic.astream(prompt):
    print(msg, end="", flush=True)

prompt = ChatPromptTemplate.from_messages(
    [("system", "Tell me a long story about {topic}")]
)

# Can define custom chains
chain = prompt | RunnableMap({
    "openai": openai,
    "anthropic": anthropic,
})

chain.batch([{"topic": "parrots"}, {"topic": "cats"}])
```

**TypeScript**

éœ€è¦ `LangChain.js` ç‰ˆæœ¬ 0.0.166 æˆ–æ›´é«˜ç‰ˆæœ¬:

```javascript
import { RemoteRunnable } from "@langchain/core/runnables/remote";

const chain = new RemoteRunnable({
    url: `http://localhost:8000/joke/`,
});
const result = await chain.invoke({
    topic: "cats",
});
```

**Python ä½¿ç”¨ `requests`**

```python
import requests

response = requests.post(
    "http://localhost:8000/joke/invoke",
    json={'input': {'topic': 'cats'}}
)
response.json()
```

**curl**

```bash
curl --location --request POST 'http://localhost:8000/joke/invoke' \
    --header 'Content-Type: application/json' \
    --data-raw '{
        "input": {
            "topic": "cats"
        }
    }'
```

## Endpoints

ä¸‹é¢çš„ç¨‹å¼ç¢¼ï¼š

```python
...
add_routes(
    app,
    runnable,
    path="/my_runnable",
)
```

æœƒå°‡é€™äº›ç«¯é»æ–°å¢è‡³ä¼ºæœå™¨çµ¦å¤–é¢å‘¼å«:

- `POST /my_runnable/invoke` - invoke the runnable on a single input
- `POST /my_runnable/batch` - invoke the runnable on a batch of inputs
- `POST /my_runnable/stream` - invoke on a single input and stream the output
- `POST /my_runnable/stream_log` - invoke on a single input and stream the output, including output of intermediate steps as it's generated
- `POST /my_runnable/astream_events` - invoke on a single input and stream events as they are generated, including from intermediate steps.
- `GET /my_runnable/input_schema` - json schema for input to the runnable
- `GET /my_runnable/output_schema` - json schema for output of the runnable
- `GET /my_runnable/config_schema` - json schema for config of the runnable

é€™äº›ç«¯é»èˆ‡ [LangChain è¡¨é”å¼èªè¨€ä»‹é¢](https://python.langchain.com/docs/expression_language/interface)ç›¸ç¬¦ - è«‹åƒè€ƒæ­¤æ–‡ä»¶ä»¥å–å¾—æ›´å¤šè©³ç´°è³‡è¨Šã€‚

## Playground

æ‚¨å¯ä»¥åœ¨ `/my_runnable/playground/` æ‰¾åˆ°æ‚¨çš„ runnable çš„ Playground é é¢ã€‚é€™å…¬é–‹äº†ä¸€å€‹ç°¡å–®çš„ UIï¼Œç”¨æ–¼é…ç½®å’Œå‘¼å«å…·æœ‰æµè¼¸å‡ºå’Œä¸­é–“æ­¥é©Ÿçš„å¯é‹è¡Œç‰©ä»¶ã€‚

![](./assets/langserve_playground.png)

### Widgets

Playground æ”¯æ´ widgetsï¼Œå¯ç”¨æ–¼ä½¿ç”¨ä¸åŒçš„è¼¸å…¥ä¾†æ¸¬è©¦æ‚¨çš„å¯é‹è¡Œç¨‹å¼ã€‚æœ‰é—œæ›´å¤šè©³ç´°ä¿¡æ¯ï¼Œè«‹åƒé–±ä¸‹é¢çš„ widgets éƒ¨åˆ†ã€‚

### Sharing

æ­¤å¤–ï¼Œå°æ–¼å¯é…ç½®çš„å¯é‹è¡Œå°è±¡ï¼Œplayground å°‡å…è¨±æ‚¨é…ç½®å¯é‹è¡Œå°è±¡ä¸¦å…±ç”¨é…ç½®é€£çµï¼š

![](./assets/langserve_configurable.png)

## Chat playground

LangServe ä¹Ÿæ”¯æ´ä»¥èŠå¤©ç‚ºä¸­å¿ƒçš„ Playgroundï¼Œå¯ä»¥é¸æ“‡åœ¨ `/my_runnable/playground/` ä¸‹ä½¿ç”¨ã€‚èˆ‡ä¸€èˆ¬çš„ Playground ä¸åŒï¼Œåƒ…æ”¯æ´æŸäº›é¡å‹çš„å¯é‹è¡Œç‰©ä»¶ - å¯é‹è¡Œç‰©ä»¶çš„è¼¸å…¥æ¨¡å¼å¿…é ˆæ˜¯å…·æœ‰ä»¥ä¸‹ä»»ä¸€å…§å®¹çš„ `dict`ï¼š

- å–®ä¸€éµï¼Œä¸”è©²éµçš„å€¼å¿…é ˆæ˜¯èŠå¤©è¨Šæ¯æ¸…å–®ã€‚
- å…©å€‹éµï¼Œä¸€å€‹çš„å€¼æ˜¯è¨Šæ¯åˆ—è¡¨ï¼Œå¦ä¸€å€‹ä»£è¡¨æœ€æ–°çš„è¨Šæ¯ã€‚

æˆ‘å€‘å»ºè­°æ‚¨ä½¿ç”¨ç¬¬ä¸€ç¨®æ ¼å¼ã€‚

å¯é‹è¡Œç‰©ä»¶ä¹Ÿå¿…é ˆå‚³å› `AIMessage` æˆ–å­—ä¸²ã€‚

è¦å•Ÿç”¨å®ƒï¼Œæ‚¨å¿…é ˆåœ¨æ–°å¢è·¯ç·šæ™‚è¨­å®š `playground_type ="chat"`ã€‚åº•ä¸‹æ˜¯ä¸€å€‹ä¾‹å­ï¼š

```python
# Declare a chain
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful, professional assistant named Cob."),
        MessagesPlaceholder(variable_name="messages"),
    ]
)

chain = prompt | ChatAnthropic(model="claude-2")


class InputChat(BaseModel):
    """Input for the chat endpoint."""

    messages: List[Union[HumanMessage, AIMessage, SystemMessage]] = Field(
        ...,
        description="The chat messages representing the current conversation.",
    )


add_routes(
    app,
    chain.with_types(input_type=InputChat),
    enable_feedback_endpoint=True,
    enable_public_trace_link_endpoint=True,
    playground_type="chat",
)
```

## Legacy Chains

LangServe å¯èˆ‡ Runnableï¼ˆé€é LangChain è¡¨é”å¼èªè¨€æ§‹é€ ï¼‰å’Œ legacy chainï¼ˆç¹¼æ‰¿è‡ª `Chain` é¡åˆ¥ï¼‰ä¸€èµ·ä½¿ç”¨ã€‚ç„¶è€Œï¼Œlegacy chainã€€çš„ä¸€äº›è¼¸å…¥ schema å¯èƒ½ä¸å®Œæ•´/ä¸æ­£ç¢ºï¼Œå°è‡´éŒ¯èª¤ã€‚é€™å¯ä»¥é€éæ›´æ–° LangChain ä¸­é€™äº›éˆçš„ `input_schema` å±¬æ€§ä¾†è§£æ±ºã€‚å¦‚æœæ‚¨é‡åˆ°ä»»ä½•éŒ¯èª¤ï¼Œè«‹åœ¨æ­¤å„²å­˜åº«ä¸Šæå‡ºå•é¡Œï¼Œæˆ‘å€‘å°‡ç›¡åŠ›è§£æ±ºå®ƒã€‚

## Pydantic

LangServe æä¾›å° Pydantic 2 çš„æ”¯æŒï¼Œä½†æœ‰ä¸€äº›é™åˆ¶ã€‚

1. ç•¶ä½¿ç”¨ Pydantic V2 æ™‚ï¼Œlangserve ä¸æœƒç‚º invoke/batch/stream/stream_log ç”¢ç”Ÿ OpenAPI æ–‡ä»¶ã€‚ Fast API ä¸æ”¯æ´ [æ··åˆ pydantic v1 å’Œ v2 å‘½åç©ºé–“]ã€‚
2. LangChain åœ¨ Pydantic v2 ä¸­ä½¿ç”¨ v1 å‘½åç©ºé–“ã€‚è«‹é–±è®€ä»¥ä¸‹æŒ‡å—ä»¥ç¢ºä¿èˆ‡ LangChain çš„å…¼å®¹æ€§

## é€²éš

### èº«ä»½èªè­‰

å¦‚æœæ‚¨éœ€è¦å‘ä¼ºæœå™¨æ–°å¢èº«ä»½é©—è­‰ï¼Œè«‹é–±è®€ Fast API æœ‰é—œä¾è³´é …å’Œå®‰å…¨æ€§çš„æ–‡ä»¶ã€‚

ä»¥ä¸‹ç¯„ä¾‹å±•ç¤ºå¦‚ä½•ä½¿ç”¨ FastAPI é€£ç·šé©—è­‰é‚è¼¯ LangServe ç«¯é»ã€‚

**ä½¿ç”¨ `add_routes`**

å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯ `add_routes` ä¾†é€²è¡Œèº«ä»½èªè­‰ï¼Œè«‹åƒé–±æ­¤è™•çš„[ç¯„ä¾‹](https://github.com/langchain-ai/langserve/tree/main/examples/auth)ã€‚

|æè¿°	|é€£çµ|
|---------------|-----|
|**Auth** with `add_routes`: Simple authentication that can be applied across all endpoints associated with app. (Not useful on its own for implementing per user logic.)|[server](https://github.com/langchain-ai/langserve/tree/main/examples/auth/global_deps/server.py)|
|**Auth** with `add_routes`: Simple authentication mechanism based on path dependencies. (No useful on its own for implementing per user logic.)|[server](https://github.com/langchain-ai/langserve/tree/main/examples/auth/path_dependencies/server.py)|
|**Auth** with `add_routes`: Implement per user logic and auth for endpoints that use per request config modifier. (**Note**: At the moment, does not integrate with OpenAPI docs.)|[server](https://github.com/langchain-ai/langserve/tree/main/examples/auth/per_req_config_modifier/server.py), [client](https://github.com/langchain-ai/langserve/tree/main/examples/auth/per_req_config_modifier/client.ipynb)|

æˆ–è€…ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ FastAPI çš„ middlewareã€‚

ä½¿ç”¨å…¨åŸŸä¾è³´é …å’Œè·¯å¾‘ä¾è³´é …çš„å„ªé»æ˜¯ OpenAPI æ–‡ä»¶é é¢å°‡æ­£ç¢ºæ”¯æ´èº«ä»½é©—è­‰ï¼Œä½†é€™äº›ä¸è¶³ä»¥å¯¦ç¾æ¯å€‹ä½¿ç”¨è€…çš„é‚è¼¯ï¼ˆä¾‹å¦‚ï¼Œè£½ä½œåªèƒ½åœ¨ä½¿ç”¨è€…æ“æœ‰çš„æ–‡ä»¶ä¸­æœå°‹çš„æ‡‰ç”¨ç¨‹å¼ï¼‰ã€‚

å¦‚æœæ‚¨éœ€è¦å¯¦ä½œæ¯å€‹ä½¿ç”¨è€…é‚è¼¯ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ `per_req_config_modifier` æˆ– `APIHandler` ä¾†å¯¦ä½œæ­¤é‚è¼¯ã€‚

**Per User**

å¦‚æœæ‚¨éœ€è¦ä¾è³´ä½¿ç”¨è€…çš„æˆæ¬Šæˆ–é‚è¼¯ï¼Œè«‹åœ¨ä½¿ç”¨ `add_routes` æ™‚æŒ‡å®š `per_req_config_modifier`ã€‚ä½¿ç”¨å¯å‘¼å«å°è±¡æ¥æ”¶åŸå§‹ `Request` å°è±¡ï¼Œä¸¦å¯å¾ä¸­æå–ç›¸é—œè³‡è¨Šä»¥ç”¨æ–¼èº«ä»½é©—è­‰å’Œæˆæ¬Šç›®çš„ã€‚


**Using APIHandler**

å¦‚æœæ‚¨ç†Ÿæ‚‰ FastAPI å’Œ pythonï¼Œå‰‡å¯ä»¥ä½¿ç”¨ LangServe çš„ [APIHandler]ã€‚

|æè¿°	|é€£çµ|
|---------------|-----|
|**Auth** with `APIHandler`: Implement per user logic and auth that shows how to search only within user owned documents.|[server](https://github.com/langchain-ai/langserve/tree/main/examples/auth/api_handler/server.py), [client](https://github.com/langchain-ai/langserve/tree/main/examples/auth/api_handler/client.ipynb)|
|**APIHandler** Shows how to use APIHandler instead of `add_routes`. This provides more flexibility for developers to define endpoints. Works well with all FastAPI patterns, but takes a bit more effort.|[server](https://github.com/langchain-ai/langserve/tree/main/examples/api_handler_examples/server.py), [client](https://github.com/langchain-ai/langserve/tree/main/examples/api_handler_examples/client.ipynb)|

é€™éœ€è¦å†å¤šå¯«ä¸€äº›ç¨‹å¼ç¢¼ï¼Œä½†å¯ä»¥è®“æ‚¨å®Œå…¨æ§åˆ¶ç«¯é»å®šç¾©ï¼Œå› æ­¤æ‚¨å¯ä»¥åŸ·è¡Œèº«ä»½é©—è­‰æ‰€éœ€çš„ä»»ä½•è‡ªè¨‚é‚è¼¯ã€‚

### Files

LLM æ‡‰ç”¨ç¨‹å¼ç¶“å¸¸è™•ç†æ–‡ä»¶ã€‚å¯ä»¥æ¡ç”¨ä¸åŒçš„æ¶æ§‹ä¾†å¯¦ç¾æ–‡ä»¶è™•ç†:

1. æª”æ¡ˆå¯ä»¥é€éå°ˆç”¨ç«¯é»ä¸Šå‚³åˆ°ä¼ºæœå™¨ä¸¦ä½¿ç”¨å–®ç¨çš„ç«¯é»é€²è¡Œè™•ç†
2. æ–‡ä»¶å¯ä»¥é€éå€¼ï¼ˆæ–‡ä»¶ä½å…ƒçµ„ï¼‰æˆ–å¼•ç”¨ï¼ˆä¾‹å¦‚ï¼Œæ–‡ä»¶å…§å®¹çš„ s3 urlï¼‰ä¸Šå‚³
3. è™•ç†ç«¯é»å¯ä»¥æ˜¯é˜»å¡çš„æˆ–éé˜»å¡çš„
4. å¦‚æœéœ€è¦å¤§é‡è™•ç†ï¼Œå¯ä»¥å°‡è™•ç†å¸è¼‰åˆ°å°ˆç”¨é€²ç¨‹æ± 

æ‚¨æ‡‰è©²ç¢ºå®šé©åˆæ‚¨çš„æ‡‰ç”¨ç¨‹å¼çš„æ¶æ§‹ã€‚

ç›®å‰ï¼Œè¦å°‡æ–‡ä»¶æŒ‰å€¼ä¸Šå‚³åˆ°å¯é‹è¡Œæ–‡ä»¶ï¼Œè«‹å°æ–‡ä»¶ä½¿ç”¨ `base64` ç·¨ç¢¼ï¼ˆå°šä¸æ”¯æ´ `multipart/form-data`ï¼‰ã€‚

ä¸‹é¢çš„[ç¯„ä¾‹](https://github.com/langchain-ai/langserve/tree/main/examples/file_processing)å±•ç¤ºå¦‚ä½•ä½¿ç”¨ Base64 ç·¨ç¢¼å°‡æª”æ¡ˆå‚³é€åˆ°é ç«¯å¯é‹è¡Œç¨‹å¼ã€‚

### Custom Input and Output Types

Input å’Œ Output é¡å‹åœ¨æ‰€æœ‰ runnable ä¸Šå®šç¾©ã€‚

æ‚¨å¯ä»¥é€é `input_schema` å’Œ `output_schema` å±¬æ€§å­˜å–å®ƒå€‘ã€‚

`LangServe` ä½¿ç”¨é€™äº›é¡å‹é€²è¡Œé©—è­‰å’Œè¨˜éŒ„ã€‚

å¦‚æœè¦è¦†å¯«é è¨­çš„æ¨æ–·é¡å‹ï¼Œå¯ä»¥ä½¿ç”¨ `with_types` æ–¹æ³•ã€‚

é€™æ˜¯ä¸€å€‹æ¼”ç¤ºé€™å€‹æƒ³æ³•çš„ç¯„ä¾‹:

```python
from typing import Any

from fastapi import FastAPI
from langchain.schema.runnable import RunnableLambda

app = FastAPI()


def func(x: Any) -> int:
    """Mistyped function that should accept an int but accepts anything."""
    return x + 1


runnable = RunnableLambda(func).with_types(
    input_type=int,
)

add_routes(app, runnable)
```

### Custom User Types

å¦‚æœæ‚¨å¸Œæœ›è³‡æ–™ååºåˆ—åŒ–ç‚º `pydantic` æ¨¡å‹è€Œä¸æ˜¯ç­‰æ•ˆçš„ `dict` è¡¨ç¤ºï¼Œè«‹ç¹¼æ‰¿è‡ª `CustomUserType`ã€‚

ç›®å‰ï¼Œæ­¤é¡å‹åƒ…é©ç”¨æ–¼ä¼ºæœå™¨ç«¯ï¼Œä¸¦ç”¨æ–¼æŒ‡å®šæ‰€éœ€çš„è§£ç¢¼è¡Œç‚ºã€‚å¦‚æœç¹¼æ‰¿æ­¤é¡å‹ï¼Œä¼ºæœå™¨æœƒå°‡è§£ç¢¼å¾Œçš„é¡å‹ä¿ç•™ç‚º `pydantic` æ¨¡å‹ï¼Œè€Œä¸æ˜¯å°‡å…¶è½‰æ›ç‚ºå­—å…¸ã€‚

```python
from fastapi import FastAPI
from langchain.schema.runnable import RunnableLambda

from langserve import add_routes
from langserve.schema import CustomUserType

app = FastAPI()


class Foo(CustomUserType):
    bar: int


def func(foo: Foo) -> int:
    """Sample function that expects a Foo type which is a pydantic model"""
    assert isinstance(foo, Foo)
    return foo.bar


# Note that the input and output type are automatically inferred!
# You do not need to specify them.
# runnable = RunnableLambda(func).with_types( # <-- Not needed in this case
#     input_type=Foo,
#     output_type=int,
#
add_routes(app, RunnableLambda(func), path="/foo")
```

### Playground Widgets

Playground å…è¨±æ‚¨å¾å¾Œç«¯ç‚ºå¯é‹è¡Œç‰©ä»¶å®šç¾©è‡ªè¨‚å°å·¥å…·ã€‚

é€™è£¡æœ‰ä¸€äº›ä¾‹å­ï¼š

|æè¿°	|é€£çµ|
|---------------|-----|
|**Widgets** Different widgets that can be used with playground (file upload and chat)|[server](https://github.com/langchain-ai/langserve/tree/main/examples/widgets/chat/tuples/server.py), [client](https://github.com/langchain-ai/langserve/tree/main/examples/widgets/client.ipynb)|
|**Widgets** File upload widget used for LangServe playground.|[server](https://github.com/langchain-ai/langserve/tree/main/examples/file_processing/server.py), [client](https://github.com/langchain-ai/langserve/tree/main/examples/file_processing/client.ipynb)|

**Schema**

- widget åœ¨æ¬„ä½å±¤ç´šæŒ‡å®šï¼Œä¸¦ä½œç‚ºè¼¸å…¥é¡å‹çš„ JSON æ¶æ§‹çš„ä¸€éƒ¨åˆ†æä¾›
- widget å¿…é ˆåŒ…å«ä¸€å€‹åç‚º `type` çš„éµï¼Œå…¶å€¼æ˜¯çœ¾æ‰€å‘¨çŸ¥çš„ widget åˆ—è¡¨ä¹‹ä¸€
- å…¶ä»– widget éµå°‡èˆ‡æè¿° JSON ç‰©ä»¶ä¸­çš„è·¯å¾‘çš„å€¼é—œè¯

```python
type JsonPath = number | string | (number | string)[];
type NameSpacedPath = { title: string; path: JsonPath }; // Using title to mimick json schema, but can use namespace
type OneOfPath = { oneOf: JsonPath[] };

type Widget = {
    type: string // Some well known type (e.g., base64file, chat etc.)
    [key: string]: JsonPath | NameSpacedPath | OneOfPath;
};
```

### Available Widgets

ç”¨æˆ¶ç¾åœ¨åªèƒ½æ‰‹å‹•æŒ‡å®šå…©ç¨® widget:

1. File Upload Widget
2. Chat History Widget

è«‹åƒé–±ä¸‹é¢æœ‰é—œé€™äº› widget çš„æ›´å¤šè³‡è¨Šã€‚

Playground UI ä¸Šçš„æ‰€æœ‰å…¶ä»–å°å·¥å…·éƒ½æ˜¯ç”± UI æ ¹æ“š Runnable çš„é…ç½®æ¶æ§‹è‡ªå‹•å»ºç«‹å’Œç®¡ç†çš„ã€‚ç•¶æ‚¨å»ºç«‹å¯é…ç½®çš„å¯é‹è¡Œç‰©ä»¶æ™‚ï¼Œplayground æ‡‰è©²æœƒå»ºç«‹é©ç•¶çš„å°å·¥å…·ä¾›æ‚¨æ§åˆ¶è¡Œç‚ºã€‚

#### File Upload Widget

å…è¨±åœ¨ UI Playground ä¸­ç‚º Base64 ç·¨ç¢¼å­—ä¸²ä¸Šå‚³çš„æª”æ¡ˆå»ºç«‹æª”æ¡ˆä¸Šå‚³è¼¸å…¥ã€‚é€™æ˜¯å®Œæ•´çš„ç¯„ä¾‹ã€‚

```python
try:
    from pydantic.v1 import Field
except ImportError:
    from pydantic import Field

from langserve import CustomUserType


# ATTENTION: Inherit from CustomUserType instead of BaseModel otherwise
#            the server will decode it into a dict instead of a pydantic model.
class FileProcessingRequest(CustomUserType):
    """Request including a base64 encoded file."""

    # The extra field is used to specify a widget for the playground UI.
    file: str = Field(..., extra={"widget": {"type": "base64file"}})
    num_chars: int = 100
```

![](./assets/file_upload_widget.png)

#### Chat Widget

æŸ¥çœ‹[ç¯„ä¾‹](https://github.com/langchain-ai/langserve/tree/main/examples/widgets/chat/tuples/server.py)ã€‚

è‹¥è¦å®šç¾© chat widgetï¼Œè«‹ç¢ºä¿å‚³é `"type":"chat"`ã€‚

- `input` æ˜¯è«‹æ±‚ä¸­åŒ…å«æ–°è¼¸å…¥è¨Šæ¯çš„æ¬„ä½çš„ `JSONPath`ã€‚
- `output` æ˜¯å…·æœ‰æ–°è¼¸å‡ºè¨Šæ¯çš„å›æ‡‰ä¸­æ¬„ä½çš„ `JSONPath`ã€‚
- å¦‚æœæ‡‰åŸæ¨£ä½¿ç”¨æ•´å€‹è¼¸å…¥æˆ–è¼¸å‡ºï¼ˆä¾‹å¦‚ï¼Œå¦‚æœè¼¸å‡ºæ˜¯èŠå¤©è¨Šæ¯æ¸…å–®ï¼‰ï¼Œå‰‡ä¸è¦æŒ‡å®šé€™äº›æ¬„ä½ã€‚

```python
class ChatHistory(CustomUserType):
    chat_history: List[Tuple[str, str]] = Field(
        ...,
        examples=[[("human input", "ai response")]],
        extra={"widget": {"type": "chat", "input": "question", "output": "answer"}},
    )
    question: str


def _format_to_messages(input: ChatHistory) -> List[BaseMessage]:
    """Format the input to a list of messages."""
    history = input.chat_history
    user_input = input.question

    messages = []

    for human, ai in history:
        messages.append(HumanMessage(content=human))
        messages.append(AIMessage(content=ai))
    messages.append(HumanMessage(content=user_input))
    return messages


model = ChatOpenAI()
chat_model = RunnableParallel({"answer": (RunnableLambda(_format_to_messages) | model)})
add_routes(
    app,
    chat_model.with_types(input_type=ChatHistory),
    config_keys=["configurable"],
    path="/chat",
)
```

![](./assets/chat_widget.png)

æ‚¨ä¹Ÿå¯ä»¥ç›´æ¥æŒ‡å®šè¨Šæ¯æ¸…å–®ä½œç‚ºåƒæ•¸ï¼Œå¦‚ä¸‹åˆ—ç¨‹å¼ç¢¼æ®µæ‰€ç¤º:

```python
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful assisstant named Cob."),
        MessagesPlaceholder(variable_name="messages"),
    ]
)

chain = prompt | ChatAnthropic(model="claude-2")


class MessageListInput(BaseModel):
    """Input for the chat endpoint."""
    messages: List[Union[HumanMessage, AIMessage]] = Field(
        ...,
        description="The chat messages representing the current conversation.",
        extra={"widget": {"type": "chat", "input": "messages"}},
    )


add_routes(
    app,
    chain.with_types(input_type=MessageListInput),
    path="/chat",
)
```

æœ‰é—œç¯„ä¾‹ï¼Œè«‹åƒé–±æ­¤[ç¯„ä¾‹æ–‡ä»¶](https://github.com/langchain-ai/langserve/tree/main/examples/widgets/chat/message_list/server.py)ã€‚

## Enabling / Disabling Endpoints

æ‚¨å¯ä»¥ enable/disable ç‰¹å®š route çš„å“ªäº›ç«¯é»ã€‚


Enable: ä¸‹é¢çš„ç¨‹å¼ç¢¼å°‡åƒ…å•Ÿç”¨ `invoke`ã€`batch` å’Œå°æ‡‰çš„ `config_hash` ç«¯é»è®Šé«”ã€‚

```python
add_routes(app, chain, enabled_endpoints=["invoke", "batch", "config_hashes"], path="/mychain")
```

Disable: ä¸‹é¢çš„ç¨‹å¼ç¢¼å°‡ç¦ç”¨ chain çš„ playground

```python
add_routes(app, chain, disabled_endpoints=["playground"], path="/mychain")
```

