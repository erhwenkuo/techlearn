# Content Type è§£ç¢¼

MLServer é€éæ–°å¢å° `content_type` è¨»è§£çš„æ”¯æ´ä¾†æ“´å±• V2 inference protocolã€‚æ­¤è¨»é‡‹å¯ä»¥é€éæ¨¡å‹å…ƒè³‡æ–™åƒæ•¸æˆ– `parameters` æä¾›ã€‚é€éåˆ©ç”¨ `content_type` è¨»é‡‹ï¼Œæˆ‘å€‘å¯ä»¥å‘ MLServer æä¾›å¿…è¦çš„è¨Šæ¯ï¼Œä»¥ä¾¿å®ƒå¯ä»¥å°‡ä¾†è‡ª V2 inference protocol çš„è¼¸å…¥ payload è§£ç¢¼ç‚ºå°æ¨¡å‹/ä½¿ç”¨è€…æœ‰æ„ç¾©çš„å…§å®¹ï¼ˆä¾‹å¦‚ NumPy é™£åˆ—ï¼‰ã€‚

æœ¬ç¯„ä¾‹å°‡å¼•å°æ‚¨å®Œæˆä¸€äº›ç¯„ä¾‹ï¼Œèªªæ˜å…¶å·¥ä½œåŸç†ä»¥åŠå¦‚ä½•æ“´å±•ã€‚

## Echo Inference Runtime

é¦–å…ˆï¼Œæˆ‘å€‘å°‡ç·¨å¯«ä¸€å€‹ dummy runtimeï¼Œå®ƒåªæœƒåˆ—å°è¼¸å…¥ã€è§£ç¢¼å¾Œçš„è¼¸å…¥ä¸¦è¿”å›å®ƒã€‚é€™å°‡ä½œç‚ºå±•ç¤º `content_type` æ”¯æ´å¦‚ä½•é‹ä½œçš„æ¸¬è©¦å¹³å°ã€‚

æ¥ä¸‹ä¾†ï¼Œæˆ‘å€‘å°‡é€éæ–°å¢è‡ªè¨‚ç·¨è§£ç¢¼å™¨ä¾†æ“´å±•æ­¤ dummy runtimeï¼Œé€™äº›ç·¨è§£ç¢¼å™¨æœƒå°‡æˆ‘å€‘çš„  V2 inference protocol çš„è¼¸å…¥ payload è§£ç¢¼ç‚ºè‡ªè¨‚é¡å‹ã€‚

```python title="runtime.py"
import json

from mlserver import MLModel
from mlserver.types import InferenceRequest, InferenceResponse, ResponseOutput
from mlserver.codecs import DecodedParameterName

_to_exclude = {
    "parameters": {DecodedParameterName, "headers"},
    'inputs': {"__all__": {"parameters": {DecodedParameterName, "headers"}}}
}


class EchoRuntime(MLModel):
    async def predict(self, payload: InferenceRequest) -> InferenceResponse:
        outputs = []
        for request_input in payload.inputs:
            decoded_input = self.decode(request_input)

            print(f"------ Encoded Input ({request_input.name}) ------")
            as_dict = request_input.dict(exclude=_to_exclude)  # type: ignore
            print(json.dumps(as_dict, indent=2))
            print(f"------ Decoded input ({request_input.name}) ------")
            print(decoded_input)

            outputs.append(
                ResponseOutput(
                    name=request_input.name,
                    datatype=request_input.datatype,
                    shape=request_input.shape,
                    data=request_input.data
                )
            )

        return InferenceResponse(model_name=self.name, outputs=outputs)
```

å¦‚æ‚¨åœ¨ä¸Šé¢çœ‹åˆ°çš„ï¼Œè©²é‹è¡Œæ™‚å°‡é€éå‘¼å« `self.decode()` è¼”åŠ©æ–¹æ³•ä¾†è§£ç¢¼å‚³å…¥çš„æœ‰æ•ˆè² è¼‰ã€‚æ­¤æ–¹æ³•å°‡æŒ‰ä»¥ä¸‹é †åºæª¢æŸ¥æ¯å€‹è¼¸å…¥çš„æ­£ç¢ºå…§å®¹é¡å‹ï¼š

1. **request payload** ä¸­çš„ `input[].parameters.content_type` æ¬„ä½ä¸­æ˜¯å¦å®šç¾©äº†ä»»ä½•å…§å®¹é¡å‹ï¼Ÿ
2. **model metadata** ä¸­çš„ `input[].parameters.content_type` æ¬„ä½ä¸­æ˜¯å¦å®šç¾©äº†ä»»ä½•å…§å®¹é¡å‹ï¼Ÿ
3. æ˜¯å¦æœ‰ä»»ä½•æ‡‰è©²å‡å®šçš„é è¨­å…§å®¹é¡å‹ï¼Ÿ

### Model Settings

ç‚ºäº†å•Ÿç”¨æ­¤é‹è¡Œæ™‚ï¼Œæˆ‘å€‘é‚„å°‡å»ºç«‹ä¸€å€‹ `model-settings.json` æª”æ¡ˆã€‚è©²æª”æ¡ˆæ‡‰è©²å­˜åœ¨æ–¼ï¼ˆæˆ–å¯å­˜å–ï¼‰æˆ‘å€‘åŸ·è¡Œ `mlserver start` çš„è³‡æ–™å¤¾ä¸­ã€‚

```json title="model-settings.json"
{
    "name": "content-type-example",
    "implementation": "runtime.EchoRuntime"
}
```

### é–‹å§‹æ¨¡å‹æ¨è«–æœå‹™

ç¾åœ¨æˆ‘å€‘å·²ç¶“æœ‰äº†é…ç½®ï¼Œæˆ‘å€‘å¯ä»¥é€éåŸ·è¡Œ `mlserver start` ä¾†å•Ÿå‹•ä¼ºæœå™¨ã€‚

```bash
mlserver start .
```

ç”±æ–¼æ­¤å‘½ä»¤å°‡å•Ÿå‹•ä¼ºæœå™¨ä¸¦å°é–çµ‚ç«¯ï¼Œç­‰å¾…è«‹æ±‚ï¼Œå› æ­¤éœ€è¦åœ¨å–®ç¨çš„çµ‚ç«¯æ©Ÿä¸Šåœ¨èƒŒæ™¯åŸ·è¡Œã€‚

## Request Inputs

æˆ‘å€‘çš„ç¬¬ä¸€æ­¥æ˜¯æ ¹æ“šå‚³å…¥çš„ `input[].parameters` æ¬„ä½æ±ºå®šå…§å®¹é¡å‹ã€‚ç‚ºæ­¤ï¼Œæˆ‘å€‘å°‡åœ¨èƒŒæ™¯å•Ÿå‹• MLServerï¼ˆä¾‹å¦‚åŸ·è¡Œ `mlserver start` ã€‚ï¼‰

```python title="test_infer.py"
import requests

payload = {
    "inputs": [
        {
            "name": "parameters-np",
            "datatype": "INT32",
            "shape": [2, 2],
            "data": [1, 2, 3, 4],
            "parameters": {
                "content_type": "np"
            }
        },
        {
            "name": "parameters-str",
            "datatype": "BYTES",
            "shape": [1],
            "data": "hello world ğŸ˜",
            "parameters": {
                "content_type": "str"
            }
        }
    ]
}

response = requests.post(
    "http://localhost:8080/v2/models/content-type-example/infer",
    json=payload
)

# æ‰“å°çµæœ
print(json.dumps(response.json(), indent=2))
```

çµæœ:

```json
{
  "model_name": "content-type-example",
  "id": "32dd0d7f-36c6-4b0e-a391-a39f1a5c351f",
  "parameters": {},
  "outputs": [
    {
      "name": "parameters-np",
      "shape": [
        2,
        2
      ],
      "datatype": "INT32",
      "data": [
        1,
        2,
        3,
        4
      ]
    },
    {
      "name": "parameters-str",
      "shape": [
        1
      ],
      "datatype": "BYTES",
      "data": "hello world ğŸ˜"
    }
  ]
}
```

### Codecs

æ‚¨å¯èƒ½å·²ç¶“æ³¨æ„åˆ°ï¼Œç·¨å¯«ç¬¦åˆ V2 Inference Protocol çš„è«‹æ±‚æœ‰æ•ˆè² è¼‰éœ€è¦å° V2 è¦ç¯„å’Œæ¯ç¨®å…§å®¹é¡å‹æ‰€éœ€çš„çµæ§‹æœ‰ä¸€å®šçš„äº†è§£ã€‚ç‚ºäº†è§£æ±ºé€™å€‹å•é¡Œä¸¦ç°¡åŒ–ä½¿ç”¨ï¼ŒMLServer å¥—ä»¶å…¬é–‹äº†ä¸€çµ„å¯¦ç”¨ç¨‹åºï¼Œå¯å”åŠ©æ‚¨é€é V2 å”å®šèˆ‡æ¨¡å‹é€²è¡Œäº’å‹•ã€‚

é€™äº›å·¥å…·ä¸»è¦è¢«å¡‘é€ ç‚º "codecs"ã€‚ä¹Ÿå°±æ˜¯èªªï¼ŒçŸ¥é“å¦‚ä½•å°‡ä»»æ„ Python è³‡æ–™é¡å‹èˆ‡ V2 æ¨ç†å”å®šã€Œç·¨ç¢¼ã€å’Œã€Œè§£ç¢¼ã€çš„æŠ½è±¡åŒ–ã€‚

é€šå¸¸ï¼Œæˆ‘å€‘å»ºè­°ä½¿ç”¨ç¾æœ‰çš„ç·¨è§£ç¢¼å™¨é›†ä¾†ç”¢ç”Ÿ V2 æœ‰æ•ˆè² è¼‰ã€‚é€™å°‡ç¢ºä¿è«‹æ±‚å’Œå›æ‡‰éµå¾ªæ­£ç¢ºçš„çµæ§‹ï¼Œä¸¦æ‡‰æä¾›æ›´ç„¡ç¸«çš„é«”é©—ã€‚

æŒ‰ç…§æˆ‘å€‘å…ˆå‰çš„ç¯„ä¾‹ï¼Œå¯ä»¥ä½¿ç”¨ç·¨è§£ç¢¼å™¨å°‡ç›¸åŒçš„ç¨‹å¼ç¢¼é‡å¯«ç‚ºï¼š

```python
import requests
import numpy as np

from mlserver.types import InferenceRequest, InferenceResponse
from mlserver.codecs import NumpyCodec, StringCodec

parameters_np = np.array([[1, 2], [3, 4]])
parameters_str = ["hello world ğŸ˜"]

payload = InferenceRequest(
    inputs = [
        NumpyCodec.encode_input("parameters_np", parameters_np),
        # The `use_bytes=False` flag will ensure that the encoded payload is JSON-compatible
        StringCodec.encode_input("parameters-str", parameters_str, use_bytes=False),
    ]
)

response = requests.post(
    "http://localhost:8080/v2/models/content-type-example/infer",
    json=payload.model_dump()
)


# é‡æ–°æ§‹å»ºæˆ InferenceResponse ç‰©ä»¶
raw_response = response.json()

response_payload = InferenceResponse(**raw_response)

print(NumpyCodec.decode_output(response_payload.outputs[0]))
print(StringCodec.decode_output(response_payload.outputs[1]))
```

è«‹æ³¨æ„ï¼Œé‡å¯«çš„ç¨‹å¼ç¢¼ç‰‡æ®µç¾åœ¨ä½¿ç”¨å…§å»ºçš„ `InferenceRequest` é¡åˆ¥ï¼Œå®ƒè¡¨ç¤º V2 æ¨ç†è«‹æ±‚ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œå®ƒé‚„ä½¿ç”¨ `NumpyCodec` å’Œ `StringCodec` å¯¦ç¾ï¼Œå®ƒå€‘çŸ¥é“å¦‚ä½•å°‡ Numpy array å’Œ string ç·¨ç¢¼ç‚º V2 ç›¸å®¹çš„è«‹æ±‚è¼¸å…¥ã€‚

### Model Metadata

æˆ‘å€‘çš„ä¸‹ä¸€æ­¥å°‡æ˜¯é€éæ¨¡å‹å…ƒè³‡æ–™å®šç¾©é æœŸçš„å…§å®¹é¡å‹ã€‚é€™å¯ä»¥é€éæ“´å±• `model-settings.json` æª”æ¡ˆä¸¦æ·»åŠ æœ‰é—œè¼¸å…¥çš„éƒ¨åˆ†ä¾†å®Œæˆã€‚

```json title="model-settings.json"
{
    "name": "content-type-example",
    "implementation": "runtime.EchoRuntime",
    "inputs": [
        {
            "name": "metadata-np",
            "datatype": "INT32",
            "shape": [2, 2],
            "parameters": {
                "content_type": "np"
            }
        },
        {
            "name": "metadata-str",
            "datatype": "BYTES",
            "shape": [11],
            "parameters": {
                "content_type": "str"
            }
        }
    ]
}
```

æ–°å¢æ­¤å…ƒè³‡æ–™å¾Œï¼Œæˆ‘å€‘å°‡é‡æ–°å•Ÿå‹• MLServerï¼ˆä¾‹å¦‚ `mlserver start` ï¼‰ï¼Œä¸¦ä¸”æˆ‘å€‘å°‡ç™¼é€ä¸€å€‹ä¸å¸¶ä»»ä½•æ˜ç¢ºåƒæ•¸çš„æ–°è«‹æ±‚ã€‚

```python
import requests

payload = {
    "inputs": [
        {
            "name": "metadata-np",
            "datatype": "INT32",
            "shape": [2, 2],
            "data": [1, 2, 3, 4],
        },
        {
            "name": "metadata-str",
            "datatype": "BYTES",
            "shape": [11],
            "data": "hello world ğŸ˜",
        }
    ]
}

response = requests.post(
    "http://localhost:8080/v2/models/content-type-example/infer",
    json=payload
)
```

æ­£å¦‚æ‚¨æ‡‰è©²èƒ½å¤ åœ¨ä¼ºæœå™¨æ—¥èªŒä¸­çœ‹åˆ°çš„é‚£æ¨£ï¼ŒMLServer å°‡æ ¹æ“šæ¨¡å‹å…ƒè³‡æ–™äº¤å‰å¼•ç”¨è¼¸å…¥åç¨±ä»¥æ‰¾åˆ°æ­£ç¢ºçš„å…§å®¹é¡å‹ã€‚

### å®¢åˆ¶ Codecs

åœ¨æŸäº›æƒ…æ³ä¸‹ï¼Œè‡ªè¨‚æ¨ç†é‹è¡Œæ™‚å¯èƒ½éœ€è¦ ç·¨ç¢¼/è§£ç¢¼ ç‚ºè‡ªè¨‚è³‡æ–™é¡å‹ã€‚ä½œç‚ºä¸€å€‹ä¾‹å­ï¼Œæˆ‘å€‘å¯ä»¥æƒ³åˆ°åªèƒ½å° `pillow` åœ–åƒç‰©ä»¶é€²è¡Œæ“ä½œçš„é›»è…¦è¦–è¦ºæ¨¡å‹ã€‚

åœ¨é€™äº›å ´æ™¯ä¸­ï¼Œå¯ä»¥æ“´å±• `Codec` ä»‹é¢ä¾†ç·¨å¯«æˆ‘å€‘çš„è‡ªè¨‚ç·¨ç¢¼é‚è¼¯ã€‚ç·¨è§£ç¢¼å™¨åªæ˜¯ä¸€å€‹å®šç¾©äº† `decode()` å’Œ `encode()` æ–¹æ³•çš„ç‰©ä»¶ã€‚ç‚ºäº†èªªæ˜é€™æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œæˆ‘å€‘å°‡æ“´å±•è‡ªè¨‚é‹è¡Œæ™‚ä»¥æ·»åŠ è‡ªè¨‚ `PillowCodec`ã€‚

```python title="runtime.py"
import io
import json

from PIL import Image

from mlserver import MLModel
from mlserver.types import (
    InferenceRequest,
    InferenceResponse,
    RequestInput,
    ResponseOutput,
)
from mlserver.codecs import NumpyCodec, register_input_codec, DecodedParameterName
from mlserver.codecs.utils import InputOrOutput

_to_exclude = {
    "parameters": {DecodedParameterName},
    "inputs": {"__all__": {"parameters": {DecodedParameterName}}},
}

@register_input_codec
class PillowCodec(NumpyCodec):
    ContentType = "img"
    DefaultMode = "L"

    @classmethod
    def can_encode(cls, payload: Image) -> bool:
        return isinstance(payload, Image)

    @classmethod
    def _decode(cls, input_or_output: InputOrOutput) -> Image:
        if input_or_output.datatype != "BYTES":
            # If not bytes, assume it's an array
            image_array = super().decode_input(input_or_output)  # type: ignore
            return Image.fromarray(image_array, mode=cls.DefaultMode)

        encoded = input_or_output.data
        if isinstance(encoded, str):
            encoded = encoded.encode()

        return Image.frombytes(
            mode=cls.DefaultMode, size=input_or_output.shape, data=encoded
        )

    @classmethod
    def encode_output(cls, name: str, payload: Image) -> ResponseOutput:  # type: ignore
        byte_array = io.BytesIO()
        payload.save(byte_array, mode=cls.DefaultMode)

        return ResponseOutput(
            name=name, shape=payload.size, datatype="BYTES", data=byte_array.getvalue()
        )

    @classmethod
    def decode_output(cls, response_output: ResponseOutput) -> Image:
        return cls._decode(response_output)

    @classmethod
    def encode_input(cls, name: str, payload: Image) -> RequestInput:  # type: ignore
        output = cls.encode_output(name, payload)
        return RequestInput(
            name=output.name,
            shape=output.shape,
            datatype=output.datatype,
            data=output.data,
        )

    @classmethod
    def decode_input(cls, request_input: RequestInput) -> Image:
        return cls._decode(request_input)

class EchoRuntime(MLModel):
    async def predict(self, payload: InferenceRequest) -> InferenceResponse:
        outputs = []
        for request_input in payload.inputs:
            decoded_input = self.decode(request_input)
            print(f"------ Encoded Input ({request_input.name}) ------")
            as_dict = request_input.dict(exclude=_to_exclude)  # type: ignore
            print(json.dumps(as_dict, indent=2))
            print(f"------ Decoded input ({request_input.name}) ------")
            print(decoded_input)

            outputs.append(
                ResponseOutput(
                    name=request_input.name,
                    datatype=request_input.datatype,
                    shape=request_input.shape,
                    data=request_input.data,
                )
            )

        return InferenceResponse(model_name=self.name, outputs=outputs)
```

æˆ‘å€‘ç¾åœ¨æ‡‰è©²èƒ½å¤ é‡æ–°å•Ÿå‹• MLServer å¯¦ä¾‹ï¼ˆå³ä½¿ç”¨ `mlserver start .` å‘½ä»¤ï¼‰ï¼Œä»¥å‚³é€ä¸€äº›æ¸¬è©¦è«‹æ±‚ã€‚


```python
import requests

payload = {
    "inputs": [
        {
            "name": "image-int32",
            "datatype": "INT32",
            "shape": [8, 8],
            "data": [
                1, 0, 1, 0, 1, 0, 1, 0,
                1, 0, 1, 0, 1, 0, 1, 0,
                1, 0, 1, 0, 1, 0, 1, 0,
                1, 0, 1, 0, 1, 0, 1, 0,
                1, 0, 1, 0, 1, 0, 1, 0,
                1, 0, 1, 0, 1, 0, 1, 0,
                1, 0, 1, 0, 1, 0, 1, 0,
                1, 0, 1, 0, 1, 0, 1, 0
            ],
            "parameters": {
                "content_type": "img"
            }
        },
        {
            "name": "image-bytes",
            "datatype": "BYTES",
            "shape": [8, 8],
            "data": (
                "10101010"
                "10101010"
                "10101010"
                "10101010"
                "10101010"
                "10101010"
                "10101010"
                "10101010"
            ),
            "parameters": {
                "content_type": "img"
            }
        }
    ]
}

response = requests.post(
    "http://localhost:8080/v2/models/content-type-example/infer",
    json=payload
)
```

æ­£å¦‚æ‚¨æ‡‰è©²èƒ½å¤ åœ¨ MLServer æ—¥èªŒä¸­çœ‹åˆ°çš„é‚£æ¨£ï¼Œä¼ºæœå™¨ç¾åœ¨èƒ½å¤ å°‡æœ‰æ•ˆè² è¼‰è§£ç¢¼ç‚º `Pillow` imageã€‚æ­¤ç¯„ä¾‹ä¹Ÿèªªæ˜äº† Codec ç‰©ä»¶å¦‚ä½•èˆ‡å¤šç¨®è³‡æ–™é¡å‹å€¼ï¼ˆä¾‹å¦‚æœ¬ä¾‹ä¸­çš„å¼µé‡å’Œ BYTESï¼‰ç›¸å®¹ã€‚

## Request Codecs

åˆ°ç›®å‰ç‚ºæ­¢ï¼Œæˆ‘å€‘å·²ç¶“äº†è§£å¦‚ä½•æŒ‡å®šç·¨è§£ç¢¼å™¨ï¼Œä»¥ä¾¿å°‡å®ƒå€‘å¥—ç”¨åœ¨ input levelã€‚ä½†æ˜¯ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨è«‹æ±‚ç¯„åœçš„ç·¨è§£ç¢¼å™¨ä¾†èšåˆå¤šå€‹è¼¸å…¥ä¾†è§£ç¢¼æœ‰æ•ˆè² è¼‰ã€‚é€™é€šå¸¸èˆ‡æ¨¡å‹éœ€è¦å¤šåˆ—è¼¸å…¥é¡å‹çš„æƒ…æ³ç›¸é—œï¼Œä¾‹å¦‚ Pandas DataFrameã€‚

ç‚ºäº†èªªæ˜é€™ä¸€é»ï¼Œæˆ‘å€‘å°‡é¦–å…ˆèª¿æ•´ EchoRuntimeï¼Œä»¥ä¾¿å®ƒåœ¨è«‹æ±‚å±¤ç´šåˆ—å°è§£ç¢¼çš„å…§å®¹ã€‚

```python title="runtime.py"
import json

from mlserver import MLModel
from mlserver.types import InferenceRequest, InferenceResponse, ResponseOutput
from mlserver.codecs import DecodedParameterName

_to_exclude = {
    "parameters": {DecodedParameterName},
    'inputs': {"__all__": {"parameters": {DecodedParameterName}}}
}

class EchoRuntime(MLModel):
    async def predict(self, payload: InferenceRequest) -> InferenceResponse:
        print("------ Encoded Input (request) ------")
        as_dict = payload.dict(exclude=_to_exclude)  # type: ignore
        print(json.dumps(as_dict, indent=2))
        print("------ Decoded input (request) ------")
        decoded_request = None
        if payload.parameters:
            decoded_request = getattr(payload.parameters, DecodedParameterName)
        print(decoded_request)

        outputs = []
        for request_input in payload.inputs:
            outputs.append(
                ResponseOutput(
                    name=request_input.name,
                    datatype=request_input.datatype,
                    shape=request_input.shape,
                    data=request_input.data
                )
            )

        return InferenceResponse(model_name=self.name, outputs=outputs)
```

æˆ‘å€‘ç¾åœ¨æ‡‰è©²èƒ½å¤ é‡æ–°å•Ÿå‹• MLServer å¯¦ä¾‹ï¼ˆå³ä½¿ç”¨ `mlserver start .` å‘½ä»¤ï¼‰ï¼Œä»¥å‚³é€ä¸€äº›æ¸¬è©¦è«‹æ±‚ã€‚

```python
import requests

payload = {
    "inputs": [
        {
            "name": "parameters-np",
            "datatype": "INT32",
            "shape": [2, 2],
            "data": [1, 2, 3, 4],
            "parameters": {
                "content_type": "np"
            }
        },
        {
            "name": "parameters-str",
            "datatype": "BYTES",
            "shape": [2, 11],
            "data": ["hello world ğŸ˜", "bye bye ğŸ˜"],
            "parameters": {
                "content_type": "str"
            }
        }
    ],
    "parameters": {
        "content_type": "pd"
    }
}

response = requests.post(
    "http://localhost:8080/v2/models/content-type-example/infer",
    json=payload
)
```

